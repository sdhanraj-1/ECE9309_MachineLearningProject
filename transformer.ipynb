{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d664a8f3-deb0-47ee-96dd-4a094ba076f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0718d3bf-0782-450d-8a8b-ef1130b0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset ONLY 300\n",
    "df = pd.read_csv(\"IEEE_Papers_Dataset.csv\")\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "df = df.head(400)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use a pre-trained tokenizer (e.g., BERT-based)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.data.iloc[idx][\"title\"])\n",
    "        abstract = str(self.data.iloc[idx][\"abstract\"])\n",
    "        input_text = title + \" \" + abstract  # Combine title and abstract\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"paper_index\": idx  # Used for retrieval\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = PaperDataset(df, tokenizer, 256)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50964c3e-9433-4a3a-8fa7-40b5575ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRecommender(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", embedding_dim=768, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Linear(self.embedding_dim, embedding_dim)  # Projection layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = nn.functional.normalize  # L2 normalization for retrieval\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(next(self.parameters()).device)  # Ensure inputs are on the same device as the model\n",
    "        attention_mask = attention_mask.to(next(self.parameters()).device)\n",
    "        \n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        embedding = self.fc(self.dropout(pooled_output))\n",
    "        return self.normalize(embedding, p=2, dim=1)  # Normalize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9619723-25a6-49e5-ab88-42fea483e730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2748\n",
      "Epoch 2, Loss: 0.3242\n",
      "Epoch 3, Loss: 0.2133\n",
      "Epoch 4, Loss: 0.1660\n",
      "Epoch 5, Loss: 0.1453\n",
      "Epoch 6, Loss: 0.1224\n",
      "Epoch 7, Loss: 0.1211\n",
      "Epoch 8, Loss: 0.1167\n",
      "Epoch 9, Loss: 0.1015\n",
      "Epoch 10, Loss: 0.1081\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender().to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def contrastive_loss(embeddings):\n",
    "    \"\"\"Loss function using pairwise Euclidean distances in the batch.\"\"\"\n",
    "    # Normalize embeddings (optional, helps numerical stability)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  \n",
    "    # Compute pairwise Euclidean distances\n",
    "    distance_matrix = torch.cdist(embeddings, embeddings, p=2)  # (batch_size, batch_size)\n",
    "    # Minimize the sum of all distances (encourages compact embedding space)\n",
    "    loss = distance_matrix.sum() / (embeddings.shape[0] ** 2)  # Normalize by batch_size^2\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(input_ids, attention_mask)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute loss using pairwise distances between all embeddings\n",
    "        loss = contrastive_loss(embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1bddb-dea3-4a5d-9e80-3da76064b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, model, df, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode query\n",
    "    query_tokens = tokenizer(query, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(query_tokens[\"input_ids\"], query_tokens[\"attention_mask\"]).cpu().numpy()\n",
    "\n",
    "    # Compute Euclidean distances between query and all paper embeddings\n",
    "    paper_embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_input_ids, batch_attention_mask).cpu().numpy()\n",
    "            paper_embeddings.append(batch_embeddings)\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())  # Store original indices\n",
    "\n",
    "    paper_embeddings = np.vstack(paper_embeddings)  # Stack all embeddings\n",
    "    paper_indices = np.array(paper_indices)\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = np.linalg.norm(paper_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # Get top-k closest papers (smallest distances)\n",
    "    top_indices = np.argsort(distances)[:top_k]\n",
    "\n",
    "    print(\"\\nRecommended Papers:\")\n",
    "    for idx in top_indices:\n",
    "        paper_idx = paper_indices[idx]\n",
    "        print(f\"Title: {df.iloc[paper_idx]['title']}\\nAbstract: {df.iloc[paper_idx]['abstract']}\\nDistance: {distances[idx]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a25f812-4c97-4260-a369-d4a766f40744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Papers:\n",
      "Title: Corrections to “Complex Permittivity of NaOH Solutions Used in Liquid-Metal Circuits”\n",
      "Abstract: In the above article \n",
      "[1]\n",
      ", \n",
      "Table 1\n",
      ", Figs. 4 and 5, and Appendices A and B of the associated supplementary materials unfortunately contain minor errors. This article serves to correct those errors.\n",
      "Distance: 0.0740\n",
      "\n",
      "Title: Correction to “A Novel Unbalance Compensation Method for Distribution Solid-State Transformer Based on Reduced Order Generalized Integrator”\n",
      "Abstract: 1. In page 108598, the title “IV ICOMPENSATION ABILITY ANALYSIS” should be “IV COMPENSATION ABILITY ANALYSIS”.\n",
      "Distance: 0.0743\n",
      "\n",
      "Title: Comments on “On Favorable Propagation in Massive MIMO Systems and Different Antenna Configurations”\n",
      "Abstract: It is shown that the condition of Theorem 1 in (X. Wu, N. C. Beaulieu, D. Liu, “On favorable propagation in massive MIMO systems and different antenna configurations,” IEEE Access, vol. 5, pp. 5578-5593, May 2017) never holds in practice and that Theorem 2 is incorrect under the stated condition. Extra assumptions or/and modifications are needed to make the conclusions of Theorem 1 and 2 above valid, which are provided below.\n",
      "Distance: 0.0768\n",
      "\n",
      "Title: IEEE Access Special Section: Radio Frequency Identification and Security Techniques\n",
      "Abstract: Radio Frequency Identification (RFID) systems have been receiving much attention in the last few decades due to their effective role in our everyday life. They propose different solutions to many vital applications. Moreover, RFID systems are the backbone of modern Internet-of-Things (IoT) and Near-Field Communication (NFC) systems. Extending the capacity of such systems and making them more secure is the desired objective of the research community.\n",
      "Distance: 0.0768\n",
      "\n",
      "Title: Trajectory Tracking With Constrained Sensors and Unreliable Communication Networks\n",
      "Abstract: This work investigates the remote vehicle tracking issue over constrained monitoring sensors and unreliable communication networks. A saturation function is used to describe the bounded time varying acceleration of the vehicle. A set of matrices are introduced to model the sensor monitoring conditions called captured states (CSs), and a Markov chain with time varying and partially unknown transition probability (TVPUTP) is proposed to analyze the conditions of the CSs. Then, a CS dependent nonfragile estimator is designed based on the measured unreliable vehicle information, and the estimation error system (EES) is derived. Two theorems are established to ensure that the EES satisfies the finite-horizon (FH) H\n",
      "∞\n",
      " performance. Finally, an example is introduced to show the effectiveness of the results.\n",
      "Distance: 0.0772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend_papers(\"deep learning for edge computing\", model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd8b34-d743-441f-8b39-6b8162e06224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
