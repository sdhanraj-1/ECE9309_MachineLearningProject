{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d664a8f3-deb0-47ee-96dd-4a094ba076f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steph\\miniforge3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "\n",
    "from helper import cosine_similarity_mean_median, compute_tfidf_similarity, compute_pearson_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60393e37-8f4c-4abc-866f-024d0d1d697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7745, 8), Validation: (2213, 8), Test: (1107, 8)\n"
     ]
    }
   ],
   "source": [
    "# train 70%, val 20%, test 10%\n",
    "#df = pd.read_csv(\"Research_Papers_Dataset_clean.csv\")\n",
    "df = pd.read_csv(\"database_clean.csv\")\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "#df = df.head(10000)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "X_val, X_test = train_test_split(X_temp, test_size=1/3, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0718d3bf-0782-450d-8a8b-ef1130b0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.data.iloc[idx][\"title\"])\n",
    "        abstract = str(self.data.iloc[idx][\"abstract\"])\n",
    "        input_text = title + \" \" + abstract  # Combine title and abstract\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"paper_index\": idx  # Used for retrieval\n",
    "        }\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset_train = PaperDataset(X_train, tokenizer, 256)\n",
    "dataset_val = PaperDataset(X_val, tokenizer, 256)\n",
    "dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f242b441-9990-4523-92cc-243e5fa7facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input IDs Shape: torch.Size([8, 256])\n",
      "Batch Attention Mask Shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Attention Mask Shape:\", batch[\"attention_mask\"].shape)\n",
    "    break  # Check only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50964c3e-9433-4a3a-8fa7-40b5575ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRecommender(nn.Module):\n",
    "    def __init__(self, model_name, embedding_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim  # Ensure this is correctly set\n",
    "\n",
    "        # Ensure projection layer matches `embedding_dim`\n",
    "        self.fc = nn.Linear(self.encoder.config.hidden_size, embedding_dim)  \n",
    "        \n",
    "        # Multiheaded attention layer using the correct `embedding_dim`\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = nn.functional.normalize\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(next(self.parameters()).device)\n",
    "        attention_mask = attention_mask.to(next(self.parameters()).device)\n",
    "\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "\n",
    "        embedding = self.fc(self.dropout(pooled_output))  # Apply projection\n",
    "        embedding = embedding.unsqueeze(0)  # Reshape for MultiheadAttention\n",
    "\n",
    "        # Multiheaded Attention\n",
    "        attn_output, _ = self.attention(embedding, embedding, embedding)\n",
    "        attn_output = attn_output.squeeze(0)  # Remove batch dim\n",
    "\n",
    "        return self.normalize(attn_output, p=2, dim=1)  # Normalize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3558e342-5329-4e21-b2fd-36a7a60540dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0315, 0.0706,  ..., 0.0540, 0.0357, 0.0396],\n",
      "        [0.0315, 1.0000, 0.0360,  ..., 0.0391, 0.0290, 0.0229],\n",
      "        [0.0706, 0.0360, 1.0000,  ..., 0.0709, 0.0375, 0.0412],\n",
      "        ...,\n",
      "        [0.0540, 0.0391, 0.0709,  ..., 1.0000, 0.0511, 0.0347],\n",
      "        [0.0357, 0.0290, 0.0375,  ..., 0.0511, 1.0000, 0.0244],\n",
      "        [0.0396, 0.0229, 0.0412,  ..., 0.0347, 0.0244, 1.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([[ True, False, False,  ..., False, False, False],\n",
      "        [False,  True, False,  ..., False, False, False],\n",
      "        [False, False,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True, False, False],\n",
      "        [False, False, False,  ..., False,  True, False],\n",
      "        [False, False, False,  ..., False, False,  True]], device='cuda:0')\n",
      "tensor([[False,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True, False,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True, False,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf_similarity_matrix(df):\n",
    "    corpus = df['title'] + \" \" + df['abstract']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "similarity_matrix_np = compute_tfidf_similarity_matrix(df)\n",
    "# Convert to tensor for GPU computation\n",
    "similarity_matrix = torch.tensor(similarity_matrix_np, dtype=torch.float32).to(\"cuda\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "threshold = 0.0785\n",
    "\n",
    "positive_pairs = similarity_matrix > threshold\n",
    "negative_pairs = ~positive_pairs\n",
    "print(positive_pairs)\n",
    "print(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b2aaef-c81f-4998-9ac3-44d3d2904eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAHECAYAAAAwOIA0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6U0lEQVR4nO3de1xUdf7H8fcMyE0kQVEstnLdAG94v+X9kprW5q12NbUyL6lpammZvzRrvWZmiamVZpZurmmlaRdr3SwjDbpoa5q22mqKoKKI3GTm/P4wZh1BhZmBGY6v5+PBAznne77zOV8O8uac75xjMQzDEAAAgAlZvV0AAABAaSHoAAAA0yLoAAAA0yLoAAAA0yLoAAAA0yLoAAAA0yLoAAAA0yLoAAAA0yLoAAAA0/L3dgG+aunSpfryyy/15ptvFqv9jh07NHjw4CLXRUdH67PPPvNkeQAAoBgIOkVYtWqVFixYoKZNmxZ7m0aNGunLL790Wvb9999rzJgxGjVqlKdLBAAAxUDQucjx48c1bdo07dixQzfffHOJtg0ICFBkZKTj66ysLM2aNUu9e/dW3759PVwpAAAoDuboXOTf//63KlSooA0bNqhBgwaF1m/dulV9+vRRfHy8brvtNi1YsEB5eXlF9rVkyRJlZ2fr8ccfL+2yAQDAZXBG5yKdOnVSp06dily3bds2jRs3TpMnT9att96q//73v3r22Wd18OBBvfjii05tT506pRUrVujRRx9V5cqVy6ByAABQFIJOMS1ZskT33HOP/vrXv0qSbrzxRk2fPl333Xefjhw5oujoaEfb1atXq1KlSvrLX/7irXIBAIAIOsW2Z88e7dq1S++8845jmWEYkqRffvnFKei899576tWrl4KCgsq8TgAA8D8EnWKy2+0aOnSoevfuXWjdxZOQ9+7dq8OHD+vOO+8sy/IAAEARmIxcTLfccosOHjyom266yfGRkpKiuXPn6ty5c452SUlJqlKliuLi4rxYLQAAkAg6xTZs2DB9/PHHSkhI0MGDB5WYmKjJkyfr7NmzTmd09uzZo9jYWC9WCgAACnDpqpi6d++uF154QUuXLtWSJUtUuXJlderUSY899phTu7S0NN5pBQCAj7AYBTNqAQAATIZLVwAAwLQIOgAAwLQIOgAAwLSYjKwLN/6z20tnqpLVaim1vvE/jHPZYJzLBuNcNhjnslFa42y1WmSxWK7ajqAjyW43dOrUuas3LCF/f6vCwysqIyNL+fl2j/ePCxjnssE4lw3GuWwwzmWjNMc5IqKi/PyuHnS4dAUAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEyLoAMAAEzL39sF4NpmtVpktVrc6sPPj7wOACgaQQdeY7VaVDk8RH5W94OK3bDLYnEvMAEAzIegA6+xWi3ys1r1j10fKC3zpMv9VAutqrvje7p9ZggAYD4EHXhdWuZJHT2b6noHnMkBAFwGkxsAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBpEXQAAIBp+Xu7AJRPVqtFVqvFrT78/MjZAIDSRdBBiVmtFlUOD5Gf1UNBxeJeYAIA4HIIOigxq9UiP6tV/9j1gdIyT7rczy2RNdX1lnYerAwAAGcEHbgsLfOkjp5NdXn7qhUjPFgNAACFMUkCAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYFkEHAACYlteDzunTpzV16lS1a9dOjRs3Vv/+/ZWUlHTZ9keOHNGIESPUuHFjtWnTRgsWLJDNZivDigEAQHnh9aAzYcIEfffdd5o/f77WrVun2rVr68EHH9R//vOfQm3Pnz+vBx98UJL09ttv6+mnn9bf//53LVq0qKzLBgAA5YBXg86vv/6q7du36+mnn1bTpk1Vs2ZNPfXUU6pWrZo2btxYqP3HH3+so0ePau7cuYqJiVGXLl00YcIEvfHGG8rLy/PCHgAAAF/m1aATHh6uV155RfXr13css1gsslgsysjIKNQ+KSlJdevW1XXXXedY1rJlS2VmZuqnn34qk5oBAED54e/NFw8LC1P79u2dln388cf69ddf9eSTTxZqn5KSoqioKKdl1apVkyQdO3ZMDRo0cLkWf3/PZz4/P6vTZ7Nw7M/vodRlv29rscitfgo2tVotpfJ9xAVmPZ59DeNcNhjnsuEL4+zVoHOpb7/9VpMnT1bXrl3VoUOHQutzcnIUFhbmtCwwMFCSlJub6/LrWq0WhYdXdHn7qwkLCy61vr3J399PFSr4ub694wfAvX78/C5sGxoa5HIfKD6zHs++hnEuG4xz2fDmOPtM0Pn000/12GOPqXHjxpo3b16RbYKCggrNxSkIOCEhIS6/tt1uKCMjy+XtL8fPz6qwsGBlZGTLZrN7vH9vKdiv/Hybzp93/R1v+b+Pic3mXj8F77rLzMxxqx9cmVmPZ1/DOJcNxrlslOY4h4UFF+tMkU8EnbfeekszZsxQ9+7dNWfOHAUEBBTZLioqSj///LPTstTUVElS9erV3aohP7/0DnSbzV6q/XuNYcgwDLe2L/jkTj8Fm9rthjnH2ceY9nj2MYxz2WCcy4Y3x9nrFydXr16tZ599Vvfee6/mz59/2ZAjSc2aNdOePXuUmZnpWPb111+rYsWKiouLK4tyAQBAOeLVoHPw4EHNnDlTt912m0aMGKETJ04oLS1NaWlpOnv2rPLy8pSWlua4XNWlSxdFRkZq3Lhx2rt3rz799FPNnz9fQ4YMuWJAAgAA1yavXrr6+OOPdf78eW3ZskVbtmxxWte7d2/17t1bgwcP1sqVK9WiRQsFBgbqtdde0/Tp03XPPffouuuu04ABAzRq1Cgv7QEAAPBlXg06Dz30kB566KErttm3b5/T1zfddJOWL19emmUBAACT8PocHQAAgNJC0AEAAKZF0AEAAKZF0AEAAKZF0AEAAKZF0AEAAKblE4+AADzBE08vt9sN2e1uPNYCAOBTCDoo90IDKspu2D3y9HKb3a7T6VmEHQAwCYIOyr2gCoGyWqxau3uTUs+ecLmfyNAquif+DlmtFoIOAJgEQQemkXbupI6eTfV2GQAAH8JkZAAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFo+FXSWLl2qQYMGXbHNhg0bFBsbW+jjyJEjZVQlAAAoL/y9XUCBVatWacGCBWratOkV2+3bt0/NmzfX/PnznZZHRESUZnkAAKAc8nrQOX78uKZNm6YdO3bo5ptvvmr7n3/+WbGxsYqMjCz94gAAQLnm9UtX//73v1WhQgVt2LBBDRo0uGr7ffv2qVatWmVQGQAAKO+8fkanU6dO6tSpU7HanjlzRsePH1dSUpJWr16t9PR0xcfHa+LEiapZs6Zbdfj7ez7z+flZnT6bhWN/LBZZLBbXO/p9W4tFbvVjcXz2TD1m+355ilmPZ1/DOJcNxrls+MI4ez3olMT+/fslSYZhaNasWcrJydHixYs1YMAAbdy4UVWrVnWpX6vVovDwip4s1UlYWHCp9e1N/v5+qlDBz/XtHT8A7vVT8ANk9bO6V4//hW3N+v3yFManbDDOZYNxLhveHOdyFXSaNm2qxMREhYeHO/5yT0hIUIcOHbR+/XoNHz7cpX7tdkMZGVmeLFXShV/AYWHBysjIls1m93j/3lKwX/n5Np0/b3O5n/zfx8Rmc6+fgrG12+zu1ZN/YVuzfb88xazHs69hnMsG41w2SnOcw8KCi3WmqFwFHanwu6uCg4MVHR2t48ePu9Vvfn7pHeg2m71U+/caw5BhGG5tX/DJnX4Mx2fP1GPa75eHMD5lg3EuG4xz2fDmOJeri5Nr1qxRixYtlJX1v7MvmZmZOnTokP70pz95sTIAAOCLfDro2Gw2paWlKScnR5LUrl072e12TZo0Sfv379fu3bs1ZswYRUREqE+fPl6uFgAA+BqfDjrHjh1TmzZttHnzZklSjRo1tGLFCmVlZal///66//77ValSJa1cuVKBgYFerhYAAPgan5qjM3v2bKevo6OjtW/fPqdldevW1fLly8uyLAAAUE759BkdAAAAdxB0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAafnU28sBX+CJp+za7YbsdjceRwEA8AiCDvC70ICKsht2jzxl12a363R6FmEHALyMoAP8LqhCoKwWq9bu2qTUzBMu9xMZWkX3xN8hq9VC0AEALyPoAJdIPXdSR8+mersMAIAHMBkZAACYFmd0rjFWq0VWq8WtPjwxWRcAgLJA0LmGWK0WVQ4PkZ/VQ0HF4l5gAgCgtBF0riFWq0V+Vqv+sesDpWWedLmfWyJrqust7TxYGQAApYOgcw1Ky3Rvsm3VihEerAYAgNLDZAsAAGBaBB0AAGBaBB0AAGBaBB0AAGBaBB0AAGBaBB0AAGBaBB0AAGBaBB0AAGBapRJ0UlJSSqNbAACAEnEp6NSuXVu7du0qcl1SUpJuv/12t4oCAADwhGI/AmL58uXKysqSJBmGobVr12rbtm2F2n333XcKCAjwXIUAAAAuKnbQyc3NVUJCgiTJYrFo7dq1hdpYrVZVqlRJI0eO9FyFAAAALip20Bk5cqQjwMTFxekf//iH4uPjS60wAAAAd7n09PK9e/d6ug4AAACPcynoSNL27du1detWZWdny263O62zWCyaOXOm28UBAAC4w6Wgs3z5cs2dO1eBgYGKiIiQxWJxWn/p1wAAAN7gUtB56623dOedd2rGjBm8wwoAAPgsl+6jc+LECfXr14+QAwAAfJpLQadOnTrav3+/p2sBAADwKJcuXT355JMaN26cQkJC1KBBAwUHBxdqc/3117tdHAAAgDtcCjr9+/eX3W7Xk08+edmJxz/99JNbhQEAALjLpaDz7LPP8s4qAADg81wKOn369PF0HQAAAB7nUtD55ptvrtqmWbNmrnQNAADgMS4FnUGDBsliscgwDMeySy9lMUcHAAB4m0tBZ+XKlYWWZWVlKSkpSe+//74WLlzodmEAAADucinoNG/evMjlHTp0UEhIiBYvXqylS5e6VRgAAIC7XLph4JU0bdpUO3fu9HS3AAAAJebxoPPPf/5TFStW9HS3AAAAJebSpavBgwcXWma325WSkqLffvtNw4YNc7swAAAAd7kUdC5+t1UBq9WqmJgYjRgxQn379nW7MAAAAHe5FHTefPNNT9cBAADgcS4FnQLbtm3Tzp07lZGRoYiICDVp0kRt27b1VG0AAABucSno5OXladSoUfryyy/l5+en8PBwpaena+nSpWrZsqWWLl2qgIAAT9cKAABQIi6962rhwoVKTk7W3LlztWvXLn355Zf64YcfNGvWLH3//fdavHixp+sEAAAoMZeCzgcffKCHH35Yf/7zn+Xn5ydJ8vf3V69evfTwww9r48aNHi0SAADAFS4FnVOnTqlOnTpFrqtTp46OHz/uVlEAAACe4FLQufHGG5WcnFzkum+++UY1atRwqygAAABPcGky8l//+lfNnj1bQUFB6tmzp6pWraoTJ07ogw8+0KuvvqqHH37Y03UCAACUmEtBp3///tqzZ4/mzZun559/3rHcMAz17t1bw4cP91iBAAAArnL57eUzZszQkCFDtHPnTp05c0YWi0VdunRRrVq1PF0jAACAS0o0R2ffvn3q27evXn/9dUlSrVq11L9/fw0YMEAvvviiJkyYoIMHD5ZKoQAAACVV7KBz5MgRDR48WCdOnFDNmjWd1lWoUEGTJk3S6dOnNWDAAN51BQAAfEKxg84rr7yiypUr691331X37t2d1gUHB+v+++/XO++8o8DAQC1dutTjhQIAAJRUsYNOYmKihg4dqoiIiMu2iYyM1JAhQ7R9+3aPFAcAAOCOYged1NRU3XzzzVdtFxMTo5SUFHdqAgAA8IhiB52IiAilpqZetV16erquu+46t4oCAADwhGIHnWbNmmn9+vVXbffee+9d9vEQAAAAZanYQWfQoEHasWOHZs+erdzc3ELr8/LyNHfuXG3btk333nuvR4sEAABwRbFvGFi/fn1NnjxZM2fO1Pvvv69WrVopOjpaNptNR48e1Y4dO5Senq5HHnlEbdu2Lc2aAQAAiqVEd0a+9957FRcXp2XLlumzzz5znNmpWLGi2rRpoyFDhqhBgwalUigAAEBJlfgREE2aNFGTJk0kSadOnZK/v7/CwsI8XhgAAIC7SvQIiEtFRER4NOQsXbpUgwYNumKb9PR0Pfroo2rWrJmaN2+u6dOnKzs722M1AAAA83DpoZ6lYdWqVVqwYIGaNm16xXZjx45Vdna2VqxYoYyMDE2ZMkVZWVmaM2dOGVUKAADKC68HnePHj2vatGnasWPHVW9I+N1332nnzp3avHmz4ynpzzzzjIYOHaoJEyaoevXqZVAxAAAoL7wedP7973+rQoUK2rBhgxYtWqTffvvtsm2TkpIUGRnpCDmS1Lx5c1ksFiUnJ6tHjx4u1+Hv79ZVvCL5+VmdPnubow6LRRaLxfWOft/WYpFP9GNxfPat/fKV77un+NrxbFaMc9lgnMuGL4yz14NOp06d1KlTp2K1PX78uGrUqOG0LCAgQJUrV9axY8dcrsFqtSg8vKLL219NWFhwqfXtCn9/P1Wo4Of69o4D1zf6KfgBsvpZfaIef/8L2/ra991TzLpfvoZxLhuMc9nw5jh7PeiURHZ2tgICAgotDwwMLPImhsVltxvKyMhyp7Qi+flZFRYWrIyMbNlsdo/372o9+fk2nT9vc7mf/N/3xWbzjX4KxtZus/tEPfn5F7b1le+7p/ja8WxWjHPZYJzLRmmOc1hYcLHOFJWroBMUFKS8vLxCy3NzcxUSEuJW3/n5pXeg22z2Uu2/xAxDhmG4tX3BJ1/ox3B89q398rnvu4eYdb98DeNcNhjnsuHNcS5XFyejoqIKPVg0Ly9Pp0+fVrVq1bxUFQAA8FXlKug0a9ZMKSkp+vXXXx3Ldu7cKUmOmxgCAAAU8OmgY7PZlJaWppycHElSgwYN1LhxY40fP167du3S119/ralTp6pXr168tRwAABTi00Hn2LFjatOmjTZv3izpwlt+ExISFB0drfvuu0/jxo1Tu3bt9PTTT3u3UAAA4JN8ajLy7Nmznb6Ojo7Wvn37nJZVqVJFL730UlmWBbjEE/eNsNsN2e1uTIwGgGucTwUdwAxCAyrKbtg9ct8Im92u0+lZhB0AcBFBB/CwoAqBslqsWrtrk1IzT7jcT2RoFd0Tf4esVgtBBwBcRNABSknquZM6ejb16g0BAKXGpycjAwAAuIOgAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATMvf2wUAuDI/P/f/HrHbDdnthgeqAYDyhaAD+KjQgIqyG3aFhQW73ZfNbtfp9CzCDoBrDkEH8FFBFQJltVi1dtcmpWaecLmfyNAquif+DlmtFoIOgGsOQQfwcannTuro2VRvlwEA5RKTkQEAgGkRdAAAgGkRdAAAgGkRdAAAgGkRdAAAgGkRdAAAgGkRdAAAgGkRdAAAgGkRdAAAgGl5PejY7Xa99NJLatu2rRo2bKhhw4bp8OHDl22/YcMGxcbGFvo4cuRIGVYNAADKA68/AuLll1/W6tWrNXv2bEVFRem5557T0KFDtXHjRgUEBBRqv2/fPjVv3lzz5893Wh4REVFWJQMAgHLCq2d08vLytHz5co0dO1YdOnRQXFycXnjhBaWkpOiTTz4pcpuff/5ZsbGxioyMdPrw8/Mr4+oBAICv82rQ2bt3r86dO6dWrVo5loWFhalOnTr65ptvitxm3759qlWrVlmVCAAAyjGvXrpKSUmRJNWoUcNpebVq1RzrLnbmzBkdP35cSUlJWr16tdLT0xUfH6+JEyeqZs2abtXi7+/5zOfnZ3X67G2OOiwWWSwW1zv6fVuLRT7Rj8Xx2Vz75el+3D0Ofe14NivGuWwwzmXDF8bZq0EnOztbkgrNxQkMDNSZM2cKtd+/f78kyTAMzZo1Szk5OVq8eLEGDBigjRs3qmrVqi7VYbVaFB5e0aVtiyMsLLjU+naFv7+fKlRw/VKfv+PA9Y1+Cn6ArH5Wn6jH5/rxv7Ctp45DXzuezYpxLhuMc9nw5jh7NegEBQVJujBXp+DfkpSbm6vg4MKD0rRpUyUmJio8PNzxF25CQoI6dOig9evXa/jw4S7VYbcbysjIcmnbK/HzsyosLFgZGdmy2ewu92OxWBQWFiSr1TOJOD/fpvPnba5v//u+2Gy+0U/B2Nptdp+ox+f6yb+wrbvHoaeOZ1wZ41w2GOeyUZrjHBYWXKwzRV4NOgWXrFJTU3XjjTc6lqempio2NrbIbS59d1VwcLCio6N1/Phxt2rJzy+9A91ms7vVv7+/VVarVf/Y9YHSMk+63M8tkTXV9ZZ2ki6cFXPZ79sahm/0Yzg+Gz5Rj6/24+5xWMBT/eDKGOeywTiXDW+Os1eDTlxcnEJDQ7Vjxw5H0MnIyNCePXs0cODAQu3XrFmj+fPna+vWrQoJCZEkZWZm6tChQ+rXr1+Z1u4NaZkndfRsqsvbV63IW/ABANcWr87CCggI0MCBAzVv3jx99tln2rt3r8aPH6+oqCh17dpVNptNaWlpysnJkSS1a9dOdrtdkyZN0v79+7V7926NGTNGERER6tOnjzd3BQAA+CCvTzcfO3as+vXrp//7v/9T//795efnp2XLlqlChQo6duyY2rRpo82bN0u6cKlrxYoVysrKUv/+/XX//ferUqVKWrlypQIDA728JwAAwNd4/c7Ifn5+mjhxoiZOnFhoXXR0tPbt2+e0rG7dulq+fHlZlQcAAMoxr5/RAQAAKC0EHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFpef6gngLLh5+fe3zXubg8A3kDQAUwuNKCi7IZdYWHBbvdlN+yyWCweqAoAygZBBzC5oAqBslqsWrtrk1IzT7jcT7XQqro7vqesVoIOgPKDoANcI1LPndTRs6mud8CZHADlEBfdAQCAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAaRF0AACAafl7uwAA5YvVapG/v3t/I9nthux2w0MVAcDlEXQAFEtoQEXZDbtCQ4Pc7stmt+t0ehZhB0CpI+gAKJagCoGyWqxau3uTUs+ecLmfyNAquif+DlmtFoIOgFJH0AFQImnnTuro2VRvlwEAxcJkZAAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFoEHQAAYFrcMLAM+Pm5lyfd3R4AgGsVQacUWSwW2Q27wsKCPdWhZ/oBAOAaQdApRVar5cKzgXZtUmqm688GuiWyprre0s6DlQHe54kzlTwFHcDVEHTKQKqbzwaqWjHCg9UA3lXwFHRPnOnkKegAroagA6BMOZ6C7uaZTp6CDqA4CDoAvMLdM50AUBy8nQcAAJgWQQcAAJgWQQcAAJgWQQcAAJgWk5EBlGvcjwfAlRB0AJRL3I8HQHEQdACUS9yPB0BxEHQAlGvcjwfAlTAZGQAAmBZndABATGoGzIqgA+CaxqRmwNwIOgCuaUxqBsyNoAMA8tykZncvgXniEhqA//F60LHb7UpISNDatWt19uxZNWvWTFOnTtUf/vCHItunp6frb3/7m7Zt2yaLxaKePXtq0qRJCg52/7QzALjKk5fA7IZdFovFA1UB8HrQefnll7V69WrNnj1bUVFReu655zR06FBt3LhRAQEBhdqPHTtW2dnZWrFihTIyMjRlyhRlZWVpzpw5XqgeAC7w1CWwaqFVdXd8T/n7W2UY7l0CY3I04OWgk5eXp+XLl+uxxx5Thw4dJEkvvPCC2rZtq08++UR33HGHU/vvvvtOO3fu1ObNm1WrVi1J0jPPPKOhQ4dqwoQJql69elnvAgA4cfcSWGhgqOyGXaGhQW7XYrPbdTYjh8CEa5pXg87evXt17tw5tWrVyrEsLCxMderU0TfffFMo6CQlJSkyMtIRciSpefPmslgsSk5OVo8ePcqsdgAoDY4zQ7s3KfWs62eGbgq/QT3iOqly5RC3a/JUYPIlzIW6dlgMLx65n3zyicaMGaMffvhBQUH/++vlkUceUU5OjpYuXerU/m9/+5t++OEHrV271ml5q1atNHToUD344IMu1WEYpfPXisUiWa1WZeZlyWa3udxPBT9/hVQIph/6oR/6KXE/2edzZDfsLvfjZ/FToH+AR+YMGYbhc/1c+A1ongDni6xWq+x2uzydNqxWS7GOA6+e0cnOzpakQnNxAgMDdebMmSLbFzVvJzAwULm5uS7XYbFY5OdXehP/QgPc/4uKfuiHfujHFcEV3L8E5imemmDtyX4udMXE79JmtXrvDJpXz90VnMXJy8tzWp6bm1vku6iCgoIKtS1oHxLimf8UAACAeXg16NSoUUOSlJrqPHEvNTW1yInFUVFRhdrm5eXp9OnTqlatWukVCgAAyiWvBp24uDiFhoZqx44djmUZGRnas2ePmjVrVqh9s2bNlJKSol9//dWxbOfOnZKkJk2alH7BAACgXPHqHJ2AgAANHDhQ8+bNU0REhG644QY999xzioqKUteuXWWz2XTq1ClVqlRJQUFBatCggRo3bqzx48fr6aefVlZWlqZOnapevXrx1nIAAFCIV991JUk2m03z58/X+vXrlZOT47gzcnR0tI4cOaLOnTtr1qxZ6tOnjyTp5MmTmj59ur744gsFBgaqe/fumjx5sgIDA725GwAAwAd5PegAAACUFu6YBAAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugU0x2u10vvfSS2rZtq4YNG2rYsGE6fPjwZdunp6fr0UcfVbNmzdS8eXNNnz7d8bT2Ah9++KF69Oih+Ph49erVS4mJiaW9Gz6vNMa5a9euio2Ndfp44oknSntXfF5Jx/ri7YYOHaqFCxcWWscxXVhpjDPHdGElHef9+/dr+PDhatGihVq1aqWxY8fq6NGjTm1WrVqlzp07Kz4+XgMGDNCePXtKezd8nqfH2WazKT4+vtDxXNRx7zIDxbJw4UKjRYsWxtatW42ffvrJGDJkiNG1a1cjNze3yPYDBw40+vbta/z444/GV199ZXTs2NGYNGmSY31iYqJRt25d44033jAOHDhgzJ4926hXr55x4MCBstoln+TpcT537pwRFxdnbN261UhNTXV8ZGRklNUu+aySjrVhGEZubq7x+OOPGzExMcZLL73ktI5jumieHmeO6aKVZJxPnTpltG7d2hgzZoyxb98+Y/fu3ca9995r3H777UZOTo5hGIaxfv16Iz4+3nj//feN/fv3GxMnTjSaN29unDx5sqx3zad4epwPHDhgxMTEGD/99JPT8ZyZmemxmgk6xZCbm2s0atTIWLVqlWPZmTNnjPj4eGPjxo2F2n/77bdGTEyM03/wX3zxhREbG2ukpKQYhmEYQ4YMMR555BGn7f7yl78YTz31VOnsRDlQGuP8ww8/GDExMcbp06dLfwfKkZKOtWEYRnJystGzZ0+jc+fORtOmTQv9AuaYLqw0xpljurCSjvM//vEPo1GjRkZ2drZj2dGjR42YmBjjq6++MgzDMLp27WrMnTvXsf78+fNG+/btjSVLlpTinvi20hjnTZs2GY0bNy7Vurl0VQx79+7VuXPn1KpVK8eysLAw1alTR998802h9klJSYqMjFStWrUcy5o3by6LxaLk5GTZ7XZ9++23Tv1JUosWLYrs71rh6XGWpH379qlq1aq67rrrSn8HypGSjrUkff7552rbtq3ee+89VapUyWkdx3TRPD3OEsd0UUo6zq1atdLLL7+soKAgxzKr9cKvw4yMDJ08eVKHDh1y6s/f319NmzblePbgOEsXjueL/w8vDV59enl5kZKSIkmqUaOG0/Jq1ao51l3s+PHjhdoGBASocuXKOnbsmDIyMpSVlaWoqKhi9Xet8PQ4Sxd+iEJCQjR27Fh9++23Cg8PV9++fTV48GDHD9y1qKRjLUnjx4+/bH8c00Xz9DhLHNNFKek4R0dHKzo62mnZK6+8oqCgIDVr1szx/0dR/e3du9eTpZcrnh5nSfr555+Vn5+vBx98UHv37lX16tV133336a677vJY3dfmT0UJFUxuDQgIcFoeGBio3NzcIttf2vbi9jk5OSXq71rh6XGWLkyEy8jIULdu3bRs2TL1799fL774omcnupVDJR3rq+GYLpqnx1nimC6Ku+P85ptv6q233tJjjz2miIiIUvm+mYGnx1m6cDyfPn1agwYN0rJly9StWzdNnjxZ77zzjsfq5oxOMRScdsvLy3M6BZebm6vg4OAi2+fl5RVanpubq5CQEAUGBjr6u3R9Uf1dKzw9zpL06quvKjc313EJIDY2VpmZmVq8eLHGjBlzzf4FXNKxvhqO6aJ5epwljumiuDrOhmHoxRdf1OLFizVy5EgNGjSoUH8X43j27DhL0gcffCCbzaaKFStKkuLi4nT06FEtW7ZM/fr180jd195PhAsKTtOlpqY6LU9NTVX16tULtY+KiirUNi8vT6dPn1a1atVUuXJlhYSEFLu/a4Wnx1m68JfHpfMcYmJilJWVpTNnzniy/HKlpGN9NRzTRfP0OEsc00VxZZzPnz+viRMnasmSJZo8ebLGjRvnVn/XAk+Ps3QhPBWEnAIxMTEeveRN0CmGuLg4hYaGaseOHY5lGRkZ2rNnj+M648WaNWumlJQU/frrr45lO3fulCQ1adJEFotFjRs3diwrsGPHDjVt2rSU9sL3eXqcDcNQly5dlJCQ4LTd7t27FRkZqfDw8FLaE99X0rG+Go7ponl6nDmmi+bKOE+aNEkfffSRnn/+ed1///1O66pUqaKaNWs69Zefn6+kpCSXvm9m4elxzsjIUPPmzbV+/Xqn5bt379Ytt9zisbq5dFUMAQEBGjhwoObNm6eIiAjdcMMNeu655xQVFaWuXbvKZrPp1KlTqlSpkoKCgtSgQQM1btxY48eP19NPP62srCxNnTpVvXr1cqTeBx54QMOHD1edOnXUrl07rVu3Tj/99JNmzJjh5b31ntIY59tuu03Lli3TH//4R9WrV0+JiYl67bXXNGXKFC/vrXeVdKyLg2O6ME+Ps8Vi4ZguQknHef369dq8ebMmTZqk5s2bKy0tzdFXQZshQ4ZoxowZuummm1S/fn298sorysnJ8djllPLI0+McFhamli1b6oUXXlCVKlV000036ZNPPtGGDRu0dOlSzxVeqm9eN5H8/Hxj7ty5RsuWLY2GDRsaw4YNMw4fPmwYhmEcPnzYiImJMdatW+dof+LECWPMmDFGw4YNjRYtWhjTpk1z3CCpwLvvvmvcdtttRv369Y3evXs77itwLfP0OJ8/f95ISEgwOnfubNStW9fo1q2bsWbNmjLfL19U0rG+WMeOHQvd38UwOKaL4ulx5pguWknG+YEHHjBiYmKK/Lj4e/Haa68Z7dq1M+Lj440BAwYYe/bs8cq++RJPj/PZs2eNmTNnGu3btzfq1atn3HXXXcaWLVs8WrPFMAzDc7EJAADAdzBHBwAAmBZBBwAAmBZBBwAAmBZBBwAAmBZBBwAAmBZBBwAAmBZBBwAAmBZBB0Cx+Pott3y9vpLwhX3xhRoATyDoAC5avny5HnvsMcfX+fn5WrFihXr37q2GDRuqUaNG6t27t5YvX+70FOQdO3YoNjbW6Xkxrli4cKFiY2MdX3fq1ElPPPGEW31KhetLSUnR8OHD9dtvv7nV76BBgxQbG6u//vWvl20zfvx4xcbGlng/kpOTNXz48Ku2u3TMXPHEE08oNjbW8REXF6eGDRvqzjvvVEJCgnJycpzaDxo0yOlpzVfj6r6U9HUuJy8vTzNnztTGjRsdy5544gl16tTJ7b4vlZiYqLvuukvnz5/3eN9AAZ51Bbjgl19+0dKlS7VhwwbHsqeeekqffPKJhg8frnr16slutyspKUkLFixQcnKyFi1aJEmqW7eu1qxZoz/96U9u1XD33Xerbdu2bvVRlEvr++qrr/T55597pG+r1arvv/9eKSkpioqKclqXlZWlrVu3utTv2rVr9csvv1y1nafGLDIy0vFgTbvdrrNnzyopKUlLly7Vl19+qTfeeEOBgYGSpGnTppWo77Lel0ulpqbqjTfe0KxZsxzLRo0apcGDB3v8tVq1aqUbbrhBL7/8sh555BGP9w9IBB3AJc8995zuuOMOx8NDjx49qnfffVfPPPOM7rnnHke7tm3bKiIiQjNnztSuXbsUHx+v0NBQNWzY0O0aoqKiCoUFT/BUfUWpU6eODhw4oI8++qjQk4y3bt2q4OBghYWFlcprS54bs4CAgEJj1L59ezVo0ECjR4/W8uXLNXLkSElyO9BeTml9/4ty4403llrfI0eO1IABA9S/f39Vq1at1F4H1y4uXQEl9PPPP+tf//qX7rjjDseyEydOyDAM2e32Qu3vvPNOTZgwwfEL/NJLQwsXLlT37t21ZcsW3XHHHapfv77uuusufffdd/r+++919913Kz4+XnfccYcSExMd/V7tMsyRI0c0adIktWnTRnXr1lWrVq00adIkpaenO9p06tRJM2fO1H333af4+HhNmTLFqb7169dr8uTJkqTOnTvriSee0Jw5cxQfH6+zZ886vd7LL7+sJk2aKDs7+7I1hYSEqH379vroo48Krdu8ebO6desmf3/nv79OnTql6dOnq2PHjqpXr56aN2+u0aNH68iRI5IuXFZ599139dtvvyk2Nlbr16/XkSNHFBsbq9dff13du3dXgwYNtG7dOqcx+/HHH1W3bl2ny2QnT55Uq1at9MADD7g0R6VLly5q2LCh3n77bceySy8pbd++Xffcc48aNWqkZs2aaeTIkY4zOK7uy8UWLVqkW2+9VY0aNdKoUaN0+PBhx7qiLkEV9F/wWp07d5YkTZ482dH20u1sNptWrVqlO++8U/Hx8erQoYPmzZun3Nxcp9e6//77tW7dOnXr1k316tXTXXfdpW3btjm9fv369XX99dfr9ddfL/F4A8VB0AFKaOPGjYqMjHT6iz4uLk41atTQrFmzNH36dG3btk2ZmZmSpIiICI0YMUI333zzZftMSUnR7Nmz9dBDD+nFF19URkaGxo4dqwkTJujuu+/WokWLZBiGxo8fX2gOSFGys7M1ePBg/fLLL5o2bZqWLVumwYMHa9OmTXrhhRec2q5atUr169fXyy+/rH79+jmt69Chg+PMREJCgkaNGqV+/fopNze3UFh5//331aNHDwUHB1+xth49ejguXxXIzMzUtm3bnMKjdGFC7IgRI7R9+3Y99thjWrZsmR5++GElJiY6LgmNGjVK7du3V2RkpNasWaMOHTo4tl+4cKGGDRumuXPnqnXr1k5916tXT8OGDdO7777rCJBTp06V3W7X7NmzZbFYrrgfl9O6dWulpKQUOafp8OHDGjVqlOrVq6fFixdrxowZOnjwoIYPHy673e7yvhRITk7Wpk2bNHXqVP3tb3/T3r17NXjwYMexeDXVqlVzXJIbOXKk49+Xmjp1qmbNmqUuXbpo8eLFuvfee/XWW29p1KhRTgHxxx9/1LJlyzR27FgtWrRIfn5+GjNmjM6cOePUX/fu3fXBBx8Uq0agpLh0BZTQ119/rfr16zv9IgwICNArr7yiSZMmafXq1Vq9erWsVqvq1q2r22+/Xffee6+CgoIu22d2dramTZumdu3aSZIOHDig559/XjNmzHCEj6ysLI0dO1YHDx5U7dq1r1jjoUOHFBUVpTlz5ugPf/iDJKlly5b64YcftHPnTqe2119/vdOk6osnSUdERDguW9SuXVvR0dGSpEaNGun999/X3XffLUn69ttvdejQIc2ePfvKg6cL4Sk4ONjp8tWWLVtUpUoVNWnSxKltamqqgoOD9fjjj6tp06aSpBYtWui///2v1qxZI+nCZZWIiAiny0lZWVmSpNtvv119+/a9bC2jR4/WP//5T02fPl3Dhw/Xp59+qhdffNFxSdIVVatWlXThLN8NN9zgtG7Xrl3KycnRiBEjHK8RFRWlzz77TFlZWW7tiyT5+flp+fLljktaf/zjH9WrVy+99957Gjhw4FVrDwgIcBxbN954o+rUqVOozYEDB/TOO+/o0UcfdUyabt26tapVq6ZJkyZp27Ztat++vSTp7NmzWr9+veMYCgkJ0cCBA/X111+rW7dujj7r16+vJUuW6JdfflGtWrWuWidQEpzRAUro8OHDjl/4F4uJidF7772nd955R+PGjVOLFi20f/9+zZ07V71799apU6eu2G/jxo0d/y74ZdmgQQPHssqVK0uSMjIyrlpj7dq1tXr1at1www06dOiQPv/8cy1btkz/+c9/nN4BVtC2pPr27aukpCTHWYt3331XNWvWVKNGja66bVBQkDp16uR0RmjTpk26/fbbC51FqV69ulauXKkmTZroyJEj2r59u9588019++23hfajKFfbtwoVKmjOnDk6cuSIpkyZot69e6t79+5X7fdKCs5oFHVGqEGDBgoMDFS/fv00Y8YMffHFF4qLi9P48eMVGhp6xX6L831q3Lix07yd2rVr6w9/+IO++eabEu7F5RUE5Z49ezot79mzp/z8/C4blCU5arv08mbBz1PB5UjAkwg6QAllZmZe8fJM/fr1NXLkSK1YsUJff/21xo4dq//85z969dVXr9hvUb/ornYZ6Epef/11tWrVSt26ddOTTz6pnTt3FtlfSEhIifsuuET1/vvvKzc3Vx9++KH69OlT7O1vv/12x+Wr9PR0JSYmFvrFWWDDhg3q2LGjOnfurAkTJuizzz674tmxixVn32rXrq3Y2FjZ7XZ17Nix2PtwOcePH5ekIs8KRUdH66233lKDBg30zjvvaOjQoWrdurVeeOGFq84JKs6+FATki1WpUqVY4bi4Ci47RUZGOi339/dXeHi409ytS4+3gvB36Vy2gnaXzvsCPIGgA5RQ5cqVC/2HPGfOnCLPBAQHB2v06NGKi4vTgQMHyqpEbdy4UbNnz9awYcOUmJio7du3a+nSpVecJ1QSFStWVPfu3fXhhx/qiy++UFZWlu66665ib9+uXTtVrFhRH330kbZs2aLo6GjVq1evULukpCQ9/vjj6tq1q7Zt26YdO3ZoxYoVHn1X2Jo1a/Tjjz8qLi5OM2bMcDsUfPXVV7rpppsue/krPj5eCQkJjn1p3bq1lixZUuQE7ZK6dO6LJKWlpSkiIkLShaBhs9mc1hdcGiuu6667ztHvxc6fP6/09HSFh4eXqD/pf3W7si1wNQQdoIRuuOEGHTt2zGlZzZo1dfDgQW3evLlQ+3Pnzik1NVUxMTFlVaKSk5MVFhamoUOHOn7JnTt3TsnJyUW+M+xKrNai/5vo16+ffv75Z73xxhu69dZbSzSvJSAgQF26dNHHH3+sDz/88LJnc7777jvZ7XaNGTPG0b/NZtNXX30l6X9nBi5X49X89ttvmjNnjvr166clS5bo7NmzmjFjhkt9SdK//vUv7d69W/379y9y/YoVK9SxY0fl5eUpICBArVq10rPPPivpwi0KJNf3Rbrwfb84hP/www/67bff1LJlS0kXAmp6errTu6OSk5Od+vDz87viazRv3lzShcuNF9u0aZNsNluheVbFUXAW7Prrry/xtsDVMBkZKKHWrVtr9erVMgzDcSq+V69e2rhxoyZNmqQdO3aoffv2CgsL06FDh7Ry5UoFBQVpyJAhZVZjfHy8/v73v2v27Nnq2LGjUlNTtWzZMp04ccLxF3lxFbwtfsuWLWrXrp1jsmiTJk1Us2ZN7dy5s9A7uYqjR48eGjFihKxWq/7v//7vsvshSc8884z69u2rM2fOaNWqVdq7d6+kC2cjQkNDFRYWphMnTujzzz8v9pwjwzA0ZcoUBQcHa9KkSbruuus0btw4zZw5U926dbvinYDz8vL0/fffO/rJyMhQUlKSVq5cqRYtWlx24m/Lli01b948jR49WgMHDpSfn5/efvttBQQEOC6bubIvBex2u4YPH66HHnpI6enpev755xUTE6M///nPkqSOHTvqzTff1JQpUxxB9fXXX3cKN5UqVZJ04a7FtWrVcponJl24L1Dv3r310ksvKTs7W82aNdNPP/2khIQEtWjRwqWbGCYnJys6Olo1a9Ys8bbA1RB0gBLq2rWrFi1apF27djl+CQQEBGjZsmVauXKlPvroI23atEk5OTmqVq2aOnXqpJEjR6pKlSplVmPv3r115MgRrVu3TqtXr1b16tXVvn17DRgwQE899VSJ3t3SokUL3XrrrXr++eeVmJioV155xbGuQ4cOOnXqlLp06VLiGm+99VaFhYWpRo0al62lRYsWmjp1ql5//XV99NFHqlq1qlq0aKGEhASNHj1aycnJat++vfr06aPPP/9co0eP1tixY9WjR4+rvv7q1auVmJioBQsWOMLfoEGDtHHjRk2dOlWNGzd2TAC/VFpamv7yl784vg4JCVHNmjU1duxYDRo0SBUqVChyu7i4OC1ZskSLFi3ShAkTZLPZVK9ePS1fvlx//OMfJcmlfSnQpUsXXX/99Zo4caLy8/PVsWNHTZkyxXGX5tatW+vxxx/Xm2++qY8//lh169ZVQkKC02M5QkND9cADD2jNmjX6/PPPtX379kKvM2PGDN10001at26dXn31VVWrVk2DBw/WqFGjXDoj9cUXX7g9CRy4HIvBk9uAEnvooYcUHh7udJv8a41hGOrZs6fatGmjJ5980tvloJxKSkrSkCFD9Omnn3JnZJQK5ugALhg/frw++eQTx7yKa0lmZqYSEhL00EMP6fDhwx55kCSuXa+99pruu+8+Qg5KDUEHcEFsbKxGjBihefPmebuUMhcUFKS3335bu3fv1syZMx03JARKKjExUUePHtWYMWO8XQpMjEtXAADAtDijAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATIugAwAATOv/AU3/DL5N6BRPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85th percentile: 0.0769\n",
      "90th percentile: 0.0854\n",
      "95th percentile: 0.0998\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#Organize Data\n",
    "similarity_values_np = similarity_matrix_np.flatten()\n",
    "similarity_values_np_filtered = similarity_values_np[similarity_values_np <= 0.25]\n",
    "X = pd.Series(similarity_values_np_filtered, name=\"(Similarity Matrix Distribution)\")\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(X, bins=25, color=\"g\", ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Compute percentiles\n",
    "percentiles = [85, 90, 95]\n",
    "percentile_values = np.percentile(similarity_values_np_filtered, percentiles)\n",
    "\n",
    "# Print results\n",
    "for p, val in zip(percentiles, percentile_values):\n",
    "    print(f\"{p}th percentile: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb2e70a-fe2c-4669-ad62-8c38b01505f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, loss, path=\"./paper_recommender_Large.pth\"):\n",
    "    \"\"\"Save the model, optimizer state, and training metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Model saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa12bbd-aa94-4282-b3e9-4bafbbf51259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path = \"./paper_recommender_Large.pth\"):\n",
    "    \"\"\"Load the model and optionally the optimizer.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Model loaded from {path}, trained until epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Optimizer state restored.\")\n",
    "\n",
    "    return checkpoint[\"epoch\"], checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc03d8a2-8a92-4a9b-8b63-b2549cc36377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5):\n",
    "    \"\"\"\n",
    "    Contrastive loss using TF-IDF similarity as ground truth.\n",
    "    \n",
    "    embeddings: (batch_size, embedding_dim)\n",
    "    similarity_matrix: Precomputed TF-IDF cosine similarity.\n",
    "    indices: Indices of batch samples in dataset.\n",
    "    margin: Margin for contrastive loss.\n",
    "    \"\"\"\n",
    "    batch_size = embeddings.shape[0]\n",
    "\n",
    "    # Ensure embeddings are L2 normalized\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = torch.mm(embeddings, embeddings.T)  # (batch_size, batch_size)\n",
    "    cosine_distances = 1 - cosine_sim  # Convert similarity to distance\n",
    "\n",
    "    # Extract ground truth similarity values for batch samples\n",
    "    ground_truth_similarities = similarity_matrix[indices][:, indices]\n",
    "\n",
    "    # Define positive and negative pairs\n",
    "    threshold = 0.1205  # Adjust this value if needed\n",
    "    positive_pairs = (ground_truth_similarities > threshold).float()\n",
    "    negative_pairs = (ground_truth_similarities <= threshold).float()\n",
    "\n",
    "    # Compute losses\n",
    "    positive_loss = (cosine_distances * positive_pairs).sum() / (positive_pairs.sum() + 1e-8)\n",
    "    negative_loss = torch.clamp(margin - cosine_distances, min=0) * negative_pairs\n",
    "    negative_loss = negative_loss.sum() / (negative_pairs.sum() + 1e-8)\n",
    "\n",
    "    loss = positive_loss + negative_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea65a89c-88ae-45e3-b7e1-957058b21736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [256, 512, 768])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2,4,8,16])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "\n",
    "    # Initialize model with selected hyperparameters\n",
    "    model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                             embedding_dim, num_heads, dropout).to(\"cuda\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False)\n",
    "\n",
    "    num_epochs = 3  # Use fewer epochs for tuning\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(dataloader_train)\n",
    "\n",
    "        # Validation step (compute loss on validation set)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_val:\n",
    "                input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "                attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "                indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "                embeddings = model(input_ids, attention_mask)\n",
    "                val_loss += contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(dataloader_val)\n",
    "        total_loss += avg_val_loss\n",
    "\n",
    "    return total_loss / num_epochs  # Minimize validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1396d4-1570-443a-93d7-8ffc52a66caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-22 10:40:58,448] A new study created in memory with name: no-name-ba3f3067-c3c0-48f3-8087-df69d676aedf\n",
      "[I 2025-03-22 11:11:03,899] Trial 0 finished with value: 0.09098326430212489 and parameters: {'embedding_dim': 512, 'num_heads': 8, 'dropout': 0.14380577624493165}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 11:38:33,280] Trial 1 finished with value: 0.19301498531577965 and parameters: {'embedding_dim': 512, 'num_heads': 8, 'dropout': 0.2704141693905229}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 12:05:56,476] Trial 2 finished with value: 0.10596397437018519 and parameters: {'embedding_dim': 512, 'num_heads': 4, 'dropout': 0.1856256086503944}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 12:33:12,648] Trial 3 finished with value: 0.23747724854128457 and parameters: {'embedding_dim': 512, 'num_heads': 4, 'dropout': 0.2837015151771267}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 13:00:24,499] Trial 4 finished with value: 0.18092329280784916 and parameters: {'embedding_dim': 256, 'num_heads': 8, 'dropout': 0.2944799818789088}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 13:27:36,567] Trial 5 finished with value: 0.14035518102742348 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.20120530180049098}. Best is trial 0 with value: 0.09098326430212489.\n",
      "[I 2025-03-22 13:54:48,568] Trial 6 finished with value: 0.08626134407734366 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.16599419228366424}. Best is trial 6 with value: 0.08626134407734366.\n",
      "[I 2025-03-22 14:22:00,038] Trial 7 finished with value: 0.25622349172364356 and parameters: {'embedding_dim': 256, 'num_heads': 16, 'dropout': 0.2926213307004354}. Best is trial 6 with value: 0.08626134407734366.\n",
      "[I 2025-03-22 14:49:14,742] Trial 8 finished with value: 0.16051252588414544 and parameters: {'embedding_dim': 512, 'num_heads': 4, 'dropout': 0.23189730838946143}. Best is trial 6 with value: 0.08626134407734366.\n",
      "[I 2025-03-22 15:16:26,715] Trial 9 finished with value: 0.10955369927854279 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.17630685708622812}. Best is trial 6 with value: 0.08626134407734366.\n",
      "[I 2025-03-22 15:44:27,900] Trial 10 finished with value: 0.08289169642410806 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.10116424076742593}. Best is trial 10 with value: 0.08289169642410806.\n",
      "[I 2025-03-22 16:13:06,920] Trial 11 finished with value: 0.08465818299914157 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.11641501879126456}. Best is trial 10 with value: 0.08289169642410806.\n",
      "[I 2025-03-22 16:42:11,318] Trial 12 finished with value: 0.07453273305432524 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.1000802102003194}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 17:10:33,764] Trial 13 finished with value: 0.08471615002818393 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.10085762617852198}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 17:39:18,515] Trial 14 finished with value: 0.08162551096704408 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.13242187367296415}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 18:08:34,995] Trial 15 finished with value: 0.08524595580538528 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.13710669223777794}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 18:37:19,484] Trial 16 finished with value: 0.07809233363549967 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.13421426077899512}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 19:05:54,556] Trial 17 finished with value: 0.12620093916440842 and parameters: {'embedding_dim': 768, 'num_heads': 16, 'dropout': 0.15437364858261218}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 19:36:05,738] Trial 18 finished with value: 0.1111145027790994 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.2196564244950711}. Best is trial 12 with value: 0.07453273305432524.\n",
      "[I 2025-03-22 20:05:50,637] Trial 19 finished with value: 0.08653579738212998 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.12226463954041501}. Best is trial 12 with value: 0.07453273305432524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.1000802102003194}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # We want to minimize loss\n",
    "study.optimize(objective, n_trials=20)  # Run 20 trials\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9619723-25a6-49e5-ab88-42fea483e730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Epoch 1, Avg Loss: 0.0603\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Epoch 2, Avg Loss: 0.0617\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 3. Debug required.\n",
      "Epoch 3, Avg Loss: 0.0629\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 4. Debug required.\n",
      "Epoch 4, Avg Loss: 0.0620\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Epoch 5, Avg Loss: 0.0602\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Epoch 6, Avg Loss: 0.0592\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 7. Debug required.\n",
      "Warning: Zero loss at epoch 7. Debug required.\n",
      "Warning: Zero loss at epoch 7. Debug required.\n",
      "Warning: Zero loss at epoch 7. Debug required.\n",
      "Warning: Zero loss at epoch 7. Debug required.\n",
      "Epoch 7, Avg Loss: 0.0613\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Epoch 8, Avg Loss: 0.0583\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Epoch 9, Avg Loss: 0.0610\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Epoch 10, Avg Loss: 0.0618\n",
      "Model saved at paper_recommender_Large.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               study.best_params[\"embedding_dim\"],\n",
    "                               study.best_params[\"num_heads\"],\n",
    "                               study.best_params[\"dropout\"]).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            print(f\"Warning: Zero loss at epoch {epoch+1}. Debug required.\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "    save_model(model, optimizer, epoch + 1, avg_loss, \"paper_recommender_Large.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34304d1d-4b28-47f7-b4c2-f16cf607b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from paper_recommender_Large.pth, trained until epoch 10\n",
      "Optimizer state restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaperRecommender(\n",
       "  (encoder): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               768,\n",
    "                               2,\n",
    "                               0.1).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Load the model (after training)\n",
    "epoch, loss = load_model(model, optimizer, \"paper_recommender_Large.pth\")\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b1bddb-dea3-4a5d-9e80-3da76064b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, model, df, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode query\n",
    "    query_tokens = tokenizer(query, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(query_tokens[\"input_ids\"], query_tokens[\"attention_mask\"]).cpu().numpy()\n",
    "\n",
    "    # Compute Euclidean distances between query and all paper embeddings\n",
    "    paper_embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_input_ids, batch_attention_mask).cpu().numpy()\n",
    "            paper_embeddings.append(batch_embeddings)\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())  # Store original indices\n",
    "\n",
    "    paper_embeddings = np.vstack(paper_embeddings)  # Stack all embeddings\n",
    "    paper_indices = np.array(paper_indices)\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = np.linalg.norm(paper_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # Get top-k closest papers (smallest distances)\n",
    "    top_indices = np.argsort(distances)[:top_k]\n",
    "\n",
    "    print(\"\\nRecommended Papers:\")\n",
    "    for idx in top_indices:\n",
    "        paper_idx = paper_indices[idx]\n",
    "        print(f\"Title: {df.iloc[paper_idx]['title']}\\nAbstract: {df.iloc[paper_idx]['abstract']}\\nDistance: {distances[idx]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a25f812-4c97-4260-a369-d4a766f40744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Papers:\n",
      "Title: Tactile brain-computer interface using classification of P300 responses evoked by full body spatial vibrotactile stimuli\n",
      "Abstract: In this study we propose a novel stimulus-driven brain-computer interface (BCI) paradigm, which generates control commands based on classification of somatosensory modality P300 responses. Six spatial vibrotactile stimulus patterns are applied to entire back and limbs of a user. The aim of the current project is to validate an effectiveness of the vibrotactile stimulus patterns for BCI purposes and to establish a novel concept of tactile modality communication link, which shall help locked-in syndrome (LIS) patients, who lose their sight and hearing due to sensory disabilities. We define this approach as a full-body BCI (fbBCI) and we conduct psychophysical stimulus evaluation and realtime EEG response classification experiments with ten healthy body-able users. The grand mean averaged psychophysical stimulus pattern recognition accuracy have resulted at 98.18%, whereas the realtime EEG accuracy at 53.67%. An information-transfer-rate (ITR) scores of all the tested users have ranged from 0.042 to 4.154 bit/minute.\n",
      "Distance: 0.7722\n",
      "\n",
      "Title: Signal phase timing impact on traffic delay and queue length-a intersection case study\n",
      "Abstract: Traditional intersection traffic signal control strategy is pre-determined signal with certain phase timing length for each circle. Studies focusing on adaptive traffic signal strategy have somewhat achieved the goal of reducing traffic system delay to some extent. However, few of them capture the benefit of using the queue length as the criteria under the connected vehicle environment, and this paper focuses on firstly identifying the potential saving of average system delay with agent-based simulation modeling, and secondly finding out the relationship between average system delay and average queue length for traffic approaching the signalized intersections. Through applying the agent-based simulation modeling approach in AnyLogic, findings show that average system delay could be reduced using optimized parameters (e.g. arrival rate, signal phase length, etc.), specifically, 5.29% saving of total average system time, 4%--28% traffic queue reduction for different traffic lanes, and a positive relationship between average system delay and the average traffic queue length is detected.\n",
      "Distance: 0.7800\n",
      "\n",
      "Title: Mostly-optimistic concurrency control for highly contended dynamic workloads on a thousand cores\n",
      "Abstract: Future servers will be equipped with thousands of CPU cores and deep memory hierarchies. Traditional concurrency control (CC) schemes---both optimistic and pessimistic---slow down orders of magnitude in such environments for highly contended workloads. Optimistic CC (OCC) scales the best for workloads with few conflicts, but suffers from clobbered reads for high conflict workloads. Although pessimistic locking can protect reads, it floods cache-coherence backbones in deep memory hierarchies and can also cause numerous deadlock aborts.#R##N##R##N#This paper proposes a new CC scheme, mostly-optimistic concurrency control (MOCC), to address these problems. MOCC achieves orders of magnitude higher performance for dynamic workloads on modern servers. The key objective of MOCC is to avoid clobbered reads for high conflict workloads, without any centralized mechanisms or heavyweight interthread communication. To satisfy such needs, we devise a native, cancellable reader-writer spinlock and a serializable protocol that can acquire, release and re-acquire locks in any order without expensive interthread communication. For low conflict workloads, MOCC maintains OCC's high performance without taking read locks.#R##N##R##N#Our experiments with high conflict YCSB workloads on a 288-core server reveal that MOCC performs 8× and 23× faster than OCC and pessimistic locking, respectively. It achieves 17 million TPS for TPC-C and more than 110 million TPS for YCSB without conflicts, 170× faster than pessimistic methods.\n",
      "Distance: 0.7833\n",
      "\n",
      "Title: Toward a generalizable understanding of Twitter and social media use across MOOCs: who participates on MOOC hashtags and in what ways?\n",
      "Abstract: Researchers have proposed that social media provide complementary learning environments for Massive Open Online Courses (MOOCs) that might engender participation, engagement, and peer-support. Although suggestive, nearly all of the research in this area consists of case studies, making it challenging to determine whether or to what extent findings can be generalized to MOOCs beyond those studied. This mixed methods research used data mining techniques to retrieve a large-scale Twitter data set from 116 MOOCs with course-dedicated hashtags. Using quantitative and qualitative methods, it then examined users’ participation patterns, the types of users posting to those hashtags, the types of tweets that were posted, and the variation in types of posted tweets across users. While popular narratives suggest that social media provide a space for increased participation, this study provides little evidence to support these claims in the context of Twitter as an adjunct to MOOCs. Results show that learners make up only about 45% of users and contribute only about 35% of tweets. The majority of users contribute minimally, and an active minority of users contributes the preponderance of messages. These findings do not reveal substantive evidence of learners contributing to multiple hashtags, which may suggest that learners did not find Twitter to be a useful space that provided added value or responded to their needs. Ultimately, these results demonstrate the need for greater intentionality in integrating social media into MOOCs.\n",
      "Distance: 0.7858\n",
      "\n",
      "Title: OR-Play: An Optimal Relay Placement Scheme for High-Quality Wireless Network Services\n",
      "Abstract: With the development of wireless communication and social network, wireless network service demands have increased rapidly in recent years. To amplify the wireless signals and expand the coverage of wireless networks, wireless relay nodes are introduced. This paper addresses the problem of finding an optimal deployment of access points and wireless relay nodes in an arbitrary environment to provide all the potential users with higher quality wireless network services. Our ambition is to maximize the coverage rate and to minimize the energy consumption of the relay nodes. Correspondingly, we design a scheme named OR-Play: an optimal relay placement scheme to provide high-quality wireless services, which consists of three phases. First, OR-Play provides an area coverage for an arbitrary area. We use the virtual force model to determine the positions of wireless devices, including access points and relay nodes, and thus extend the network lifetime. In the second phase, OR-Play selects access points by a 2-approximation algorithm for the metric k-center problem. In the third phase, we define a new problem: k-minimum energy broadcasting trees. We design a distributed greedy strategy to determine the broadcasting trees, based on which the power of relay nodes are precisely assigned. Finally, the simulation results validate the effectiveness and efficiency of OR-Play.\n",
      "Distance: 0.8051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend_papers(\"deep learning for edge computing\", model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bd8b34-d743-441f-8b39-6b8162e06224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test Embedding: tensor([ 0.0484, -0.0271, -0.0472,  0.0290, -0.0463, -0.0087, -0.0113,  0.0026,\n",
      "        -0.0633, -0.0120,  0.0005,  0.0405,  0.0290, -0.0201, -0.0131],\n",
      "       device='cuda:0')\n",
      "Sample Train Embedding: tensor([ 0.0332, -0.0545, -0.0365, -0.0274, -0.0683,  0.0369, -0.0212,  0.0079,\n",
      "        -0.0702,  0.0382,  0.0248,  0.0978, -0.0342,  0.0356, -0.0411],\n",
      "       device='cuda:0')\n",
      "\n",
      "Test Paper 1:\n",
      "\n",
      "Test Paper 2:\n",
      "\n",
      "Test Paper 3:\n",
      "\n",
      "Test Paper 4:\n",
      "\n",
      "Test Paper 5:\n",
      "\n",
      "Test Paper 6:\n",
      "\n",
      "Test Paper 7:\n",
      "\n",
      "Test Paper 8:\n",
      "\n",
      "Test Paper 9:\n",
      "\n",
      "Test Paper 10:\n",
      "\n",
      "Test Paper 11:\n",
      "\n",
      "Test Paper 12:\n",
      "\n",
      "Test Paper 13:\n",
      "\n",
      "Test Paper 14:\n",
      "\n",
      "Test Paper 15:\n",
      "\n",
      "Test Paper 16:\n",
      "\n",
      "Test Paper 17:\n",
      "\n",
      "Test Paper 18:\n",
      "\n",
      "Test Paper 19:\n",
      "\n",
      "Test Paper 20:\n",
      "\n",
      "Test Paper 21:\n",
      "\n",
      "Test Paper 22:\n",
      "\n",
      "Test Paper 23:\n",
      "\n",
      "Test Paper 24:\n",
      "\n",
      "Test Paper 25:\n",
      "\n",
      "Test Paper 26:\n",
      "\n",
      "Test Paper 27:\n",
      "\n",
      "Test Paper 28:\n",
      "\n",
      "Test Paper 29:\n",
      "\n",
      "Test Paper 30:\n",
      "\n",
      "Test Paper 31:\n",
      "\n",
      "Test Paper 32:\n",
      "\n",
      "Test Paper 33:\n",
      "\n",
      "Test Paper 34:\n",
      "\n",
      "Test Paper 35:\n",
      "\n",
      "Test Paper 36:\n",
      "\n",
      "Test Paper 37:\n",
      "\n",
      "Test Paper 38:\n",
      "\n",
      "Test Paper 39:\n",
      "\n",
      "Test Paper 40:\n",
      "\n",
      "Test Paper 41:\n",
      "\n",
      "Test Paper 42:\n",
      "\n",
      "Test Paper 43:\n",
      "\n",
      "Test Paper 44:\n",
      "\n",
      "Test Paper 45:\n",
      "\n",
      "Test Paper 46:\n",
      "\n",
      "Test Paper 47:\n",
      "\n",
      "Test Paper 48:\n",
      "\n",
      "Test Paper 49:\n",
      "\n",
      "Test Paper 50:\n",
      "\n",
      "Test Paper 51:\n",
      "\n",
      "Test Paper 52:\n",
      "\n",
      "Test Paper 53:\n",
      "\n",
      "Test Paper 54:\n",
      "\n",
      "Test Paper 55:\n",
      "\n",
      "Test Paper 56:\n",
      "\n",
      "Test Paper 57:\n",
      "\n",
      "Test Paper 58:\n",
      "\n",
      "Test Paper 59:\n",
      "\n",
      "Test Paper 60:\n",
      "\n",
      "Test Paper 61:\n",
      "\n",
      "Test Paper 62:\n",
      "\n",
      "Test Paper 63:\n",
      "\n",
      "Test Paper 64:\n",
      "\n",
      "Test Paper 65:\n",
      "\n",
      "Test Paper 66:\n",
      "\n",
      "Test Paper 67:\n",
      "\n",
      "Test Paper 68:\n",
      "\n",
      "Test Paper 69:\n",
      "\n",
      "Test Paper 70:\n",
      "\n",
      "Test Paper 71:\n",
      "\n",
      "Test Paper 72:\n",
      "\n",
      "Test Paper 73:\n",
      "\n",
      "Test Paper 74:\n",
      "\n",
      "Test Paper 75:\n",
      "\n",
      "Test Paper 76:\n",
      "\n",
      "Test Paper 77:\n",
      "\n",
      "Test Paper 78:\n",
      "\n",
      "Test Paper 79:\n",
      "\n",
      "Test Paper 80:\n",
      "\n",
      "Test Paper 81:\n",
      "\n",
      "Test Paper 82:\n",
      "\n",
      "Test Paper 83:\n",
      "\n",
      "Test Paper 84:\n",
      "\n",
      "Test Paper 85:\n",
      "\n",
      "Test Paper 86:\n",
      "\n",
      "Test Paper 87:\n",
      "\n",
      "Test Paper 88:\n",
      "\n",
      "Test Paper 89:\n",
      "\n",
      "Test Paper 90:\n",
      "\n",
      "Test Paper 91:\n",
      "\n",
      "Test Paper 92:\n",
      "\n",
      "Test Paper 93:\n",
      "\n",
      "Test Paper 94:\n",
      "\n",
      "Test Paper 95:\n",
      "\n",
      "Test Paper 96:\n",
      "\n",
      "Test Paper 97:\n",
      "\n",
      "Test Paper 98:\n",
      "\n",
      "Test Paper 99:\n",
      "\n",
      "Test Paper 100:\n",
      "\n",
      "Test Paper 101:\n",
      "\n",
      "Test Paper 102:\n",
      "\n",
      "Test Paper 103:\n",
      "\n",
      "Test Paper 104:\n",
      "\n",
      "Test Paper 105:\n",
      "\n",
      "Test Paper 106:\n",
      "\n",
      "Test Paper 107:\n",
      "\n",
      "Test Paper 108:\n",
      "\n",
      "Test Paper 109:\n",
      "\n",
      "Test Paper 110:\n",
      "\n",
      "Test Paper 111:\n",
      "\n",
      "Test Paper 112:\n",
      "\n",
      "Test Paper 113:\n",
      "\n",
      "Test Paper 114:\n",
      "\n",
      "Test Paper 115:\n",
      "\n",
      "Test Paper 116:\n",
      "\n",
      "Test Paper 117:\n",
      "\n",
      "Test Paper 118:\n",
      "\n",
      "Test Paper 119:\n",
      "\n",
      "Test Paper 120:\n",
      "\n",
      "Test Paper 121:\n",
      "\n",
      "Test Paper 122:\n",
      "\n",
      "Test Paper 123:\n",
      "\n",
      "Test Paper 124:\n",
      "\n",
      "Test Paper 125:\n",
      "\n",
      "Test Paper 126:\n",
      "\n",
      "Test Paper 127:\n",
      "\n",
      "Test Paper 128:\n",
      "\n",
      "Test Paper 129:\n",
      "\n",
      "Test Paper 130:\n",
      "\n",
      "Test Paper 131:\n",
      "\n",
      "Test Paper 132:\n",
      "\n",
      "Test Paper 133:\n",
      "\n",
      "Test Paper 134:\n",
      "\n",
      "Test Paper 135:\n",
      "\n",
      "Test Paper 136:\n",
      "\n",
      "Test Paper 137:\n",
      "\n",
      "Test Paper 138:\n",
      "\n",
      "Test Paper 139:\n",
      "\n",
      "Test Paper 140:\n",
      "\n",
      "Test Paper 141:\n",
      "\n",
      "Test Paper 142:\n",
      "\n",
      "Test Paper 143:\n",
      "\n",
      "Test Paper 144:\n",
      "\n",
      "Test Paper 145:\n",
      "\n",
      "Test Paper 146:\n",
      "\n",
      "Test Paper 147:\n",
      "\n",
      "Test Paper 148:\n",
      "\n",
      "Test Paper 149:\n",
      "\n",
      "Test Paper 150:\n",
      "\n",
      "Test Paper 151:\n",
      "\n",
      "Test Paper 152:\n",
      "\n",
      "Test Paper 153:\n",
      "\n",
      "Test Paper 154:\n",
      "\n",
      "Test Paper 155:\n",
      "\n",
      "Test Paper 156:\n",
      "\n",
      "Test Paper 157:\n",
      "\n",
      "Test Paper 158:\n",
      "\n",
      "Test Paper 159:\n",
      "\n",
      "Test Paper 160:\n",
      "\n",
      "Test Paper 161:\n",
      "\n",
      "Test Paper 162:\n",
      "\n",
      "Test Paper 163:\n",
      "\n",
      "Test Paper 164:\n",
      "\n",
      "Test Paper 165:\n",
      "\n",
      "Test Paper 166:\n",
      "\n",
      "Test Paper 167:\n",
      "\n",
      "Test Paper 168:\n",
      "\n",
      "Test Paper 169:\n",
      "\n",
      "Test Paper 170:\n",
      "\n",
      "Test Paper 171:\n",
      "\n",
      "Test Paper 172:\n",
      "\n",
      "Test Paper 173:\n",
      "\n",
      "Test Paper 174:\n",
      "\n",
      "Test Paper 175:\n",
      "\n",
      "Test Paper 176:\n",
      "\n",
      "Test Paper 177:\n",
      "\n",
      "Test Paper 178:\n",
      "\n",
      "Test Paper 179:\n",
      "\n",
      "Test Paper 180:\n",
      "\n",
      "Test Paper 181:\n",
      "\n",
      "Test Paper 182:\n",
      "\n",
      "Test Paper 183:\n",
      "\n",
      "Test Paper 184:\n",
      "\n",
      "Test Paper 185:\n",
      "\n",
      "Test Paper 186:\n",
      "\n",
      "Test Paper 187:\n",
      "\n",
      "Test Paper 188:\n",
      "\n",
      "Test Paper 189:\n",
      "\n",
      "Test Paper 190:\n",
      "\n",
      "Test Paper 191:\n",
      "\n",
      "Test Paper 192:\n",
      "\n",
      "Test Paper 193:\n",
      "\n",
      "Test Paper 194:\n",
      "\n",
      "Test Paper 195:\n",
      "\n",
      "Test Paper 196:\n",
      "\n",
      "Test Paper 197:\n",
      "\n",
      "Test Paper 198:\n",
      "\n",
      "Test Paper 199:\n",
      "\n",
      "Test Paper 200:\n",
      "\n",
      "Test Paper 201:\n",
      "\n",
      "Test Paper 202:\n",
      "\n",
      "Test Paper 203:\n",
      "\n",
      "Test Paper 204:\n",
      "\n",
      "Test Paper 205:\n",
      "\n",
      "Test Paper 206:\n",
      "\n",
      "Test Paper 207:\n",
      "\n",
      "Test Paper 208:\n",
      "\n",
      "Test Paper 209:\n",
      "\n",
      "Test Paper 210:\n",
      "\n",
      "Test Paper 211:\n",
      "\n",
      "Test Paper 212:\n",
      "\n",
      "Test Paper 213:\n",
      "\n",
      "Test Paper 214:\n",
      "\n",
      "Test Paper 215:\n",
      "\n",
      "Test Paper 216:\n",
      "\n",
      "Test Paper 217:\n",
      "\n",
      "Test Paper 218:\n",
      "\n",
      "Test Paper 219:\n",
      "\n",
      "Test Paper 220:\n",
      "\n",
      "Test Paper 221:\n",
      "\n",
      "Test Paper 222:\n",
      "\n",
      "Test Paper 223:\n",
      "\n",
      "Test Paper 224:\n",
      "\n",
      "Test Paper 225:\n",
      "\n",
      "Test Paper 226:\n",
      "\n",
      "Test Paper 227:\n",
      "\n",
      "Test Paper 228:\n",
      "\n",
      "Test Paper 229:\n",
      "\n",
      "Test Paper 230:\n",
      "\n",
      "Test Paper 231:\n",
      "\n",
      "Test Paper 232:\n",
      "\n",
      "Test Paper 233:\n",
      "\n",
      "Test Paper 234:\n",
      "\n",
      "Test Paper 235:\n",
      "\n",
      "Test Paper 236:\n",
      "\n",
      "Test Paper 237:\n",
      "\n",
      "Test Paper 238:\n",
      "\n",
      "Test Paper 239:\n",
      "\n",
      "Test Paper 240:\n",
      "\n",
      "Test Paper 241:\n",
      "\n",
      "Test Paper 242:\n",
      "\n",
      "Test Paper 243:\n",
      "\n",
      "Test Paper 244:\n",
      "\n",
      "Test Paper 245:\n",
      "\n",
      "Test Paper 246:\n",
      "\n",
      "Test Paper 247:\n",
      "\n",
      "Test Paper 248:\n",
      "\n",
      "Test Paper 249:\n",
      "\n",
      "Test Paper 250:\n",
      "\n",
      "Test Paper 251:\n",
      "\n",
      "Test Paper 252:\n",
      "\n",
      "Test Paper 253:\n",
      "\n",
      "Test Paper 254:\n",
      "\n",
      "Test Paper 255:\n",
      "\n",
      "Test Paper 256:\n",
      "\n",
      "Test Paper 257:\n",
      "\n",
      "Test Paper 258:\n",
      "\n",
      "Test Paper 259:\n",
      "\n",
      "Test Paper 260:\n",
      "\n",
      "Test Paper 261:\n",
      "\n",
      "Test Paper 262:\n",
      "\n",
      "Test Paper 263:\n",
      "\n",
      "Test Paper 264:\n",
      "\n",
      "Test Paper 265:\n",
      "\n",
      "Test Paper 266:\n",
      "\n",
      "Test Paper 267:\n",
      "\n",
      "Test Paper 268:\n",
      "\n",
      "Test Paper 269:\n",
      "\n",
      "Test Paper 270:\n",
      "\n",
      "Test Paper 271:\n",
      "\n",
      "Test Paper 272:\n",
      "\n",
      "Test Paper 273:\n",
      "\n",
      "Test Paper 274:\n",
      "\n",
      "Test Paper 275:\n",
      "\n",
      "Test Paper 276:\n",
      "\n",
      "Test Paper 277:\n",
      "\n",
      "Test Paper 278:\n",
      "\n",
      "Test Paper 279:\n",
      "\n",
      "Test Paper 280:\n",
      "\n",
      "Test Paper 281:\n",
      "\n",
      "Test Paper 282:\n",
      "\n",
      "Test Paper 283:\n",
      "\n",
      "Test Paper 284:\n",
      "\n",
      "Test Paper 285:\n",
      "\n",
      "Test Paper 286:\n",
      "\n",
      "Test Paper 287:\n",
      "\n",
      "Test Paper 288:\n",
      "\n",
      "Test Paper 289:\n",
      "\n",
      "Test Paper 290:\n",
      "\n",
      "Test Paper 291:\n",
      "\n",
      "Test Paper 292:\n",
      "\n",
      "Test Paper 293:\n",
      "\n",
      "Test Paper 294:\n",
      "\n",
      "Test Paper 295:\n",
      "\n",
      "Test Paper 296:\n",
      "\n",
      "Test Paper 297:\n",
      "\n",
      "Test Paper 298:\n",
      "\n",
      "Test Paper 299:\n",
      "\n",
      "Test Paper 300:\n",
      "\n",
      "Test Paper 301:\n",
      "\n",
      "Test Paper 302:\n",
      "\n",
      "Test Paper 303:\n",
      "\n",
      "Test Paper 304:\n",
      "\n",
      "Test Paper 305:\n",
      "\n",
      "Test Paper 306:\n",
      "\n",
      "Test Paper 307:\n",
      "\n",
      "Test Paper 308:\n",
      "\n",
      "Test Paper 309:\n",
      "\n",
      "Test Paper 310:\n",
      "\n",
      "Test Paper 311:\n",
      "\n",
      "Test Paper 312:\n",
      "\n",
      "Test Paper 313:\n",
      "\n",
      "Test Paper 314:\n",
      "\n",
      "Test Paper 315:\n",
      "\n",
      "Test Paper 316:\n",
      "\n",
      "Test Paper 317:\n",
      "\n",
      "Test Paper 318:\n",
      "\n",
      "Test Paper 319:\n",
      "\n",
      "Test Paper 320:\n",
      "\n",
      "Test Paper 321:\n",
      "\n",
      "Test Paper 322:\n",
      "\n",
      "Test Paper 323:\n",
      "\n",
      "Test Paper 324:\n",
      "\n",
      "Test Paper 325:\n",
      "\n",
      "Test Paper 326:\n",
      "\n",
      "Test Paper 327:\n",
      "\n",
      "Test Paper 328:\n",
      "\n",
      "Test Paper 329:\n",
      "\n",
      "Test Paper 330:\n",
      "\n",
      "Test Paper 331:\n",
      "\n",
      "Test Paper 332:\n",
      "\n",
      "Test Paper 333:\n",
      "\n",
      "Test Paper 334:\n",
      "\n",
      "Test Paper 335:\n",
      "\n",
      "Test Paper 336:\n",
      "\n",
      "Test Paper 337:\n",
      "\n",
      "Test Paper 338:\n",
      "\n",
      "Test Paper 339:\n",
      "\n",
      "Test Paper 340:\n",
      "\n",
      "Test Paper 341:\n",
      "\n",
      "Test Paper 342:\n",
      "\n",
      "Test Paper 343:\n",
      "\n",
      "Test Paper 344:\n",
      "\n",
      "Test Paper 345:\n",
      "\n",
      "Test Paper 346:\n",
      "\n",
      "Test Paper 347:\n",
      "\n",
      "Test Paper 348:\n",
      "\n",
      "Test Paper 349:\n",
      "\n",
      "Test Paper 350:\n",
      "\n",
      "Test Paper 351:\n",
      "\n",
      "Test Paper 352:\n",
      "\n",
      "Test Paper 353:\n",
      "\n",
      "Test Paper 354:\n",
      "\n",
      "Test Paper 355:\n",
      "\n",
      "Test Paper 356:\n",
      "\n",
      "Test Paper 357:\n",
      "\n",
      "Test Paper 358:\n",
      "\n",
      "Test Paper 359:\n",
      "\n",
      "Test Paper 360:\n",
      "\n",
      "Test Paper 361:\n",
      "\n",
      "Test Paper 362:\n",
      "\n",
      "Test Paper 363:\n",
      "\n",
      "Test Paper 364:\n",
      "\n",
      "Test Paper 365:\n",
      "\n",
      "Test Paper 366:\n",
      "\n",
      "Test Paper 367:\n",
      "\n",
      "Test Paper 368:\n",
      "\n",
      "Test Paper 369:\n",
      "\n",
      "Test Paper 370:\n",
      "\n",
      "Test Paper 371:\n",
      "\n",
      "Test Paper 372:\n",
      "\n",
      "Test Paper 373:\n",
      "\n",
      "Test Paper 374:\n",
      "\n",
      "Test Paper 375:\n",
      "\n",
      "Test Paper 376:\n",
      "\n",
      "Test Paper 377:\n",
      "\n",
      "Test Paper 378:\n",
      "\n",
      "Test Paper 379:\n",
      "\n",
      "Test Paper 380:\n",
      "\n",
      "Test Paper 381:\n",
      "\n",
      "Test Paper 382:\n",
      "\n",
      "Test Paper 383:\n",
      "\n",
      "Test Paper 384:\n",
      "\n",
      "Test Paper 385:\n",
      "\n",
      "Test Paper 386:\n",
      "\n",
      "Test Paper 387:\n",
      "\n",
      "Test Paper 388:\n",
      "\n",
      "Test Paper 389:\n",
      "\n",
      "Test Paper 390:\n",
      "\n",
      "Test Paper 391:\n",
      "\n",
      "Test Paper 392:\n",
      "\n",
      "Test Paper 393:\n",
      "\n",
      "Test Paper 394:\n",
      "\n",
      "Test Paper 395:\n",
      "\n",
      "Test Paper 396:\n",
      "\n",
      "Test Paper 397:\n",
      "\n",
      "Test Paper 398:\n",
      "\n",
      "Test Paper 399:\n",
      "\n",
      "Test Paper 400:\n",
      "\n",
      "Test Paper 401:\n",
      "\n",
      "Test Paper 402:\n",
      "\n",
      "Test Paper 403:\n",
      "\n",
      "Test Paper 404:\n",
      "\n",
      "Test Paper 405:\n",
      "\n",
      "Test Paper 406:\n",
      "\n",
      "Test Paper 407:\n",
      "\n",
      "Test Paper 408:\n",
      "\n",
      "Test Paper 409:\n",
      "\n",
      "Test Paper 410:\n",
      "\n",
      "Test Paper 411:\n",
      "\n",
      "Test Paper 412:\n",
      "\n",
      "Test Paper 413:\n",
      "\n",
      "Test Paper 414:\n",
      "\n",
      "Test Paper 415:\n",
      "\n",
      "Test Paper 416:\n",
      "\n",
      "Test Paper 417:\n",
      "\n",
      "Test Paper 418:\n",
      "\n",
      "Test Paper 419:\n",
      "\n",
      "Test Paper 420:\n",
      "\n",
      "Test Paper 421:\n",
      "\n",
      "Test Paper 422:\n",
      "\n",
      "Test Paper 423:\n",
      "\n",
      "Test Paper 424:\n",
      "\n",
      "Test Paper 425:\n",
      "\n",
      "Test Paper 426:\n",
      "\n",
      "Test Paper 427:\n",
      "\n",
      "Test Paper 428:\n",
      "\n",
      "Test Paper 429:\n",
      "\n",
      "Test Paper 430:\n",
      "\n",
      "Test Paper 431:\n",
      "\n",
      "Test Paper 432:\n",
      "\n",
      "Test Paper 433:\n",
      "\n",
      "Test Paper 434:\n",
      "\n",
      "Test Paper 435:\n",
      "\n",
      "Test Paper 436:\n",
      "\n",
      "Test Paper 437:\n",
      "\n",
      "Test Paper 438:\n",
      "\n",
      "Test Paper 439:\n",
      "\n",
      "Test Paper 440:\n",
      "\n",
      "Test Paper 441:\n",
      "\n",
      "Test Paper 442:\n",
      "\n",
      "Test Paper 443:\n",
      "\n",
      "Test Paper 444:\n",
      "\n",
      "Test Paper 445:\n",
      "\n",
      "Test Paper 446:\n",
      "\n",
      "Test Paper 447:\n",
      "\n",
      "Test Paper 448:\n",
      "\n",
      "Test Paper 449:\n",
      "\n",
      "Test Paper 450:\n",
      "\n",
      "Test Paper 451:\n",
      "\n",
      "Test Paper 452:\n",
      "\n",
      "Test Paper 453:\n",
      "\n",
      "Test Paper 454:\n",
      "\n",
      "Test Paper 455:\n",
      "\n",
      "Test Paper 456:\n",
      "\n",
      "Test Paper 457:\n",
      "\n",
      "Test Paper 458:\n",
      "\n",
      "Test Paper 459:\n",
      "\n",
      "Test Paper 460:\n",
      "\n",
      "Test Paper 461:\n",
      "\n",
      "Test Paper 462:\n",
      "\n",
      "Test Paper 463:\n",
      "\n",
      "Test Paper 464:\n",
      "\n",
      "Test Paper 465:\n",
      "\n",
      "Test Paper 466:\n",
      "\n",
      "Test Paper 467:\n",
      "\n",
      "Test Paper 468:\n",
      "\n",
      "Test Paper 469:\n",
      "\n",
      "Test Paper 470:\n",
      "\n",
      "Test Paper 471:\n",
      "\n",
      "Test Paper 472:\n",
      "\n",
      "Test Paper 473:\n",
      "\n",
      "Test Paper 474:\n",
      "\n",
      "Test Paper 475:\n",
      "\n",
      "Test Paper 476:\n",
      "\n",
      "Test Paper 477:\n",
      "\n",
      "Test Paper 478:\n",
      "\n",
      "Test Paper 479:\n",
      "\n",
      "Test Paper 480:\n",
      "\n",
      "Test Paper 481:\n",
      "\n",
      "Test Paper 482:\n",
      "\n",
      "Test Paper 483:\n",
      "\n",
      "Test Paper 484:\n",
      "\n",
      "Test Paper 485:\n",
      "\n",
      "Test Paper 486:\n",
      "\n",
      "Test Paper 487:\n",
      "\n",
      "Test Paper 488:\n",
      "\n",
      "Test Paper 489:\n",
      "\n",
      "Test Paper 490:\n",
      "\n",
      "Test Paper 491:\n",
      "\n",
      "Test Paper 492:\n",
      "\n",
      "Test Paper 493:\n",
      "\n",
      "Test Paper 494:\n",
      "\n",
      "Test Paper 495:\n",
      "\n",
      "Test Paper 496:\n",
      "\n",
      "Test Paper 497:\n",
      "\n",
      "Test Paper 498:\n",
      "\n",
      "Test Paper 499:\n",
      "\n",
      "Test Paper 500:\n",
      "\n",
      "Test Paper 501:\n",
      "\n",
      "Test Paper 502:\n",
      "\n",
      "Test Paper 503:\n",
      "\n",
      "Test Paper 504:\n",
      "\n",
      "Test Paper 505:\n",
      "\n",
      "Test Paper 506:\n",
      "\n",
      "Test Paper 507:\n",
      "\n",
      "Test Paper 508:\n",
      "\n",
      "Test Paper 509:\n",
      "\n",
      "Test Paper 510:\n",
      "\n",
      "Test Paper 511:\n",
      "\n",
      "Test Paper 512:\n",
      "\n",
      "Test Paper 513:\n",
      "\n",
      "Test Paper 514:\n",
      "\n",
      "Test Paper 515:\n",
      "\n",
      "Test Paper 516:\n",
      "\n",
      "Test Paper 517:\n",
      "\n",
      "Test Paper 518:\n",
      "\n",
      "Test Paper 519:\n",
      "\n",
      "Test Paper 520:\n",
      "\n",
      "Test Paper 521:\n",
      "\n",
      "Test Paper 522:\n",
      "\n",
      "Test Paper 523:\n",
      "\n",
      "Test Paper 524:\n",
      "\n",
      "Test Paper 525:\n",
      "\n",
      "Test Paper 526:\n",
      "\n",
      "Test Paper 527:\n",
      "\n",
      "Test Paper 528:\n",
      "\n",
      "Test Paper 529:\n",
      "\n",
      "Test Paper 530:\n",
      "\n",
      "Test Paper 531:\n",
      "\n",
      "Test Paper 532:\n",
      "\n",
      "Test Paper 533:\n",
      "\n",
      "Test Paper 534:\n",
      "\n",
      "Test Paper 535:\n",
      "\n",
      "Test Paper 536:\n",
      "\n",
      "Test Paper 537:\n",
      "\n",
      "Test Paper 538:\n",
      "\n",
      "Test Paper 539:\n",
      "\n",
      "Test Paper 540:\n",
      "\n",
      "Test Paper 541:\n",
      "\n",
      "Test Paper 542:\n",
      "\n",
      "Test Paper 543:\n",
      "\n",
      "Test Paper 544:\n",
      "\n",
      "Test Paper 545:\n",
      "\n",
      "Test Paper 546:\n",
      "\n",
      "Test Paper 547:\n",
      "\n",
      "Test Paper 548:\n",
      "\n",
      "Test Paper 549:\n",
      "\n",
      "Test Paper 550:\n",
      "\n",
      "Test Paper 551:\n",
      "\n",
      "Test Paper 552:\n",
      "\n",
      "Test Paper 553:\n",
      "\n",
      "Test Paper 554:\n",
      "\n",
      "Test Paper 555:\n",
      "\n",
      "Test Paper 556:\n",
      "\n",
      "Test Paper 557:\n",
      "\n",
      "Test Paper 558:\n",
      "\n",
      "Test Paper 559:\n",
      "\n",
      "Test Paper 560:\n",
      "\n",
      "Test Paper 561:\n",
      "\n",
      "Test Paper 562:\n",
      "\n",
      "Test Paper 563:\n",
      "\n",
      "Test Paper 564:\n",
      "\n",
      "Test Paper 565:\n",
      "\n",
      "Test Paper 566:\n",
      "\n",
      "Test Paper 567:\n",
      "\n",
      "Test Paper 568:\n",
      "\n",
      "Test Paper 569:\n",
      "\n",
      "Test Paper 570:\n",
      "\n",
      "Test Paper 571:\n",
      "\n",
      "Test Paper 572:\n",
      "\n",
      "Test Paper 573:\n",
      "\n",
      "Test Paper 574:\n",
      "\n",
      "Test Paper 575:\n",
      "\n",
      "Test Paper 576:\n",
      "\n",
      "Test Paper 577:\n",
      "\n",
      "Test Paper 578:\n",
      "\n",
      "Test Paper 579:\n",
      "\n",
      "Test Paper 580:\n",
      "\n",
      "Test Paper 581:\n",
      "\n",
      "Test Paper 582:\n",
      "\n",
      "Test Paper 583:\n",
      "\n",
      "Test Paper 584:\n",
      "\n",
      "Test Paper 585:\n",
      "\n",
      "Test Paper 586:\n",
      "\n",
      "Test Paper 587:\n",
      "\n",
      "Test Paper 588:\n",
      "\n",
      "Test Paper 589:\n",
      "\n",
      "Test Paper 590:\n",
      "\n",
      "Test Paper 591:\n",
      "\n",
      "Test Paper 592:\n",
      "\n",
      "Test Paper 593:\n",
      "\n",
      "Test Paper 594:\n",
      "\n",
      "Test Paper 595:\n",
      "\n",
      "Test Paper 596:\n",
      "\n",
      "Test Paper 597:\n",
      "\n",
      "Test Paper 598:\n",
      "\n",
      "Test Paper 599:\n",
      "\n",
      "Test Paper 600:\n",
      "\n",
      "Test Paper 601:\n",
      "\n",
      "Test Paper 602:\n",
      "\n",
      "Test Paper 603:\n",
      "\n",
      "Test Paper 604:\n",
      "\n",
      "Test Paper 605:\n",
      "\n",
      "Test Paper 606:\n",
      "\n",
      "Test Paper 607:\n",
      "\n",
      "Test Paper 608:\n",
      "\n",
      "Test Paper 609:\n",
      "\n",
      "Test Paper 610:\n",
      "\n",
      "Test Paper 611:\n",
      "\n",
      "Test Paper 612:\n",
      "\n",
      "Test Paper 613:\n",
      "\n",
      "Test Paper 614:\n",
      "\n",
      "Test Paper 615:\n",
      "\n",
      "Test Paper 616:\n",
      "\n",
      "Test Paper 617:\n",
      "\n",
      "Test Paper 618:\n",
      "\n",
      "Test Paper 619:\n",
      "\n",
      "Test Paper 620:\n",
      "\n",
      "Test Paper 621:\n",
      "\n",
      "Test Paper 622:\n",
      "\n",
      "Test Paper 623:\n",
      "\n",
      "Test Paper 624:\n",
      "\n",
      "Test Paper 625:\n",
      "\n",
      "Test Paper 626:\n",
      "\n",
      "Test Paper 627:\n",
      "\n",
      "Test Paper 628:\n",
      "\n",
      "Test Paper 629:\n",
      "\n",
      "Test Paper 630:\n",
      "\n",
      "Test Paper 631:\n",
      "\n",
      "Test Paper 632:\n",
      "\n",
      "Test Paper 633:\n",
      "\n",
      "Test Paper 634:\n",
      "\n",
      "Test Paper 635:\n",
      "\n",
      "Test Paper 636:\n",
      "\n",
      "Test Paper 637:\n",
      "\n",
      "Test Paper 638:\n",
      "\n",
      "Test Paper 639:\n",
      "\n",
      "Test Paper 640:\n",
      "\n",
      "Test Paper 641:\n",
      "\n",
      "Test Paper 642:\n",
      "\n",
      "Test Paper 643:\n",
      "\n",
      "Test Paper 644:\n",
      "\n",
      "Test Paper 645:\n",
      "\n",
      "Test Paper 646:\n",
      "\n",
      "Test Paper 647:\n",
      "\n",
      "Test Paper 648:\n",
      "\n",
      "Test Paper 649:\n",
      "\n",
      "Test Paper 650:\n",
      "\n",
      "Test Paper 651:\n",
      "\n",
      "Test Paper 652:\n",
      "\n",
      "Test Paper 653:\n",
      "\n",
      "Test Paper 654:\n",
      "\n",
      "Test Paper 655:\n",
      "\n",
      "Test Paper 656:\n",
      "\n",
      "Test Paper 657:\n",
      "\n",
      "Test Paper 658:\n",
      "\n",
      "Test Paper 659:\n",
      "\n",
      "Test Paper 660:\n",
      "\n",
      "Test Paper 661:\n",
      "\n",
      "Test Paper 662:\n",
      "\n",
      "Test Paper 663:\n",
      "\n",
      "Test Paper 664:\n",
      "\n",
      "Test Paper 665:\n",
      "\n",
      "Test Paper 666:\n",
      "\n",
      "Test Paper 667:\n",
      "\n",
      "Test Paper 668:\n",
      "\n",
      "Test Paper 669:\n",
      "\n",
      "Test Paper 670:\n",
      "\n",
      "Test Paper 671:\n",
      "\n",
      "Test Paper 672:\n",
      "\n",
      "Test Paper 673:\n",
      "\n",
      "Test Paper 674:\n",
      "\n",
      "Test Paper 675:\n",
      "\n",
      "Test Paper 676:\n",
      "\n",
      "Test Paper 677:\n",
      "\n",
      "Test Paper 678:\n",
      "\n",
      "Test Paper 679:\n",
      "\n",
      "Test Paper 680:\n",
      "\n",
      "Test Paper 681:\n",
      "\n",
      "Test Paper 682:\n",
      "\n",
      "Test Paper 683:\n",
      "\n",
      "Test Paper 684:\n",
      "\n",
      "Test Paper 685:\n",
      "\n",
      "Test Paper 686:\n",
      "\n",
      "Test Paper 687:\n",
      "\n",
      "Test Paper 688:\n",
      "\n",
      "Test Paper 689:\n",
      "\n",
      "Test Paper 690:\n",
      "\n",
      "Test Paper 691:\n",
      "\n",
      "Test Paper 692:\n",
      "\n",
      "Test Paper 693:\n",
      "\n",
      "Test Paper 694:\n",
      "\n",
      "Test Paper 695:\n",
      "\n",
      "Test Paper 696:\n",
      "\n",
      "Test Paper 697:\n",
      "\n",
      "Test Paper 698:\n",
      "\n",
      "Test Paper 699:\n",
      "\n",
      "Test Paper 700:\n",
      "\n",
      "Test Paper 701:\n",
      "\n",
      "Test Paper 702:\n",
      "\n",
      "Test Paper 703:\n",
      "\n",
      "Test Paper 704:\n",
      "\n",
      "Test Paper 705:\n",
      "\n",
      "Test Paper 706:\n",
      "\n",
      "Test Paper 707:\n",
      "\n",
      "Test Paper 708:\n",
      "\n",
      "Test Paper 709:\n",
      "\n",
      "Test Paper 710:\n",
      "\n",
      "Test Paper 711:\n",
      "\n",
      "Test Paper 712:\n",
      "\n",
      "Test Paper 713:\n",
      "\n",
      "Test Paper 714:\n",
      "\n",
      "Test Paper 715:\n",
      "\n",
      "Test Paper 716:\n",
      "\n",
      "Test Paper 717:\n",
      "\n",
      "Test Paper 718:\n",
      "\n",
      "Test Paper 719:\n",
      "\n",
      "Test Paper 720:\n",
      "\n",
      "Test Paper 721:\n",
      "\n",
      "Test Paper 722:\n",
      "\n",
      "Test Paper 723:\n",
      "\n",
      "Test Paper 724:\n",
      "\n",
      "Test Paper 725:\n",
      "\n",
      "Test Paper 726:\n",
      "\n",
      "Test Paper 727:\n",
      "\n",
      "Test Paper 728:\n",
      "\n",
      "Test Paper 729:\n",
      "\n",
      "Test Paper 730:\n",
      "\n",
      "Test Paper 731:\n",
      "\n",
      "Test Paper 732:\n",
      "\n",
      "Test Paper 733:\n",
      "\n",
      "Test Paper 734:\n",
      "\n",
      "Test Paper 735:\n",
      "\n",
      "Test Paper 736:\n",
      "\n",
      "Test Paper 737:\n",
      "\n",
      "Test Paper 738:\n",
      "\n",
      "Test Paper 739:\n",
      "\n",
      "Test Paper 740:\n",
      "\n",
      "Test Paper 741:\n",
      "\n",
      "Test Paper 742:\n",
      "\n",
      "Test Paper 743:\n",
      "\n",
      "Test Paper 744:\n",
      "\n",
      "Test Paper 745:\n",
      "\n",
      "Test Paper 746:\n",
      "\n",
      "Test Paper 747:\n",
      "\n",
      "Test Paper 748:\n",
      "\n",
      "Test Paper 749:\n",
      "\n",
      "Test Paper 750:\n",
      "\n",
      "Test Paper 751:\n",
      "\n",
      "Test Paper 752:\n",
      "\n",
      "Test Paper 753:\n",
      "\n",
      "Test Paper 754:\n",
      "\n",
      "Test Paper 755:\n",
      "\n",
      "Test Paper 756:\n",
      "\n",
      "Test Paper 757:\n",
      "\n",
      "Test Paper 758:\n",
      "\n",
      "Test Paper 759:\n",
      "\n",
      "Test Paper 760:\n",
      "\n",
      "Test Paper 761:\n",
      "\n",
      "Test Paper 762:\n",
      "\n",
      "Test Paper 763:\n",
      "\n",
      "Test Paper 764:\n",
      "\n",
      "Test Paper 765:\n",
      "\n",
      "Test Paper 766:\n",
      "\n",
      "Test Paper 767:\n",
      "\n",
      "Test Paper 768:\n",
      "\n",
      "Test Paper 769:\n",
      "\n",
      "Test Paper 770:\n",
      "\n",
      "Test Paper 771:\n",
      "\n",
      "Test Paper 772:\n",
      "\n",
      "Test Paper 773:\n",
      "\n",
      "Test Paper 774:\n",
      "\n",
      "Test Paper 775:\n",
      "\n",
      "Test Paper 776:\n",
      "\n",
      "Test Paper 777:\n",
      "\n",
      "Test Paper 778:\n",
      "\n",
      "Test Paper 779:\n",
      "\n",
      "Test Paper 780:\n",
      "\n",
      "Test Paper 781:\n",
      "\n",
      "Test Paper 782:\n",
      "\n",
      "Test Paper 783:\n",
      "\n",
      "Test Paper 784:\n",
      "\n",
      "Test Paper 785:\n",
      "\n",
      "Test Paper 786:\n",
      "\n",
      "Test Paper 787:\n",
      "\n",
      "Test Paper 788:\n",
      "\n",
      "Test Paper 789:\n",
      "\n",
      "Test Paper 790:\n",
      "\n",
      "Test Paper 791:\n",
      "\n",
      "Test Paper 792:\n",
      "\n",
      "Test Paper 793:\n",
      "\n",
      "Test Paper 794:\n",
      "\n",
      "Test Paper 795:\n",
      "\n",
      "Test Paper 796:\n",
      "\n",
      "Test Paper 797:\n",
      "\n",
      "Test Paper 798:\n",
      "\n",
      "Test Paper 799:\n",
      "\n",
      "Test Paper 800:\n",
      "\n",
      "Test Paper 801:\n",
      "\n",
      "Test Paper 802:\n",
      "\n",
      "Test Paper 803:\n",
      "\n",
      "Test Paper 804:\n",
      "\n",
      "Test Paper 805:\n",
      "\n",
      "Test Paper 806:\n",
      "\n",
      "Test Paper 807:\n",
      "\n",
      "Test Paper 808:\n",
      "\n",
      "Test Paper 809:\n",
      "\n",
      "Test Paper 810:\n",
      "\n",
      "Test Paper 811:\n",
      "\n",
      "Test Paper 812:\n",
      "\n",
      "Test Paper 813:\n",
      "\n",
      "Test Paper 814:\n",
      "\n",
      "Test Paper 815:\n",
      "\n",
      "Test Paper 816:\n",
      "\n",
      "Test Paper 817:\n",
      "\n",
      "Test Paper 818:\n",
      "\n",
      "Test Paper 819:\n",
      "\n",
      "Test Paper 820:\n",
      "\n",
      "Test Paper 821:\n",
      "\n",
      "Test Paper 822:\n",
      "\n",
      "Test Paper 823:\n",
      "\n",
      "Test Paper 824:\n",
      "\n",
      "Test Paper 825:\n",
      "\n",
      "Test Paper 826:\n",
      "\n",
      "Test Paper 827:\n",
      "\n",
      "Test Paper 828:\n",
      "\n",
      "Test Paper 829:\n",
      "\n",
      "Test Paper 830:\n",
      "\n",
      "Test Paper 831:\n",
      "\n",
      "Test Paper 832:\n",
      "\n",
      "Test Paper 833:\n",
      "\n",
      "Test Paper 834:\n",
      "\n",
      "Test Paper 835:\n",
      "\n",
      "Test Paper 836:\n",
      "\n",
      "Test Paper 837:\n",
      "\n",
      "Test Paper 838:\n",
      "\n",
      "Test Paper 839:\n",
      "\n",
      "Test Paper 840:\n",
      "\n",
      "Test Paper 841:\n",
      "\n",
      "Test Paper 842:\n",
      "\n",
      "Test Paper 843:\n",
      "\n",
      "Test Paper 844:\n",
      "\n",
      "Test Paper 845:\n",
      "\n",
      "Test Paper 846:\n",
      "\n",
      "Test Paper 847:\n",
      "\n",
      "Test Paper 848:\n",
      "\n",
      "Test Paper 849:\n",
      "\n",
      "Test Paper 850:\n",
      "\n",
      "Test Paper 851:\n",
      "\n",
      "Test Paper 852:\n",
      "\n",
      "Test Paper 853:\n",
      "\n",
      "Test Paper 854:\n",
      "\n",
      "Test Paper 855:\n",
      "\n",
      "Test Paper 856:\n",
      "\n",
      "Test Paper 857:\n",
      "\n",
      "Test Paper 858:\n",
      "\n",
      "Test Paper 859:\n",
      "\n",
      "Test Paper 860:\n",
      "\n",
      "Test Paper 861:\n",
      "\n",
      "Test Paper 862:\n",
      "\n",
      "Test Paper 863:\n",
      "\n",
      "Test Paper 864:\n",
      "\n",
      "Test Paper 865:\n",
      "\n",
      "Test Paper 866:\n",
      "\n",
      "Test Paper 867:\n",
      "\n",
      "Test Paper 868:\n",
      "\n",
      "Test Paper 869:\n",
      "\n",
      "Test Paper 870:\n",
      "\n",
      "Test Paper 871:\n",
      "\n",
      "Test Paper 872:\n",
      "\n",
      "Test Paper 873:\n",
      "\n",
      "Test Paper 874:\n",
      "\n",
      "Test Paper 875:\n",
      "\n",
      "Test Paper 876:\n",
      "\n",
      "Test Paper 877:\n",
      "\n",
      "Test Paper 878:\n",
      "\n",
      "Test Paper 879:\n",
      "\n",
      "Test Paper 880:\n",
      "\n",
      "Test Paper 881:\n",
      "\n",
      "Test Paper 882:\n",
      "\n",
      "Test Paper 883:\n",
      "\n",
      "Test Paper 884:\n",
      "\n",
      "Test Paper 885:\n",
      "\n",
      "Test Paper 886:\n",
      "\n",
      "Test Paper 887:\n",
      "\n",
      "Test Paper 888:\n",
      "\n",
      "Test Paper 889:\n",
      "\n",
      "Test Paper 890:\n",
      "\n",
      "Test Paper 891:\n",
      "\n",
      "Test Paper 892:\n",
      "\n",
      "Test Paper 893:\n",
      "\n",
      "Test Paper 894:\n",
      "\n",
      "Test Paper 895:\n",
      "\n",
      "Test Paper 896:\n",
      "\n",
      "Test Paper 897:\n",
      "\n",
      "Test Paper 898:\n",
      "\n",
      "Test Paper 899:\n",
      "\n",
      "Test Paper 900:\n",
      "\n",
      "Test Paper 901:\n",
      "\n",
      "Test Paper 902:\n",
      "\n",
      "Test Paper 903:\n",
      "\n",
      "Test Paper 904:\n",
      "\n",
      "Test Paper 905:\n",
      "\n",
      "Test Paper 906:\n",
      "\n",
      "Test Paper 907:\n",
      "\n",
      "Test Paper 908:\n",
      "\n",
      "Test Paper 909:\n",
      "\n",
      "Test Paper 910:\n",
      "\n",
      "Test Paper 911:\n",
      "\n",
      "Test Paper 912:\n",
      "\n",
      "Test Paper 913:\n",
      "\n",
      "Test Paper 914:\n",
      "\n",
      "Test Paper 915:\n",
      "\n",
      "Test Paper 916:\n",
      "\n",
      "Test Paper 917:\n",
      "\n",
      "Test Paper 918:\n",
      "\n",
      "Test Paper 919:\n",
      "\n",
      "Test Paper 920:\n",
      "\n",
      "Test Paper 921:\n",
      "\n",
      "Test Paper 922:\n",
      "\n",
      "Test Paper 923:\n",
      "\n",
      "Test Paper 924:\n",
      "\n",
      "Test Paper 925:\n",
      "\n",
      "Test Paper 926:\n",
      "\n",
      "Test Paper 927:\n",
      "\n",
      "Test Paper 928:\n",
      "\n",
      "Test Paper 929:\n",
      "\n",
      "Test Paper 930:\n",
      "\n",
      "Test Paper 931:\n",
      "\n",
      "Test Paper 932:\n",
      "\n",
      "Test Paper 933:\n",
      "\n",
      "Test Paper 934:\n",
      "\n",
      "Test Paper 935:\n",
      "\n",
      "Test Paper 936:\n",
      "\n",
      "Test Paper 937:\n",
      "\n",
      "Test Paper 938:\n",
      "\n",
      "Test Paper 939:\n",
      "\n",
      "Test Paper 940:\n",
      "\n",
      "Test Paper 941:\n",
      "\n",
      "Test Paper 942:\n",
      "\n",
      "Test Paper 943:\n",
      "\n",
      "Test Paper 944:\n",
      "\n",
      "Test Paper 945:\n",
      "\n",
      "Test Paper 946:\n",
      "\n",
      "Test Paper 947:\n",
      "\n",
      "Test Paper 948:\n",
      "\n",
      "Test Paper 949:\n",
      "\n",
      "Test Paper 950:\n",
      "\n",
      "Test Paper 951:\n",
      "\n",
      "Test Paper 952:\n",
      "\n",
      "Test Paper 953:\n",
      "\n",
      "Test Paper 954:\n",
      "\n",
      "Test Paper 955:\n",
      "\n",
      "Test Paper 956:\n",
      "\n",
      "Test Paper 957:\n",
      "\n",
      "Test Paper 958:\n",
      "\n",
      "Test Paper 959:\n",
      "\n",
      "Test Paper 960:\n",
      "\n",
      "Test Paper 961:\n",
      "\n",
      "Test Paper 962:\n",
      "\n",
      "Test Paper 963:\n",
      "\n",
      "Test Paper 964:\n",
      "\n",
      "Test Paper 965:\n",
      "\n",
      "Test Paper 966:\n",
      "\n",
      "Test Paper 967:\n",
      "\n",
      "Test Paper 968:\n",
      "\n",
      "Test Paper 969:\n",
      "\n",
      "Test Paper 970:\n",
      "\n",
      "Test Paper 971:\n",
      "\n",
      "Test Paper 972:\n",
      "\n",
      "Test Paper 973:\n",
      "\n",
      "Test Paper 974:\n",
      "\n",
      "Test Paper 975:\n",
      "\n",
      "Test Paper 976:\n",
      "\n",
      "Test Paper 977:\n",
      "\n",
      "Test Paper 978:\n",
      "\n",
      "Test Paper 979:\n",
      "\n",
      "Test Paper 980:\n",
      "\n",
      "Test Paper 981:\n",
      "\n",
      "Test Paper 982:\n",
      "\n",
      "Test Paper 983:\n",
      "\n",
      "Test Paper 984:\n",
      "\n",
      "Test Paper 985:\n",
      "\n",
      "Test Paper 986:\n",
      "\n",
      "Test Paper 987:\n",
      "\n",
      "Test Paper 988:\n",
      "\n",
      "Test Paper 989:\n",
      "\n",
      "Test Paper 990:\n",
      "\n",
      "Test Paper 991:\n",
      "\n",
      "Test Paper 992:\n",
      "\n",
      "Test Paper 993:\n",
      "\n",
      "Test Paper 994:\n",
      "\n",
      "Test Paper 995:\n",
      "\n",
      "Test Paper 996:\n",
      "\n",
      "Test Paper 997:\n",
      "\n",
      "Test Paper 998:\n",
      "\n",
      "Test Paper 999:\n",
      "\n",
      "Test Paper 1000:\n",
      "\n",
      "Test Paper 1001:\n",
      "\n",
      "Test Paper 1002:\n",
      "\n",
      "Test Paper 1003:\n",
      "\n",
      "Test Paper 1004:\n",
      "\n",
      "Test Paper 1005:\n",
      "\n",
      "Test Paper 1006:\n",
      "\n",
      "Test Paper 1007:\n",
      "\n",
      "Test Paper 1008:\n",
      "\n",
      "Test Paper 1009:\n",
      "\n",
      "Test Paper 1010:\n",
      "\n",
      "Test Paper 1011:\n",
      "\n",
      "Test Paper 1012:\n",
      "\n",
      "Test Paper 1013:\n",
      "\n",
      "Test Paper 1014:\n",
      "\n",
      "Test Paper 1015:\n",
      "\n",
      "Test Paper 1016:\n",
      "\n",
      "Test Paper 1017:\n",
      "\n",
      "Test Paper 1018:\n",
      "\n",
      "Test Paper 1019:\n",
      "\n",
      "Test Paper 1020:\n",
      "\n",
      "Test Paper 1021:\n",
      "\n",
      "Test Paper 1022:\n",
      "\n",
      "Test Paper 1023:\n",
      "\n",
      "Test Paper 1024:\n",
      "\n",
      "Test Paper 1025:\n",
      "\n",
      "Test Paper 1026:\n",
      "\n",
      "Test Paper 1027:\n",
      "\n",
      "Test Paper 1028:\n",
      "\n",
      "Test Paper 1029:\n",
      "\n",
      "Test Paper 1030:\n",
      "\n",
      "Test Paper 1031:\n",
      "\n",
      "Test Paper 1032:\n",
      "\n",
      "Test Paper 1033:\n",
      "\n",
      "Test Paper 1034:\n",
      "\n",
      "Test Paper 1035:\n",
      "\n",
      "Test Paper 1036:\n",
      "\n",
      "Test Paper 1037:\n",
      "\n",
      "Test Paper 1038:\n",
      "\n",
      "Test Paper 1039:\n",
      "\n",
      "Test Paper 1040:\n",
      "\n",
      "Test Paper 1041:\n",
      "\n",
      "Test Paper 1042:\n",
      "\n",
      "Test Paper 1043:\n",
      "\n",
      "Test Paper 1044:\n",
      "\n",
      "Test Paper 1045:\n",
      "\n",
      "Test Paper 1046:\n",
      "\n",
      "Test Paper 1047:\n",
      "\n",
      "Test Paper 1048:\n",
      "\n",
      "Test Paper 1049:\n",
      "\n",
      "Test Paper 1050:\n",
      "\n",
      "Test Paper 1051:\n",
      "\n",
      "Test Paper 1052:\n",
      "\n",
      "Test Paper 1053:\n",
      "\n",
      "Test Paper 1054:\n",
      "\n",
      "Test Paper 1055:\n",
      "\n",
      "Test Paper 1056:\n",
      "\n",
      "Test Paper 1057:\n",
      "\n",
      "Test Paper 1058:\n",
      "\n",
      "Test Paper 1059:\n",
      "\n",
      "Test Paper 1060:\n",
      "\n",
      "Test Paper 1061:\n",
      "\n",
      "Test Paper 1062:\n",
      "\n",
      "Test Paper 1063:\n",
      "\n",
      "Test Paper 1064:\n",
      "\n",
      "Test Paper 1065:\n",
      "\n",
      "Test Paper 1066:\n",
      "\n",
      "Test Paper 1067:\n",
      "\n",
      "Test Paper 1068:\n",
      "\n",
      "Test Paper 1069:\n",
      "\n",
      "Test Paper 1070:\n",
      "\n",
      "Test Paper 1071:\n",
      "\n",
      "Test Paper 1072:\n",
      "\n",
      "Test Paper 1073:\n",
      "\n",
      "Test Paper 1074:\n",
      "\n",
      "Test Paper 1075:\n",
      "\n",
      "Test Paper 1076:\n",
      "\n",
      "Test Paper 1077:\n",
      "\n",
      "Test Paper 1078:\n",
      "\n",
      "Test Paper 1079:\n",
      "\n",
      "Test Paper 1080:\n",
      "\n",
      "Test Paper 1081:\n",
      "\n",
      "Test Paper 1082:\n",
      "\n",
      "Test Paper 1083:\n",
      "\n",
      "Test Paper 1084:\n",
      "\n",
      "Test Paper 1085:\n",
      "\n",
      "Test Paper 1086:\n",
      "\n",
      "Test Paper 1087:\n",
      "\n",
      "Test Paper 1088:\n",
      "\n",
      "Test Paper 1089:\n",
      "\n",
      "Test Paper 1090:\n",
      "\n",
      "Test Paper 1091:\n",
      "\n",
      "Test Paper 1092:\n",
      "\n",
      "Test Paper 1093:\n",
      "\n",
      "Test Paper 1094:\n",
      "\n",
      "Test Paper 1095:\n",
      "\n",
      "Test Paper 1096:\n",
      "\n",
      "Test Paper 1097:\n",
      "\n",
      "Test Paper 1098:\n",
      "\n",
      "Test Paper 1099:\n",
      "\n",
      "Test Paper 1100:\n",
      "\n",
      "Test Paper 1101:\n",
      "\n",
      "Test Paper 1102:\n",
      "\n",
      "Test Paper 1103:\n",
      "\n",
      "Test Paper 1104:\n",
      "\n",
      "Test Paper 1105:\n",
      "\n",
      "Test Paper 1106:\n",
      "\n",
      "Test Paper 1107:\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all papers using the trained model\n",
    "def get_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "            batch_embeddings = model(input_ids, attention_mask)  # Keep as CUDA tensor\n",
    "            embeddings.append(batch_embeddings)  # Store without converting to NumPy\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0), np.array(paper_indices)  # Return PyTorch tensor\n",
    "\n",
    "# Get embeddings for train and test sets\n",
    "train_dataloader = DataLoader(PaperDataset(X_train, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(PaperDataset(X_test, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "\n",
    "train_embeddings, train_indices = get_embeddings(train_dataloader, model)\n",
    "test_embeddings, test_indices = get_embeddings(test_dataloader, model)\n",
    "\n",
    "# Compute cosine similarity in CUDA\n",
    "train_embeddings = F.normalize(train_embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "test_embeddings = F.normalize(test_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sample Test Embedding:\", test_embeddings[0][:15])  # First 15 values\n",
    "print(\"Sample Train Embedding:\", train_embeddings[0][:15])  # First 15 values\n",
    "\n",
    "similarity_matrix = torch.matmul(test_embeddings, train_embeddings.T).cpu().numpy()  # Cosine similarity\n",
    "\n",
    "# Select top-N most similar papers\n",
    "top_n = 10\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_n]\n",
    "\n",
    "# Print recommended papers\n",
    "recommended_paper_ids = []\n",
    "\n",
    "for i, test_idx in enumerate(top_indices):\n",
    "    recommended_for_test = []\n",
    "    #print(f\"\\nTest Paper {i+1}:\")\n",
    "    \n",
    "    for j, train_idx in enumerate(test_idx):\n",
    "        recommended_paper_id = X_train.iloc[train_indices[train_idx]][\"id\"]\n",
    "        recommended_for_test.append(recommended_paper_id)\n",
    "        \n",
    "        #print(f\"  {j+1}. Recommended Paper ID: {recommended_paper_id} (Similarity: {similarity_matrix[i, train_idx]:.4f})\")\n",
    "    \n",
    "    recommended_paper_ids.append(recommended_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d4e6355-963f-40ff-bf15-f753258e8677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cosine Similarity (Top 10): 0.691122\n",
      "Median Cosine Similarity (Top 10): 0.68674666\n"
     ]
    }
   ],
   "source": [
    "cosine_mean, cosine_median = cosine_similarity_mean_median(similarity_matrix)\n",
    "\n",
    "print(\"Mean Cosine Similarity (Top 10):\", np.mean(cosine_mean))\n",
    "print(\"Median Cosine Similarity (Top 10):\", np.mean(cosine_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69089e5c-c0eb-461a-b800-fe98c2dc2e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get test paper texts\n",
    "test_texts = (X_test[\"title\"] + \" \" + X_test[\"abstract\"]).tolist()\n",
    "\n",
    "top10_texts = []\n",
    "top10_embeddings = []\n",
    "\n",
    "for i in range(len(top_indices)):\n",
    "    top_paper_texts = []\n",
    "    top_paper_embeds = []\n",
    "\n",
    "    for j in top_indices[i]:\n",
    "        paper = X_train.iloc[train_indices[j]]\n",
    "        text = paper[\"title\"] + \" \" + paper[\"abstract\"]\n",
    "        top_paper_texts.append(text)\n",
    "\n",
    "        # Detach from graph, move to CPU, then convert to NumPy\n",
    "        vector = train_embeddings[j].detach().cpu().numpy()\n",
    "        top_paper_embeds.append(vector)\n",
    "\n",
    "    top10_texts.append(top_paper_texts)\n",
    "    top10_embeddings.append(top_paper_embeds)\n",
    "\n",
    "top10_embeddings = np.array(top10_embeddings)  # Shape: (num_test, 10, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92af7a75-f560-4fce-84d8-c483c7855b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Shape (Test): (1107, 5000)\n",
      "TF-IDF Shape (Top 10): (1107, 10, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix, tfidf_test_vecs, tfidf_top10_vecs = compute_tfidf_similarity(test_texts, top10_texts)\n",
    "\n",
    "# Optionally compare TF-IDF vectors\n",
    "print(\"TF-IDF Shape (Test):\", tfidf_test_vecs.shape)\n",
    "print(\"TF-IDF Shape (Top 10):\", tfidf_top10_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c25dcc7a-b126-4ecd-bbd6-00b6ac8540d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pearson Correlation (Top 10): 0.6913492523396649\n",
      "Median Pearson Correlation (Top 10): 0.6869894434565695\n"
     ]
    }
   ],
   "source": [
    "# Move test embeddings to CPU and convert to NumPy\n",
    "test_embeddings_np = test_embeddings.detach().cpu().numpy()\n",
    "\n",
    "# top10_embeddings should already be NumPy from the earlier fix\n",
    "# (check if needed: top10_embeddings = np.array([...]) was already done)\n",
    "\n",
    "# Now call the Pearson correlation evaluator\n",
    "pearson_mean, pearson_median = compute_pearson_correlation(test_embeddings_np, top10_embeddings)\n",
    "\n",
    "print(\"Mean Pearson Correlation (Top 10):\", np.mean(pearson_mean))\n",
    "print(\"Median Pearson Correlation (Top 10):\", np.mean(pearson_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd9d66-4733-46fb-8fc8-f8fdce291ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
