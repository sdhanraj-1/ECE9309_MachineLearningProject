{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d664a8f3-deb0-47ee-96dd-4a094ba076f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60393e37-8f4c-4abc-866f-024d0d1d697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7000, 8), Validation: (2000, 8), Test: (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "# train 70%, val 20%, test 10%\n",
    "df = pd.read_csv(\"Research_Papers_Dataset_clean.csv\")\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "df = df.head(10000)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "X_val, X_test = train_test_split(X_temp, test_size=1/3, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0718d3bf-0782-450d-8a8b-ef1130b0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.data.iloc[idx][\"title\"])\n",
    "        abstract = str(self.data.iloc[idx][\"abstract\"])\n",
    "        input_text = title + \" \" + abstract  # Combine title and abstract\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"paper_index\": idx  # Used for retrieval\n",
    "        }\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset_train = PaperDataset(X_train, tokenizer, 256)\n",
    "dataset_val = PaperDataset(X_val, tokenizer, 256)\n",
    "dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f242b441-9990-4523-92cc-243e5fa7facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input IDs Shape: torch.Size([8, 256])\n",
      "Batch Attention Mask Shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Attention Mask Shape:\", batch[\"attention_mask\"].shape)\n",
    "    break  # Check only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50964c3e-9433-4a3a-8fa7-40b5575ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRecommender(nn.Module):\n",
    "    def __init__(self, model_name, embedding_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim  # Ensure this is correctly set\n",
    "\n",
    "        # Ensure projection layer matches `embedding_dim`\n",
    "        self.fc = nn.Linear(self.encoder.config.hidden_size, embedding_dim)  \n",
    "        \n",
    "        # Multiheaded attention layer using the correct `embedding_dim`\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = nn.functional.normalize\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(next(self.parameters()).device)\n",
    "        attention_mask = attention_mask.to(next(self.parameters()).device)\n",
    "\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "\n",
    "        embedding = self.fc(self.dropout(pooled_output))  # Apply projection\n",
    "        embedding = embedding.unsqueeze(0)  # Reshape for MultiheadAttention\n",
    "\n",
    "        # Multiheaded Attention\n",
    "        attn_output, _ = self.attention(embedding, embedding, embedding)\n",
    "        attn_output = attn_output.squeeze(0)  # Remove batch dim\n",
    "\n",
    "        return self.normalize(attn_output, p=2, dim=1)  # Normalize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3558e342-5329-4e21-b2fd-36a7a60540dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0457, 0.0180,  ..., 0.0278, 0.0782, 0.0898],\n",
      "        [0.0457, 1.0000, 0.0227,  ..., 0.0343, 0.0432, 0.0933],\n",
      "        [0.0180, 0.0227, 1.0000,  ..., 0.0205, 0.0161, 0.0346],\n",
      "        ...,\n",
      "        [0.0278, 0.0343, 0.0205,  ..., 1.0000, 0.0207, 0.0474],\n",
      "        [0.0782, 0.0432, 0.0161,  ..., 0.0207, 1.0000, 0.0760],\n",
      "        [0.0898, 0.0933, 0.0346,  ..., 0.0474, 0.0760, 1.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([[ True, False, False,  ..., False, False,  True],\n",
      "        [False,  True, False,  ..., False, False,  True],\n",
      "        [False, False,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True, False, False],\n",
      "        [False, False, False,  ..., False,  True, False],\n",
      "        [ True,  True, False,  ..., False, False,  True]], device='cuda:0')\n",
      "tensor([[False,  True,  True,  ...,  True,  True, False],\n",
      "        [ True, False,  True,  ...,  True,  True, False],\n",
      "        [ True,  True, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True, False,  True],\n",
      "        [False, False,  True,  ...,  True,  True, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf_similarity(df):\n",
    "    corpus = df['title'] + \" \" + df['abstract']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "similarity_matrix_np = compute_tfidf_similarity(df)\n",
    "# Convert to tensor for GPU computation\n",
    "similarity_matrix = torch.tensor(similarity_matrix_np, dtype=torch.float32).to(\"cuda\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "threshold = 0.0785\n",
    "\n",
    "positive_pairs = similarity_matrix > threshold\n",
    "negative_pairs = ~positive_pairs\n",
    "print(positive_pairs)\n",
    "print(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b2aaef-c81f-4998-9ac3-44d3d2904eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85th percentile: 0.0756\n",
      "90th percentile: 0.0839\n",
      "95th percentile: 0.0980\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#Organize Data\n",
    "similarity_values_np = similarity_matrix_np.flatten()\n",
    "similarity_values_np_filtered = similarity_values_np[similarity_values_np <= 0.25]\n",
    "X = pd.Series(similarity_values_np_filtered, name=\"(Similarity Matrix Distribution)\")\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(X, bins=25, color=\"g\", ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Compute percentiles\n",
    "percentiles = [85, 90, 95]\n",
    "percentile_values = np.percentile(similarity_values_np_filtered, percentiles)\n",
    "\n",
    "# Print results\n",
    "for p, val in zip(percentiles, percentile_values):\n",
    "    print(f\"{p}th percentile: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb2e70a-fe2c-4669-ad62-8c38b01505f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, loss, path=\"./paper_recommender_Large.pth\"):\n",
    "    \"\"\"Save the model, optimizer state, and training metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Model saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa12bbd-aa94-4282-b3e9-4bafbbf51259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path = \"./paper_recommender_Large.pth\"):\n",
    "    \"\"\"Load the model and optionally the optimizer.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Model loaded from {path}, trained until epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Optimizer state restored.\")\n",
    "\n",
    "    return checkpoint[\"epoch\"], checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc03d8a2-8a92-4a9b-8b63-b2549cc36377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5):\n",
    "    \"\"\"\n",
    "    Contrastive loss using TF-IDF similarity as ground truth.\n",
    "    \n",
    "    embeddings: (batch_size, embedding_dim)\n",
    "    similarity_matrix: Precomputed TF-IDF cosine similarity.\n",
    "    indices: Indices of batch samples in dataset.\n",
    "    margin: Margin for contrastive loss.\n",
    "    \"\"\"\n",
    "    batch_size = embeddings.shape[0]\n",
    "\n",
    "    # Ensure embeddings are L2 normalized\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = torch.mm(embeddings, embeddings.T)  # (batch_size, batch_size)\n",
    "    cosine_distances = 1 - cosine_sim  # Convert similarity to distance\n",
    "\n",
    "    # Extract ground truth similarity values for batch samples\n",
    "    ground_truth_similarities = similarity_matrix[indices][:, indices]\n",
    "\n",
    "    # Define positive and negative pairs\n",
    "    threshold = 0.1205  # Adjust this value if needed\n",
    "    positive_pairs = (ground_truth_similarities > threshold).float()\n",
    "    negative_pairs = (ground_truth_similarities <= threshold).float()\n",
    "\n",
    "    # Compute losses\n",
    "    positive_loss = (cosine_distances * positive_pairs).sum() / (positive_pairs.sum() + 1e-8)\n",
    "    negative_loss = torch.clamp(margin - cosine_distances, min=0) * negative_pairs\n",
    "    negative_loss = negative_loss.sum() / (negative_pairs.sum() + 1e-8)\n",
    "\n",
    "    loss = positive_loss + negative_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea65a89c-88ae-45e3-b7e1-957058b21736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [256, 512, 768])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2,4,8,16])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "\n",
    "    # Initialize model with selected hyperparameters\n",
    "    model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                             embedding_dim, num_heads, dropout).to(\"cuda\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False)\n",
    "\n",
    "    num_epochs = 3  # Use fewer epochs for tuning\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(dataloader_train)\n",
    "\n",
    "        # Validation step (compute loss on validation set)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_val:\n",
    "                input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "                attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "                indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "                embeddings = model(input_ids, attention_mask)\n",
    "                val_loss += contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(dataloader_val)\n",
    "        total_loss += avg_val_loss\n",
    "\n",
    "    return total_loss / num_epochs  # Minimize validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1396d4-1570-443a-93d7-8ffc52a66caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 12:55:20,959] A new study created in memory with name: no-name-fc3f627a-ca05-4116-97a3-71ba41f3ec96\n",
      "[I 2025-03-16 13:21:38,183] Trial 0 finished with value: 0.07100067871809006 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.17640719951859918}. Best is trial 0 with value: 0.07100067871809006.\n",
      "[I 2025-03-16 13:46:51,205] Trial 1 finished with value: 0.08115892608650029 and parameters: {'embedding_dim': 512, 'num_heads': 8, 'dropout': 0.1464813236867561}. Best is trial 0 with value: 0.07100067871809006.\n",
      "[I 2025-03-16 14:12:07,728] Trial 2 finished with value: 0.062067316538887106 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.10617760648864663}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 14:37:24,300] Trial 3 finished with value: 0.09676277039945126 and parameters: {'embedding_dim': 768, 'num_heads': 4, 'dropout': 0.21207132702581954}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 15:02:36,747] Trial 4 finished with value: 0.08525975119943419 and parameters: {'embedding_dim': 512, 'num_heads': 16, 'dropout': 0.1394603225273703}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 15:27:49,242] Trial 5 finished with value: 0.17265357257425787 and parameters: {'embedding_dim': 512, 'num_heads': 4, 'dropout': 0.2849636618364303}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 15:53:00,493] Trial 6 finished with value: 0.1070131125797828 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.19433391843606068}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 16:18:16,874] Trial 7 finished with value: 0.06245046228048159 and parameters: {'embedding_dim': 768, 'num_heads': 4, 'dropout': 0.12171978197908014}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 16:43:33,146] Trial 8 finished with value: 0.07351670568502353 and parameters: {'embedding_dim': 768, 'num_heads': 2, 'dropout': 0.18513235790594296}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 17:08:53,181] Trial 9 finished with value: 0.13682415416340032 and parameters: {'embedding_dim': 768, 'num_heads': 4, 'dropout': 0.25045186146067544}. Best is trial 2 with value: 0.062067316538887106.\n",
      "[I 2025-03-16 17:34:12,520] Trial 10 finished with value: 0.06172989184595645 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.10114584059258339}. Best is trial 10 with value: 0.06172989184595645.\n",
      "[I 2025-03-16 17:59:23,381] Trial 11 finished with value: 0.06843521365794974 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.1045644781679962}. Best is trial 10 with value: 0.06172989184595645.\n",
      "[I 2025-03-16 18:24:41,241] Trial 12 finished with value: 0.06021268891586078 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.10103513964785935}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 18:50:00,628] Trial 13 finished with value: 0.07416176415917775 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.15223129372581828}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 19:15:13,093] Trial 14 finished with value: 0.11610416426261265 and parameters: {'embedding_dim': 256, 'num_heads': 8, 'dropout': 0.22680049096019128}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 19:40:23,915] Trial 15 finished with value: 0.07796186386297148 and parameters: {'embedding_dim': 256, 'num_heads': 16, 'dropout': 0.12618765690198308}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 20:05:34,567] Trial 16 finished with value: 0.07149272195630085 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.16406955192526043}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 20:30:45,179] Trial 17 finished with value: 0.06154242346979056 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.10065210041439608}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 20:55:56,054] Trial 18 finished with value: 0.062285665108259614 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.12209414003694065}. Best is trial 12 with value: 0.06021268891586078.\n",
      "[I 2025-03-16 21:21:06,904] Trial 19 finished with value: 0.16770457521577675 and parameters: {'embedding_dim': 256, 'num_heads': 8, 'dropout': 0.29213567050723827}. Best is trial 12 with value: 0.06021268891586078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.10103513964785935}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # We want to minimize loss\n",
    "study.optimize(objective, n_trials=20)  # Run 20 trials\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9619723-25a6-49e5-ab88-42fea483e730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 0.0673\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Epoch 2, Avg Loss: 0.0624\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Epoch 3, Avg Loss: 0.0668\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 4. Debug required.\n",
      "Warning: Zero loss at epoch 4. Debug required.\n",
      "Epoch 4, Avg Loss: 0.0620\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Epoch 5, Avg Loss: 0.0587\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Epoch 6, Avg Loss: 0.0597\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Epoch 7, Avg Loss: 0.0611\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Warning: Zero loss at epoch 8. Debug required.\n",
      "Epoch 8, Avg Loss: 0.0624\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Epoch 9, Avg Loss: 0.0586\n",
      "Model saved at paper_recommender_Large.pth\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Epoch 10, Avg Loss: 0.0610\n",
      "Model saved at paper_recommender_Large.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               study.best_params[\"embedding_dim\"],\n",
    "                               study.best_params[\"num_heads\"],\n",
    "                               study.best_params[\"dropout\"]).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            print(f\"Warning: Zero loss at epoch {epoch+1}. Debug required.\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "    save_model(model, optimizer, epoch + 1, avg_loss, \"paper_recommender_Large.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34304d1d-4b28-47f7-b4c2-f16cf607b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from paper_recommender_Large.pth, trained until epoch 10\n",
      "Optimizer state restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaperRecommender(\n",
       "  (encoder): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.15, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               256,\n",
    "                               2,\n",
    "                               0.15).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Load the model (after training)\n",
    "epoch, loss = load_model(model, optimizer, \"paper_recommender_Large.pth\")\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b1bddb-dea3-4a5d-9e80-3da76064b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, model, df, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode query\n",
    "    query_tokens = tokenizer(query, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(query_tokens[\"input_ids\"], query_tokens[\"attention_mask\"]).cpu().numpy()\n",
    "\n",
    "    # Compute Euclidean distances between query and all paper embeddings\n",
    "    paper_embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_input_ids, batch_attention_mask).cpu().numpy()\n",
    "            paper_embeddings.append(batch_embeddings)\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())  # Store original indices\n",
    "\n",
    "    paper_embeddings = np.vstack(paper_embeddings)  # Stack all embeddings\n",
    "    paper_indices = np.array(paper_indices)\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = np.linalg.norm(paper_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # Get top-k closest papers (smallest distances)\n",
    "    top_indices = np.argsort(distances)[:top_k]\n",
    "\n",
    "    print(\"\\nRecommended Papers:\")\n",
    "    for idx in top_indices:\n",
    "        paper_idx = paper_indices[idx]\n",
    "        print(f\"Title: {df.iloc[paper_idx]['title']}\\nAbstract: {df.iloc[paper_idx]['abstract']}\\nDistance: {distances[idx]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a25f812-4c97-4260-a369-d4a766f40744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Papers:\n",
      "Title: Design space abstraction and metamodeling for embedded systems design space exploration\n",
      "Abstract: In this paper, we present a design space exploration (DSE) method for embedded systems, which represents the design space as a categorical graph product, in order to overcome the challenge of performing multiple DSE activities, such as task mapping, processor allocation, and software binding. Moreover, the method adopts a Model-Driven Engineering (MDE) approach, defining a design space metamodel to represent the categorical graph product and other DSE concepts, such as solutions, costs, and DSE activities. Furthermore, exploiting the MDE approach, we use model-to-model transformation rules to implement the design constraints, which guide and prune the design space. The method is applied to the design of a real-life application, and experiments demonstrate its effectiveness.\n",
      "Distance: 0.7357\n",
      "\n",
      "Title: Data sharing in mobile ad-hoc networks - a study of replication and performance in the MIDAS data space\n",
      "Abstract: The dynamic nature of mobile ad-hoc networks (MANETs) can easily lead to data being inaccessible due to constant route changes and network partitions. One method often used for increasing reliability and availability of data is replication. However, replication comes with costs, those of transferring and storing data and keeping track of consistency between replicas. For that reason, we have identified the core factors impacting the resulting network traffic. We have performed experimental studies with a real world prototype of a distributed data management system for MANETs, called MIDAS data space. Furthermore, we have done an extensive simulation study showing where table replicas should be placed in the network, in order to minimise network traffic generated by access to the databases and by the synchronisation data. The results of the experiments are consistent and show that by using clustering techniques we can achieve close-to-optimal traffic by placing replicas on approximately 10% of nodes.\n",
      "Distance: 0.7405\n",
      "\n",
      "Title: Rigorous coupled wave transform coding of images\n",
      "Abstract: Image coding based on a linear transform has been extensively studied. On the other hand, the only nonlinear transform used for image coding, to our best knowledge, is the fractal coding. We proposed a rigorous coupled wave (RCW) nonlinear transform for image coding. Here, we model the image as a surface relief grating where the gray-scale value of the image pixel is directly converted to the pixel's depth of the grating. When a uniform light wave is shined through the grating, the amplitude of the backward- and forward-diffracted waves are quantized and recorded, while the phase of the backward- and forward-diffracted waves can be calculated based on some a priori boundary conditions. Then these quantized diffraction waves can be used to reconstruct the depth of the grating (or image signal) based on Maxwell's equations. Scalar quantizations (SQ) combined with variable length coding are tested. Respectively, the PSNR of the SQ coded RCW transformed image at a low bit rate (0.1 bit/pixel) is 30.83 dB, while the state-of-the-art JPEG-2000 only achieves 28.61 dB.\n",
      "Distance: 0.7465\n",
      "\n",
      "Title: MoRePriv: mobile OS support for application personalization and privacy\n",
      "Abstract: Privacy and personalization of mobile experiences are inherently in conflict: better personalization demands knowing more about the user, potentially violating user privacy. A promising approach to mitigate this tension is to migrate personalization to the client, an approach dubbed  client-side personalization . This paper advocates for  operating system support  for client-side personalization and describes MoRePriv, an operating system  service  implemented in the Windows Phone OS. We argue that personalization support should be as ubiquitous as location support, and should be provided by a unified system within the OS, instead of by individual apps.   We aim to provide a solution that will stoke innovation around mobile personalization. To enable easy application personalization, MoRePriv approximates users' interests using  personae  such as technophile or business executive. Using a number of case studies and crowd-sourced user studies, we illustrate how more complex personalization tasks can be achieved using MoRePriv.   For privacy protection, MoRePriv distills sensitive user information to a coarse-grained  profile , which limits the potential damage from information leaks. We see MoRePriv as a way to increase end-user privacy by enabling client-side computing, thus minimizing the need to share user data with the server. As such, MoRePriv shepherds the ecosystem towards a better privacy stance by  nudging  developers away from today's privacy-violating practices. Furthermore, MoRePriv can be  combined  with privacy-enhancing technologies and is complimentary to recent advances in data leak detection.\n",
      "Distance: 0.7732\n",
      "\n",
      "Title: A separate compilation extension to standard ML\n",
      "Abstract: We present an extension to Standard ML, called SMLSC, to support separate compilation. The system gives meaning to individual program fragments, called units. Units may depend on one another in a way specified by the programmer. A dependency may be mediated by an interface (the type of a unit); if so, the units can be compiled separately. Otherwise, they must be compiled in sequence. We also propose a methodology for programming in SMLSC that reflects code development practice and avoids syntactic repetition of interfaces. The language is given a formal semantics, and we argue that this semantics is implementable in a variety of compilers.\n",
      "Distance: 0.7736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend_papers(\"deep learning for edge computing\", model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3bd8b34-d743-441f-8b39-6b8162e06224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test Embedding: tensor([-0.0076, -0.0083,  0.0192,  0.0463,  0.0204, -0.0167,  0.0394,  0.0891,\n",
      "         0.0583, -0.0393, -0.1276,  0.0174, -0.0865, -0.0312, -0.0016],\n",
      "       device='cuda:0')\n",
      "Sample Train Embedding: tensor([ 0.0408,  0.0692,  0.0076,  0.0218,  0.0314, -0.0222,  0.0052,  0.0636,\n",
      "         0.0642,  0.0314, -0.0972,  0.0009, -0.0870,  0.0390, -0.0090],\n",
      "       device='cuda:0')\n",
      "\n",
      "Test Paper 1:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniforge3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Paper \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, train_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_idx[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m15\u001b[39m]):\n\u001b[1;32m---> 46\u001b[0m     recommended_paper_id \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     47\u001b[0m     recommended_for_test\u001b[38;5;241m.\u001b[39mappend(recommended_paper_id)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Recommended Paper ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommended_paper_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_matrix[i,\u001b[38;5;250m \u001b[39mtrain_idx]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniforge3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\miniforge3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\miniforge3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Id'"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all papers using the trained model\n",
    "def get_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "            batch_embeddings = model(input_ids, attention_mask)  # Keep as CUDA tensor\n",
    "            embeddings.append(batch_embeddings)  # Store without converting to NumPy\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0), np.array(paper_indices)  # Return PyTorch tensor\n",
    "\n",
    "# Get embeddings for train and test sets\n",
    "train_dataloader = DataLoader(PaperDataset(X_train, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(PaperDataset(X_test, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "\n",
    "train_embeddings, train_indices = get_embeddings(train_dataloader, model)\n",
    "test_embeddings, test_indices = get_embeddings(test_dataloader, model)\n",
    "\n",
    "# Compute cosine similarity in CUDA\n",
    "train_embeddings = F.normalize(train_embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "test_embeddings = F.normalize(test_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sample Test Embedding:\", test_embeddings[0][:15])  # First 15 values\n",
    "print(\"Sample Train Embedding:\", train_embeddings[0][:15])  # First 15 values\n",
    "\n",
    "similarity_matrix = torch.matmul(test_embeddings, train_embeddings.T).cpu().numpy()  # Cosine similarity\n",
    "\n",
    "# Select top-N most similar papers\n",
    "top_n = 10\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_n]\n",
    "\n",
    "# Print recommended papers\n",
    "recommended_paper_ids = []\n",
    "\n",
    "for i, test_idx in enumerate(top_indices):\n",
    "    recommended_for_test = []\n",
    "    print(f\"\\nTest Paper {i+1}:\")\n",
    "    \n",
    "    for j, train_idx in enumerate(test_idx[1:15]):\n",
    "        recommended_paper_id = X_train.iloc[train_indices[train_idx]][\"Id\"]\n",
    "        recommended_for_test.append(recommended_paper_id)\n",
    "        \n",
    "        print(f\"  {j+1}. Recommended Paper ID: {recommended_paper_id} (Similarity: {similarity_matrix[i, train_idx]:.4f})\")\n",
    "    \n",
    "    recommended_paper_ids.append(recommended_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af7a75-f560-4fce-84d8-c483c7855b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
