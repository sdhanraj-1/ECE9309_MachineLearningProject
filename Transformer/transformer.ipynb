{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d664a8f3-deb0-47ee-96dd-4a094ba076f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steph\\miniforge3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60393e37-8f4c-4abc-866f-024d0d1d697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (280, 8), Validation: (80, 8), Test: (40, 8)\n"
     ]
    }
   ],
   "source": [
    "# train 70%, val 20%, test 10%\n",
    "df = pd.read_csv(\"database_clean.csv\")\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "df = df.head(400)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "X_val, X_test = train_test_split(X_temp, test_size=1/3, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0718d3bf-0782-450d-8a8b-ef1130b0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.data.iloc[idx][\"title\"])\n",
    "        abstract = str(self.data.iloc[idx][\"abstract\"])\n",
    "        input_text = title + \" \" + abstract  # Combine title and abstract\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"paper_index\": idx  # Used for retrieval\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset_train = PaperDataset(X_train, tokenizer, 256)\n",
    "dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f242b441-9990-4523-92cc-243e5fa7facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input IDs Shape: torch.Size([8, 256])\n",
      "Batch Attention Mask Shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Attention Mask Shape:\", batch[\"attention_mask\"].shape)\n",
    "    break  # Check only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50964c3e-9433-4a3a-8fa7-40b5575ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRecommender(nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v1\", embedding_dim=768, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Linear(self.embedding_dim, embedding_dim)  # Projection layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = nn.functional.normalize  # L2 normalization for retrieval\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(next(self.parameters()).device)  # Ensure inputs are on the same device as the model\n",
    "        attention_mask = attention_mask.to(next(self.parameters()).device)\n",
    "        \n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        embedding = self.fc(self.dropout(pooled_output))\n",
    "        return self.normalize(embedding, p=2, dim=1)  # Normalize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9619723-25a6-49e5-ab88-42fea483e730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1733\n",
      "Epoch 2, Loss: 0.1268\n",
      "Epoch 3, Loss: 0.1007\n",
      "Epoch 4, Loss: 0.0749\n",
      "Epoch 5, Loss: 0.0575\n",
      "Epoch 6, Loss: 0.0380\n",
      "Epoch 7, Loss: 0.0210\n",
      "Epoch 8, Loss: 0.0088\n",
      "Epoch 9, Loss: 0.0070\n",
      "Epoch 10, Loss: 0.0057\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender().to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def contrastive_loss(embeddings):\n",
    "    \"\"\"Loss function using pairwise Euclidean distances in the batch.\"\"\"\n",
    "    # Normalize embeddings (optional, helps numerical stability)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)  \n",
    "    # Compute pairwise Euclidean distances\n",
    "    distance_matrix = torch.cdist(embeddings, embeddings, p=2)  # (batch_size, batch_size)\n",
    "    # Minimize the sum of all distances (encourages compact embedding space)\n",
    "    loss = distance_matrix.sum() / (embeddings.shape[0] ** 2)  # Normalize by batch_size^2\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(input_ids, attention_mask)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute loss using pairwise distances between all embeddings\n",
    "        loss = contrastive_loss(embeddings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b1bddb-dea3-4a5d-9e80-3da76064b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, model, df, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode query\n",
    "    query_tokens = tokenizer(query, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(query_tokens[\"input_ids\"], query_tokens[\"attention_mask\"]).cpu().numpy()\n",
    "\n",
    "    # Compute Euclidean distances between query and all paper embeddings\n",
    "    paper_embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_input_ids, batch_attention_mask).cpu().numpy()\n",
    "            paper_embeddings.append(batch_embeddings)\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())  # Store original indices\n",
    "\n",
    "    paper_embeddings = np.vstack(paper_embeddings)  # Stack all embeddings\n",
    "    paper_indices = np.array(paper_indices)\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = np.linalg.norm(paper_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # Get top-k closest papers (smallest distances)\n",
    "    top_indices = np.argsort(distances)[:top_k]\n",
    "\n",
    "    print(\"\\nRecommended Papers:\")\n",
    "    for idx in top_indices:\n",
    "        paper_idx = paper_indices[idx]\n",
    "        print(f\"Title: {df.iloc[paper_idx]['title']}\\nAbstract: {df.iloc[paper_idx]['abstract']}\\nDistance: {distances[idx]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a25f812-4c97-4260-a369-d4a766f40744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Papers:\n",
      "Title: Analysis of EV Charging Coordination Efficiency in Presence of Cheating Customers\n",
      "Abstract: Charging coordination is employed to efficiently serve electric vehicle (EV) charging requests without overloading the distribution network. Parameters such as parking duration, battery state-of-charge (SoC), and charging amount are provided by EVs to the charging coordination center to schedule their charging requests efficiently. The existing literature assumes that the customers always provide correct information. Unfortunately, customers may provide false information to gain higher charging priority. Assessing the impact of cheating behavior represents a significant and open problem. Herein paper, the impact of providing false information (e.g., parking duration) on the efficiency of the charging coordination mechanism is investigated. The charging coordination strategy is formulated as a linear optimization problem. Two different objectives are used to assess the impact of the objective function on the amount of performance degradation. Our investigations reveal that the degradation of the efficiency of the charging coordination mechanism depends on the percentage of cheating customers and cheating duration versus the typical parking duration. In addition, the impact of cheating behavior increases with the number of deployed chargers. Thus, the severity of the cheating impact will increase in the future as more fast chargers are allocated in charging networks.\n",
      "Distance: 0.1614\n",
      "\n",
      "Title: Magnetic Field and Temperature Dual-Parameter Sensor Based on Nonadiabatic Tapered Microfiber Cascaded With FBG\n",
      "Abstract: A kind of dual-parameter sensor based on magnetic-fluid-coated nonadiabatic tapered microfiber (NTF) cascaded with fiber Bragg grating (FBG) is proposed and experimentally demonstrated. Simultaneous measurement of magnetic field and temperature is realized by monitoring the variation of NTF interference spectrum and FBG characteristic dip. In the magnetic field range of 0–18 mT, the highest magnetic field sensitivity can reach 1.159 nm/mT. The maximum temperature sensitivity is up to −1.737 nm/°C in the temperature range of 25-50 °C. The proposed magnetic-fluid-coated NTF interferometer cascaded with FBG will find extensive application prospect due to its high sensitivity, easy fabrication, compactness, strong robustness, and low cost.\n",
      "Distance: 0.1630\n",
      "\n",
      "Title: Hybrid Stochastic Ranking for Constrained Optimization\n",
      "Abstract: Teaching learning based optimization (TLBO) algorithm is a distinguished nature-inspired population-based meta-heuristic, which is basically designed for unconstrained optimization. TLBO mimics teaching learning process through which learners acquire knowledge from their teachers, and improve their results/grades, accordingly. Stochastic ranking (SR) is a constrained handling technique (CHT), which produces greediness among solutions to improve their fitness values and feasibility. Violation constraint handling (VCH) technique produces more feasibility among the existing superiority of feasibility CHTs due to its additional factor of ranking based on the number of constraints violated (NCV). This work brings in a new variant of SR, namely hybrid stochastic ranking (HSR), which combines SR and VCH. For constrained optimization, the integration of some CHT with TLBO is essential. In this paper, HSR is integrated with TLBO and a new constrained version of TLBO called HSR-TLBO is designed. The efficiency of HSR-TLBO is checked on constrained test functions of the suit CEC 2017. The experimental results show that HSR-TLBO got prominent position when compared and ranked with the top four papers and our two newly designed constrained variants of TLBO, MSR-TLBO and MVCH-TLBO, based on the provided budget and ranking criteria of the mentioned suit.\n",
      "Distance: 0.1633\n",
      "\n",
      "Title: Improving IoT Federation Resiliency With Distributed Ledger Technology\n",
      "Abstract: Despite the rapid spread of Internet of Things (IoT) systems, the lack of interoperability between the systems significantly hinders their business and societal potential. Moreover, a major challenge for wider interoperability is that the IoT systems can be owned by multiple independent entities, whose collaboration will need to be organised to ensure their interoperability. One approach for achieving this is to establish federations supported by Distributed Ledger Technologies (DLTs), as this enables interoperability between entities and collaboration between business platforms, thereby overcoming many technical and administrative difficulties. DLTs can provide the required transparency and immutability for management of the federations, thus increasing trust and reducing the risk of misbehaviour that could destabilise the federation. This paper presents two system dynamics simulation models, which demonstrate that the success of a federation (with or without DLT support) is inversely related to the short-term selfishness of its members, and we then proceed to show that DLTs can improve the feedback received by the federation members on their actions by promoting a common consensus, which in turn can make the federation more resilient.\n",
      "Distance: 0.1635\n",
      "\n",
      "Title: A Novel Array Configuration Technique for Improving the Power Output of the Partial Shaded Photovoltaic System\n",
      "Abstract: Power conversion efficiency is the most important factor to be considered in PV systems because it is affected by various environmental conditions. The effect of partial shading is the most influenced factor in the reduction of power output. Various research schemes like Maximum Power Point Tracking (MPPT), array configuration scheme, reconfiguration, etc., work on the PV system to reduce the impact of partial shading. This paper presents a new kind of array configuration scheme that forms the PV array based on the moves of the Knight coin in the chess game. This arrangement creates the squared PV array of rows with distinct PV modules which is capable of evenly dispersing the shading in the partially shaded PV array. Also, this scheme is applicable for the non-squared PV arrays to create PV rows with the PV modules from a distinct location or from the same row with optimized distance to disperse the maximum level of shading. The proposed method has been discussed with the proper mathematical formulation with all necessary constraints and also it been validated with the hardware arrangements and MATLAB/Simulink\n",
      "®\n",
      " model.\n",
      "Distance: 0.1635\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend_papers(\"deep learning for edge computing\", model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3bd8b34-d743-441f-8b39-6b8162e06224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test Embedding: tensor([ 0.0266, -0.0002, -0.0016,  0.0305, -0.0097, -0.0465, -0.0027, -0.0533,\n",
      "         0.0289,  0.0493, -0.0020, -0.0550, -0.0089,  0.0416,  0.0340],\n",
      "       device='cuda:0')\n",
      "Sample Train Embedding: tensor([ 0.0265, -0.0003, -0.0018,  0.0301, -0.0097, -0.0466, -0.0028, -0.0533,\n",
      "         0.0289,  0.0491, -0.0016, -0.0551, -0.0090,  0.0417,  0.0339],\n",
      "       device='cuda:0')\n",
      "\n",
      "Test Paper 1:\n",
      "  1. Recommended Paper ID: 180 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 151 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 168 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 279 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 289 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 121 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 112 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 327 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 57 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 2:\n",
      "  1. Recommended Paper ID: 394 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 221 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 106 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 208 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 197 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 322 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 293 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 253 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 3:\n",
      "  1. Recommended Paper ID: 276 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 428 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 13 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 213 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 343 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 221 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 4:\n",
      "  1. Recommended Paper ID: 329 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 14 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 164 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 371 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 293 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 362 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 360 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 38 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 5:\n",
      "  1. Recommended Paper ID: 137 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 92 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 341 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 367 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 252 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 203 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 208 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 133 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 6:\n",
      "  1. Recommended Paper ID: 222 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 369 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 303 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 226 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 402 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 111 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 210 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 342 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 392 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 54 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 7:\n",
      "  1. Recommended Paper ID: 332 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 334 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 97 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 278 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 51 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 71 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 178 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 333 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 8:\n",
      "  1. Recommended Paper ID: 51 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 332 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 19 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 6 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 261 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 144 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 406 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 322 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 9:\n",
      "  1. Recommended Paper ID: 379 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 279 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 320 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 282 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 322 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 151 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 106 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 286 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 10:\n",
      "  1. Recommended Paper ID: 137 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 345 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 117 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 130 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 367 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 218 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 92 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 341 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 11:\n",
      "  1. Recommended Paper ID: 125 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 367 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 241 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 300 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 399 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 165 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 345 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 225 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 55 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 12:\n",
      "  1. Recommended Paper ID: 411 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 35 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 94 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 212 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 112 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 83 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 154 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 54 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 235 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 13:\n",
      "  1. Recommended Paper ID: 30 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 97 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 373 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 226 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 343 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 138 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 14:\n",
      "  1. Recommended Paper ID: 56 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 357 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 341 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 268 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 377 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 177 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 163 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 15:\n",
      "  1. Recommended Paper ID: 359 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 142 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 94 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 98 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 377 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 57 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 53 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 85 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 64 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 105 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 16:\n",
      "  1. Recommended Paper ID: 225 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 406 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 394 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 6 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 44 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 293 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 17:\n",
      "  1. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 278 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 23 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 229 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 19 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 85 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 22 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 18:\n",
      "  1. Recommended Paper ID: 341 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 236 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 152 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 367 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 383 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 56 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 218 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 71 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 19:\n",
      "  1. Recommended Paper ID: 253 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 268 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 129 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 51 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 360 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 52 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 20:\n",
      "  1. Recommended Paper ID: 362 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 329 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 393 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 316 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 44 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 1 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 226 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 314 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 358 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 21:\n",
      "  1. Recommended Paper ID: 39 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 166 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 12 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 57 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 254 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 35 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 411 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 351 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 22 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 22:\n",
      "  1. Recommended Paper ID: 53 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 297 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 85 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 64 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 106 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 197 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 23:\n",
      "  1. Recommended Paper ID: 210 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 369 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 392 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 111 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 303 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 402 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 342 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 222 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 372 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 181 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 24:\n",
      "  1. Recommended Paper ID: 329 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 322 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 371 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 370 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 219 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 293 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 399 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 345 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 25:\n",
      "  1. Recommended Paper ID: 253 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 46 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 363 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 16 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 332 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 404 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 314 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 26:\n",
      "  1. Recommended Paper ID: 209 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 111 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 342 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 123 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 113 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 118 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 227 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 410 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 323 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 303 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 27:\n",
      "  1. Recommended Paper ID: 419 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 343 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 289 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 168 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 345 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 207 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 25 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 28:\n",
      "  1. Recommended Paper ID: 123 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 113 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 11 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 342 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 303 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 323 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 359 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 156 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 327 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 151 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 29:\n",
      "  1. Recommended Paper ID: 126 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 189 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 203 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 229 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 23 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 214 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 32 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 382 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 30:\n",
      "  1. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 345 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 289 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 377 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 313 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 218 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 64 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 53 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 138 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 31:\n",
      "  1. Recommended Paper ID: 19 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 67 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 411 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 231 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 112 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 373 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 278 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 151 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 32:\n",
      "  1. Recommended Paper ID: 94 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 295 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 1 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 190 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 341 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 152 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 298 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 383 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 236 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 33:\n",
      "  1. Recommended Paper ID: 151 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 350 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 289 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 359 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 57 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 218 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 40 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 112 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 43 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 34:\n",
      "  1. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 373 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 214 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 30 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 199 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 229 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 218 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 64 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 35:\n",
      "  1. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 376 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 322 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 221 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 201 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 313 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 394 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 194 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 36:\n",
      "  1. Recommended Paper ID: 55 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 208 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 117 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 394 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 376 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 365 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 319 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 138 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 37:\n",
      "  1. Recommended Paper ID: 103 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 14 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 253 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 360 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 164 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 329 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 98 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 362 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 371 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 283 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 38:\n",
      "  1. Recommended Paper ID: 389 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 179 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 117 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 354 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 194 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 258 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 226 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 406 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 251 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 39:\n",
      "  1. Recommended Paper ID: 300 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 127 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 213 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 47 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 314 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 236 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 109 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 343 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 241 (Similarity: 1.0000)\n",
      "\n",
      "Test Paper 40:\n",
      "  1. Recommended Paper ID: 300 (Similarity: 1.0000)\n",
      "  2. Recommended Paper ID: 97 (Similarity: 1.0000)\n",
      "  3. Recommended Paper ID: 293 (Similarity: 1.0000)\n",
      "  4. Recommended Paper ID: 262 (Similarity: 1.0000)\n",
      "  5. Recommended Paper ID: 83 (Similarity: 1.0000)\n",
      "  6. Recommended Paper ID: 385 (Similarity: 1.0000)\n",
      "  7. Recommended Paper ID: 185 (Similarity: 1.0000)\n",
      "  8. Recommended Paper ID: 284 (Similarity: 1.0000)\n",
      "  9. Recommended Paper ID: 164 (Similarity: 1.0000)\n",
      "  10. Recommended Paper ID: 231 (Similarity: 1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all papers using the trained model\n",
    "def get_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "            batch_embeddings = model(input_ids, attention_mask)  # Keep as CUDA tensor\n",
    "            embeddings.append(batch_embeddings)  # Store without converting to NumPy\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0), np.array(paper_indices)  # Return PyTorch tensor\n",
    "\n",
    "# Get embeddings for train and test sets\n",
    "train_dataloader = DataLoader(PaperDataset(X_train, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(PaperDataset(X_test, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "\n",
    "train_embeddings, train_indices = get_embeddings(train_dataloader, model)\n",
    "test_embeddings, test_indices = get_embeddings(test_dataloader, model)\n",
    "\n",
    "# Compute cosine similarity in CUDA\n",
    "train_embeddings = F.normalize(train_embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "test_embeddings = F.normalize(test_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sample Test Embedding:\", test_embeddings[0][:15])  # First 15 values\n",
    "print(\"Sample Train Embedding:\", train_embeddings[0][:15])  # First 15 values\n",
    "\n",
    "similarity_matrix = torch.matmul(test_embeddings, train_embeddings.T).cpu().numpy()  # Cosine similarity\n",
    "\n",
    "# Select top-N most similar papers\n",
    "top_n = 10\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_n]\n",
    "\n",
    "# Print recommended papers\n",
    "recommended_paper_ids = []\n",
    "\n",
    "for i, test_idx in enumerate(top_indices):\n",
    "    recommended_for_test = []\n",
    "    print(f\"\\nTest Paper {i+1}:\")\n",
    "    \n",
    "    for j, train_idx in enumerate(test_idx):\n",
    "        recommended_paper_id = X_train.iloc[train_indices[train_idx]][\"Id\"]\n",
    "        recommended_for_test.append(recommended_paper_id)\n",
    "        \n",
    "        print(f\"  {j+1}. Recommended Paper ID: {recommended_paper_id} (Similarity: {similarity_matrix[i, train_idx]:.4f})\")\n",
    "    \n",
    "    recommended_paper_ids.append(recommended_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af7a75-f560-4fce-84d8-c483c7855b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
