{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d664a8f3-deb0-47ee-96dd-4a094ba076f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steph\\miniforge3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60393e37-8f4c-4abc-866f-024d0d1d697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (385, 8), Validation: (110, 8), Test: (56, 8)\n"
     ]
    }
   ],
   "source": [
    "# train 70%, val 20%, test 10%\n",
    "df = pd.read_csv(\"database_clean.csv\")\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "#df = df.head(400)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train, X_temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "X_val, X_test = train_test_split(X_temp, test_size=1/3, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0718d3bf-0782-450d-8a8b-ef1130b0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.data.iloc[idx][\"title\"])\n",
    "        abstract = str(self.data.iloc[idx][\"abstract\"])\n",
    "        input_text = title + \" \" + abstract  # Combine title and abstract\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"paper_index\": idx  # Used for retrieval\n",
    "        }\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset_train = PaperDataset(X_train, tokenizer, 256)\n",
    "dataset_val = PaperDataset(X_val, tokenizer, 256)\n",
    "dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f242b441-9990-4523-92cc-243e5fa7facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input IDs Shape: torch.Size([8, 256])\n",
      "Batch Attention Mask Shape: torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Batch Input IDs Shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Batch Attention Mask Shape:\", batch[\"attention_mask\"].shape)\n",
    "    break  # Check only the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50964c3e-9433-4a3a-8fa7-40b5575ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRecommender(nn.Module):\n",
    "    def __init__(self, model_name, embedding_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim  # Ensure this is correctly set\n",
    "\n",
    "        # Ensure projection layer matches `embedding_dim`\n",
    "        self.fc = nn.Linear(self.encoder.config.hidden_size, embedding_dim)  \n",
    "        \n",
    "        # Multiheaded attention layer using the correct `embedding_dim`\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = nn.functional.normalize\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids = input_ids.to(next(self.parameters()).device)\n",
    "        attention_mask = attention_mask.to(next(self.parameters()).device)\n",
    "\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "\n",
    "        embedding = self.fc(self.dropout(pooled_output))  # Apply projection\n",
    "        embedding = embedding.unsqueeze(0)  # Reshape for MultiheadAttention\n",
    "\n",
    "        # Multiheaded Attention\n",
    "        attn_output, _ = self.attention(embedding, embedding, embedding)\n",
    "        attn_output = attn_output.squeeze(0)  # Remove batch dim\n",
    "\n",
    "        return self.normalize(attn_output, p=2, dim=1)  # Normalize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3558e342-5329-4e21-b2fd-36a7a60540dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0750, 0.0598,  ..., 0.0521, 0.0948, 0.0936],\n",
      "        [0.0750, 1.0000, 0.0727,  ..., 0.0725, 0.0826, 0.0915],\n",
      "        [0.0598, 0.0727, 1.0000,  ..., 0.0706, 0.0856, 0.0740],\n",
      "        ...,\n",
      "        [0.0521, 0.0725, 0.0706,  ..., 1.0000, 0.0538, 0.0668],\n",
      "        [0.0948, 0.0826, 0.0856,  ..., 0.0538, 1.0000, 0.1076],\n",
      "        [0.0936, 0.0915, 0.0740,  ..., 0.0668, 0.1076, 1.0000]],\n",
      "       device='cuda:0')\n",
      "tensor([[ True, False, False,  ..., False,  True,  True],\n",
      "        [False,  True, False,  ..., False,  True,  True],\n",
      "        [False, False,  True,  ..., False,  True, False],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True, False, False],\n",
      "        [ True,  True,  True,  ..., False,  True,  True],\n",
      "        [ True,  True, False,  ..., False,  True,  True]], device='cuda:0')\n",
      "tensor([[False,  True,  True,  ...,  True, False, False],\n",
      "        [ True, False,  True,  ...,  True, False, False],\n",
      "        [ True,  True, False,  ...,  True, False,  True],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False,  True,  True],\n",
      "        [False, False, False,  ...,  True, False, False],\n",
      "        [False, False,  True,  ...,  True, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf_similarity(df):\n",
    "    corpus = df['title'] + \" \" + df['abstract']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "similarity_matrix_np = compute_tfidf_similarity(df)\n",
    "# Convert to tensor for GPU computation\n",
    "similarity_matrix = torch.tensor(similarity_matrix_np, dtype=torch.float32).to(\"cuda\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "threshold = 0.0785\n",
    "\n",
    "positive_pairs = similarity_matrix > threshold\n",
    "negative_pairs = ~positive_pairs\n",
    "print(positive_pairs)\n",
    "print(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b2aaef-c81f-4998-9ac3-44d3d2904eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85th percentile: 0.1105\n",
      "90th percentile: 0.1205\n",
      "95th percentile: 0.1374\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#Organize Data\n",
    "similarity_values_np = similarity_matrix_np.flatten()\n",
    "similarity_values_np_filtered = similarity_values_np[similarity_values_np <= 0.25]\n",
    "X = pd.Series(similarity_values_np_filtered, name=\"(Similarity Matrix Distribution)\")\n",
    "\n",
    "#Plot Data\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(X, bins=25, color=\"g\", ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Compute percentiles\n",
    "percentiles = [85, 90, 95]\n",
    "percentile_values = np.percentile(similarity_values_np_filtered, percentiles)\n",
    "\n",
    "# Print results\n",
    "for p, val in zip(percentiles, percentile_values):\n",
    "    print(f\"{p}th percentile: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb2e70a-fe2c-4669-ad62-8c38b01505f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, loss, path=\"./paper_recommender.pth\"):\n",
    "    \"\"\"Save the model, optimizer state, and training metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Model saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa12bbd-aa94-4282-b3e9-4bafbbf51259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, path = \"./paper_recommender.pth\"):\n",
    "    \"\"\"Load the model and optionally the optimizer.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Model loaded from {path}, trained until epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Optimizer state restored.\")\n",
    "\n",
    "    return checkpoint[\"epoch\"], checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc03d8a2-8a92-4a9b-8b63-b2549cc36377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5):\n",
    "    \"\"\"\n",
    "    Contrastive loss using TF-IDF similarity as ground truth.\n",
    "    \n",
    "    embeddings: (batch_size, embedding_dim)\n",
    "    similarity_matrix: Precomputed TF-IDF cosine similarity.\n",
    "    indices: Indices of batch samples in dataset.\n",
    "    margin: Margin for contrastive loss.\n",
    "    \"\"\"\n",
    "    batch_size = embeddings.shape[0]\n",
    "\n",
    "    # Ensure embeddings are L2 normalized\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = torch.mm(embeddings, embeddings.T)  # (batch_size, batch_size)\n",
    "    cosine_distances = 1 - cosine_sim  # Convert similarity to distance\n",
    "\n",
    "    # Extract ground truth similarity values for batch samples\n",
    "    ground_truth_similarities = similarity_matrix[indices][:, indices]\n",
    "\n",
    "    # Define positive and negative pairs\n",
    "    threshold = 0.1205  # Adjust this value if needed\n",
    "    positive_pairs = (ground_truth_similarities > threshold).float()\n",
    "    negative_pairs = (ground_truth_similarities <= threshold).float()\n",
    "\n",
    "    # Compute losses\n",
    "    positive_loss = (cosine_distances * positive_pairs).sum() / (positive_pairs.sum() + 1e-8)\n",
    "    negative_loss = torch.clamp(margin - cosine_distances, min=0) * negative_pairs\n",
    "    negative_loss = negative_loss.sum() / (negative_pairs.sum() + 1e-8)\n",
    "\n",
    "    loss = positive_loss + negative_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea65a89c-88ae-45e3-b7e1-957058b21736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [256, 512, 768])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2,4,8,16])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
    "\n",
    "    # Initialize model with selected hyperparameters\n",
    "    model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                             embedding_dim, num_heads, dropout).to(\"cuda\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=8, shuffle=False)\n",
    "\n",
    "    num_epochs = 3  # Use fewer epochs for tuning\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(dataloader_train)\n",
    "\n",
    "        # Validation step (compute loss on validation set)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_val:\n",
    "                input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "                attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "                indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "                embeddings = model(input_ids, attention_mask)\n",
    "                val_loss += contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(dataloader_val)\n",
    "        total_loss += avg_val_loss\n",
    "\n",
    "    return total_loss / num_epochs  # Minimize validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1396d4-1570-443a-93d7-8ffc52a66caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-08 19:02:54,670] A new study created in memory with name: no-name-d0ce2c80-dd6e-4567-b978-636059963e60\n",
      "[I 2025-03-08 19:04:15,480] Trial 0 finished with value: 0.32999143536601744 and parameters: {'embedding_dim': 768, 'num_heads': 8, 'dropout': 0.24780074119382328}. Best is trial 0 with value: 0.32999143536601744.\n",
      "[I 2025-03-08 19:05:22,588] Trial 1 finished with value: 0.2624938152730465 and parameters: {'embedding_dim': 768, 'num_heads': 16, 'dropout': 0.13151707441494287}. Best is trial 1 with value: 0.2624938152730465.\n",
      "[I 2025-03-08 19:06:29,679] Trial 2 finished with value: 0.2466896946231524 and parameters: {'embedding_dim': 768, 'num_heads': 4, 'dropout': 0.1929644720048142}. Best is trial 2 with value: 0.2466896946231524.\n",
      "[I 2025-03-08 19:07:36,489] Trial 3 finished with value: 0.30427368410996025 and parameters: {'embedding_dim': 512, 'num_heads': 2, 'dropout': 0.24878291626861107}. Best is trial 2 with value: 0.2466896946231524.\n",
      "[I 2025-03-08 19:08:43,540] Trial 4 finished with value: 0.26240025318804244 and parameters: {'embedding_dim': 768, 'num_heads': 16, 'dropout': 0.12542801849453272}. Best is trial 2 with value: 0.2466896946231524.\n",
      "[I 2025-03-08 19:09:50,320] Trial 5 finished with value: 0.24665640214724202 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.19066786397888635}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:10:57,198] Trial 6 finished with value: 0.32064824267512276 and parameters: {'embedding_dim': 512, 'num_heads': 8, 'dropout': 0.22988552632620976}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:12:04,175] Trial 7 finished with value: 0.30465857897486 and parameters: {'embedding_dim': 768, 'num_heads': 16, 'dropout': 0.2289503460173073}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:13:10,988] Trial 8 finished with value: 0.2858283874534425 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.24509438580957896}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:14:17,713] Trial 9 finished with value: 0.28265099369344254 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.16459006276632498}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:15:24,477] Trial 10 finished with value: 0.2919563384992736 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.2897442869413417}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:16:31,212] Trial 11 finished with value: 0.25039964036217754 and parameters: {'embedding_dim': 256, 'num_heads': 4, 'dropout': 0.18043993716673762}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:17:38,173] Trial 12 finished with value: 0.27018637795533457 and parameters: {'embedding_dim': 768, 'num_heads': 4, 'dropout': 0.19480657641302784}. Best is trial 5 with value: 0.24665640214724202.\n",
      "[I 2025-03-08 19:18:44,895] Trial 13 finished with value: 0.22593715472058173 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.15425711259858013}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:19:51,649] Trial 14 finished with value: 0.2456638419202396 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.1547258070100168}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:20:58,392] Trial 15 finished with value: 0.23026388865851222 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.15284258076482812}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:22:05,213] Trial 16 finished with value: 0.2537813928155672 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.107129847191492}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:23:12,098] Trial 17 finished with value: 0.26021278241560575 and parameters: {'embedding_dim': 512, 'num_heads': 2, 'dropout': 0.1468446534295717}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:24:18,874] Trial 18 finished with value: 0.22983508654648352 and parameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.10555704427829048}. Best is trial 13 with value: 0.22593715472058173.\n",
      "[I 2025-03-08 19:25:25,658] Trial 19 finished with value: 0.24173373516116822 and parameters: {'embedding_dim': 256, 'num_heads': 8, 'dropout': 0.10771521332788284}. Best is trial 13 with value: 0.22593715472058173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'embedding_dim': 256, 'num_heads': 2, 'dropout': 0.15425711259858013}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")  # We want to minimize loss\n",
    "study.optimize(objective, n_trials=20)  # Run 20 trials\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9619723-25a6-49e5-ab88-42fea483e730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Zero loss at epoch 1. Debug required.\n",
      "Epoch 1, Avg Loss: 0.2216\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 2. Debug required.\n",
      "Epoch 2, Avg Loss: 0.2415\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 3. Debug required.\n",
      "Epoch 3, Avg Loss: 0.2101\n",
      "Model saved at paper_recommender.pth\n",
      "Epoch 4, Avg Loss: 0.2120\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 5. Debug required.\n",
      "Epoch 5, Avg Loss: 0.1922\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 6. Debug required.\n",
      "Epoch 6, Avg Loss: 0.2115\n",
      "Model saved at paper_recommender.pth\n",
      "Epoch 7, Avg Loss: 0.2081\n",
      "Model saved at paper_recommender.pth\n",
      "Epoch 8, Avg Loss: 0.1847\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 9. Debug required.\n",
      "Epoch 9, Avg Loss: 0.2010\n",
      "Model saved at paper_recommender.pth\n",
      "Warning: Zero loss at epoch 10. Debug required.\n",
      "Epoch 10, Avg Loss: 0.1881\n",
      "Model saved at paper_recommender.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               study.best_params[\"embedding_dim\"],\n",
    "                               study.best_params[\"num_heads\"],\n",
    "                               study.best_params[\"dropout\"]).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        indices = batch[\"paper_index\"].cpu().numpy()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = contrastive_loss(embeddings, similarity_matrix, indices, margin=0.5)\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            print(f\"Warning: Zero loss at epoch {epoch+1}. Debug required.\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "    save_model(model, optimizer, epoch + 1, avg_loss, \"paper_recommender.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34304d1d-4b28-47f7-b4c2-f16cf607b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from paper_recommender.pth, trained until epoch 10\n",
      "Optimizer state restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaperRecommender(\n",
       "  (encoder): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.15, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PaperRecommender(\"sentence-transformers/distiluse-base-multilingual-cased-v1\",\n",
    "                               256,\n",
    "                               2,\n",
    "                               0.15).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Load the model (after training)\n",
    "epoch, loss = load_model(model, optimizer, \"paper_recommender.pth\")\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b1bddb-dea3-4a5d-9e80-3da76064b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, model, df, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and encode query\n",
    "    query_tokens = tokenizer(query, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(query_tokens[\"input_ids\"], query_tokens[\"attention_mask\"]).cpu().numpy()\n",
    "\n",
    "    # Compute Euclidean distances between query and all paper embeddings\n",
    "    paper_embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        batch_attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch_input_ids, batch_attention_mask).cpu().numpy()\n",
    "            paper_embeddings.append(batch_embeddings)\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())  # Store original indices\n",
    "\n",
    "    paper_embeddings = np.vstack(paper_embeddings)  # Stack all embeddings\n",
    "    paper_indices = np.array(paper_indices)\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = np.linalg.norm(paper_embeddings - query_embedding, axis=1)\n",
    "\n",
    "    # Get top-k closest papers (smallest distances)\n",
    "    top_indices = np.argsort(distances)[:top_k]\n",
    "\n",
    "    print(\"\\nRecommended Papers:\")\n",
    "    for idx in top_indices:\n",
    "        paper_idx = paper_indices[idx]\n",
    "        print(f\"Title: {df.iloc[paper_idx]['title']}\\nAbstract: {df.iloc[paper_idx]['abstract']}\\nDistance: {distances[idx]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a25f812-4c97-4260-a369-d4a766f40744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Papers:\n",
      "Title: Multistage Centrifugal Pump Fault Diagnosis by Selecting Fault Characteristic Modes of Vibration and Using Pearson Linear Discriminant Analysis\n",
      "Abstract: This paper proposes a three-stage fault diagnosis strategy for multistage centrifugal pumps. First, the proposed method identifies and selects fault characteristic modes of vibration to overcome the substantial noise produced by other unrelated macro-structural vibrations. In the second stage, raw hybrid statistical features are extracted from the fault characteristic modes of vibration in time, frequency, and the time-frequency domain. These extracted features result in a high-dimensional feature space. However, in general, not all of the features are best to characterize the ongoing processes in a centrifugal pump, and some of the extracted features might be irrelevant or even redundant, which can affect the fault classification capabilities of the classification algorithm. In the third stage, a novel dimensionality reduction technique, called Pearson Linear Discriminant Analysis (PLDA), is introduced. PLDA assesses the helpfulness of the feature parameters. This technique selects highly interclass-correlated features and adds them to a helpful feature pool. To achieve maximum intraclass separation while maintaining the original class information, linear discriminant analysis is then applied to the helpful feature pool. This combination of helpful feature pool formation and linear discriminant analysis forms the proposed application of PLDA. The reduced discriminant feature set obtained from PLDA is then classified using the k-nearest neighbor classification algorithm. The proposed method outperforms the previously presented methods in terms of classification accuracy.\n",
      "Distance: 0.8812\n",
      "\n",
      "Title: Towards Sustainability of Manufacturing Processes by Multiobjective Optimization: A Case Study on a Submerged Arc Welding Process\n",
      "Abstract: Optimization on the basis of sustainability brings important benefits to manufacturing process as sustainable productions constitute a crucial aspect in modern manufacturing. This paper presents a new formalized framework for optimizing the sustainability of manufacturing processes. Unlike previous approaches, the proposed technique combines a methodology for selecting the sustainability indicators and a multi-objective optimization for improving the three sustainability pillars (economy, environment and society). While selecting the significant sustainability indicators in the considered manufacturing process relies on the ABC judgment method, the Saaty's method enables weighting the chosen indicators in order to combine them into suitable economic, environmental and social sustainability indexes. Other technological aspects, usually taken as objectives in previous works, are considered constraints in the proposed approach. The optimization is performed by using nature inspired heuristics, which return the set of non-dominated solutions (also known as Pareto front), from which the most convenient alternative is chosen by the decision maker, depending on the specific conditions of the process. For illustrating the usage of the proposed framework, it is applied to the optimization of a submerged arc welding process. Compared with currently used welding parameters, the computed optimal solution outperforms the economic and environmental sustainability while keeps equal the social impact. The results show not only the effectiveness of the proposed approach, but also its flexibility by giving a set of possible solutions which can be chosen depending on how are ranked the sustainability pillars.\n",
      "Distance: 0.9002\n",
      "\n",
      "Title: DFT Spread-Optical Pulse Amplitude Modulation for Visible Light Communication Systems\n",
      "Abstract: DC-biased optical orthogonal frequency division multiplexing (DCO-OFDM) has been proposed in visible light communication (VLC) to overcome the limited modulation bandwidth of light emitting diode (LED). Due to the implementation of the inverse fast Fourier transform at the DCO-OFDM transmitter, DCO-OFDM suffers from its high peak-to-average power ratio (PAPR), which restricts its use in some VLC applications, especially where the optical power efficiency is a crucial requirement. That is because the LEDs used in VLC systems have a limited optical power-current linear range. To this end, a novel discrete Fourier transform spread-optical pulse amplitude modulation (DFTS-OPAM) signal scheme based on the single carrier-interleaved frequency division multiple access (SC-IFDMA) signal is introduced in this paper to address the high PAPR issue of OFDM. DFTS-OPAM is achieved by considering a PAM as an SC-IFDMA data symbol and duplicate the output vector of the fast Fourier transform at the SC-IFDMA transmitter side. Simulation results show that the PAPR of the proposed scheme is 7 dB lower than that of DCO-OFDM. Furthermore, this significant PAPR improvement is experimentally investigated where the practical results show that the proposed scheme can provide more 2.5 dB reduction in the average transmitted power requirement compared to DCO-OFDM and can subsequently increase the maximum achieved distance between the transmitter and the receiver up to 44%.\n",
      "Distance: 0.9072\n",
      "\n",
      "Title: Tremor Suppression With Mechanical Vibration Stimulation\n",
      "Abstract: Tremor, which is one of the most common movement disorders, is a repetitive movement that is caused by periodic muscle contraction and relaxation. To suppress tremors of upper-limb tremor patients, such as essential tremor (ET) patients, many kinds of devices have been developed. On the other hand, when mechanical vibration stimulation is applied to a human muscle, sustained muscle contraction, which is referred to as the tonic vibration reflex (TVR), is induced in the stimulated muscle. In this study, a novel tremor suppression method that utilizes the periodic TVR to induce muscle contraction/relaxation to generate the counterphase motion of the ET is proposed and applied to the forearm pronation-supination ET. In the proposed method, periodic vibration stimulation is applied to generate the periodic TVR in the pronator teres muscle and/or supinator muscle. First, the results confirmed that the TVR can be induced by applying mechanical vibration stimulation to the pronator teres muscle and supinator muscle since the forearm pronation-supination tremor is one of the key features of the ET. Furthermore, the findings also confirmed that the TVR intensity that is induced in these muscles can be adjusted by changing the vibration stimulation frequency. Second, the results show that the counterphase motion of the ET (i.e., periodic pronation-supination motion) can be generated by applying the proposed method. The effectiveness of the proposed method for tremor suppression is evaluated by comparing the generated motion with the ET motion.\n",
      "Distance: 0.9236\n",
      "\n",
      "Title: Tactical Decision-Making for Autonomous Driving Using Dueling Double Deep Q Network With Double Attention\n",
      "Abstract: Decision-making is still a significant challenge to realize fully autonomous driving. Using deep reinforcement learning (DRL) to solve autonomous driving decision-making problems is a recent trend. A common method is to encode surrounding vehicles in a grid to describe the state space to help DRL network extract the scene features. However, in the process of human driving, surrounding vehicles at different positions have different contributions to decision-making. Meanwhile, the network is difficult to fully extract useful features in a sparse state, which can also lead to catastrophic actions. Therefore, this work proposes a spatial attention module to calculate different weights to represent different contributions to decision-making results. And a channel attention module is proposed to fully extract useful features in sparse state features. These two attention modules are integrated into dueling double deep Q network, named D3QN-DA, as a high-level decision-maker of a hierarchical hierarchical control structure based decision-making system. To improve agent performance, an emergency safe checker is introduced in this system. And the agent is trained and test with a designed reward function from safety and efficiency in simulation. The experimental results show that the proposed method increases the safety rate by 54%, and the average explore distance by 30%, which is safer and more intelligent for the decision-making process of automatic driving.\n",
      "Distance: 0.9377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommend_papers(\"deep learning for edge computing\", model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bd8b34-d743-441f-8b39-6b8162e06224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test Embedding: tensor([ 0.0127, -0.0346,  0.0737, -0.0635,  0.0320, -0.0488, -0.0521,  0.0325,\n",
      "        -0.0177, -0.0099,  0.0015, -0.1020, -0.0577,  0.0155,  0.0156],\n",
      "       device='cuda:0')\n",
      "Sample Train Embedding: tensor([-0.1022,  0.0460,  0.0273, -0.0250, -0.0449, -0.0168, -0.0922,  0.0848,\n",
      "         0.0292, -0.0054, -0.0156, -0.1262,  0.0717,  0.0528, -0.0175],\n",
      "       device='cuda:0')\n",
      "\n",
      "Test Paper 1:\n",
      "  1. Recommended Paper ID: 311 (Similarity: 0.7318)\n",
      "  2. Recommended Paper ID: 126 (Similarity: 0.7293)\n",
      "  3. Recommended Paper ID: 44 (Similarity: 0.7196)\n",
      "  4. Recommended Paper ID: 331 (Similarity: 0.7157)\n",
      "  5. Recommended Paper ID: 529 (Similarity: 0.7122)\n",
      "  6. Recommended Paper ID: 419 (Similarity: 0.7094)\n",
      "  7. Recommended Paper ID: 289 (Similarity: 0.7078)\n",
      "  8. Recommended Paper ID: 285 (Similarity: 0.6976)\n",
      "  9. Recommended Paper ID: 180 (Similarity: 0.6941)\n",
      "  10. Recommended Paper ID: 406 (Similarity: 0.6857)\n",
      "\n",
      "Test Paper 2:\n",
      "  1. Recommended Paper ID: 152 (Similarity: 0.7624)\n",
      "  2. Recommended Paper ID: 399 (Similarity: 0.7573)\n",
      "  3. Recommended Paper ID: 170 (Similarity: 0.7373)\n",
      "  4. Recommended Paper ID: 523 (Similarity: 0.7358)\n",
      "  5. Recommended Paper ID: 218 (Similarity: 0.7178)\n",
      "  6. Recommended Paper ID: 47 (Similarity: 0.7168)\n",
      "  7. Recommended Paper ID: 5 (Similarity: 0.7089)\n",
      "  8. Recommended Paper ID: 402 (Similarity: 0.7041)\n",
      "  9. Recommended Paper ID: 580 (Similarity: 0.6973)\n",
      "  10. Recommended Paper ID: 557 (Similarity: 0.6960)\n",
      "\n",
      "Test Paper 3:\n",
      "  1. Recommended Paper ID: 61 (Similarity: 0.7775)\n",
      "  2. Recommended Paper ID: 393 (Similarity: 0.7660)\n",
      "  3. Recommended Paper ID: 581 (Similarity: 0.7599)\n",
      "  4. Recommended Paper ID: 137 (Similarity: 0.7597)\n",
      "  5. Recommended Paper ID: 101 (Similarity: 0.7586)\n",
      "  6. Recommended Paper ID: 491 (Similarity: 0.7555)\n",
      "  7. Recommended Paper ID: 119 (Similarity: 0.7547)\n",
      "  8. Recommended Paper ID: 98 (Similarity: 0.7519)\n",
      "  9. Recommended Paper ID: 159 (Similarity: 0.7486)\n",
      "  10. Recommended Paper ID: 227 (Similarity: 0.7432)\n",
      "\n",
      "Test Paper 4:\n",
      "  1. Recommended Paper ID: 491 (Similarity: 0.8105)\n",
      "  2. Recommended Paper ID: 468 (Similarity: 0.8039)\n",
      "  3. Recommended Paper ID: 137 (Similarity: 0.8035)\n",
      "  4. Recommended Paper ID: 435 (Similarity: 0.8001)\n",
      "  5. Recommended Paper ID: 174 (Similarity: 0.7977)\n",
      "  6. Recommended Paper ID: 61 (Similarity: 0.7932)\n",
      "  7. Recommended Paper ID: 507 (Similarity: 0.7887)\n",
      "  8. Recommended Paper ID: 14 (Similarity: 0.7839)\n",
      "  9. Recommended Paper ID: 23 (Similarity: 0.7822)\n",
      "  10. Recommended Paper ID: 34 (Similarity: 0.7771)\n",
      "\n",
      "Test Paper 5:\n",
      "  1. Recommended Paper ID: 495 (Similarity: 0.9029)\n",
      "  2. Recommended Paper ID: 23 (Similarity: 0.8857)\n",
      "  3. Recommended Paper ID: 481 (Similarity: 0.8834)\n",
      "  4. Recommended Paper ID: 529 (Similarity: 0.8771)\n",
      "  5. Recommended Paper ID: 493 (Similarity: 0.8697)\n",
      "  6. Recommended Paper ID: 562 (Similarity: 0.8680)\n",
      "  7. Recommended Paper ID: 507 (Similarity: 0.8649)\n",
      "  8. Recommended Paper ID: 478 (Similarity: 0.8609)\n",
      "  9. Recommended Paper ID: 452 (Similarity: 0.8599)\n",
      "  10. Recommended Paper ID: 174 (Similarity: 0.8594)\n",
      "\n",
      "Test Paper 6:\n",
      "  1. Recommended Paper ID: 23 (Similarity: 0.7660)\n",
      "  2. Recommended Paper ID: 163 (Similarity: 0.7588)\n",
      "  3. Recommended Paper ID: 493 (Similarity: 0.7433)\n",
      "  4. Recommended Paper ID: 263 (Similarity: 0.7376)\n",
      "  5. Recommended Paper ID: 312 (Similarity: 0.7363)\n",
      "  6. Recommended Paper ID: 159 (Similarity: 0.7350)\n",
      "  7. Recommended Paper ID: 116 (Similarity: 0.7303)\n",
      "  8. Recommended Paper ID: 412 (Similarity: 0.7273)\n",
      "  9. Recommended Paper ID: 448 (Similarity: 0.7255)\n",
      "  10. Recommended Paper ID: 478 (Similarity: 0.7202)\n",
      "\n",
      "Test Paper 7:\n",
      "  1. Recommended Paper ID: 378 (Similarity: 0.7949)\n",
      "  2. Recommended Paper ID: 550 (Similarity: 0.7724)\n",
      "  3. Recommended Paper ID: 143 (Similarity: 0.7672)\n",
      "  4. Recommended Paper ID: 530 (Similarity: 0.7507)\n",
      "  5. Recommended Paper ID: 184 (Similarity: 0.7479)\n",
      "  6. Recommended Paper ID: 370 (Similarity: 0.7475)\n",
      "  7. Recommended Paper ID: 281 (Similarity: 0.7467)\n",
      "  8. Recommended Paper ID: 442 (Similarity: 0.7445)\n",
      "  9. Recommended Paper ID: 199 (Similarity: 0.7402)\n",
      "  10. Recommended Paper ID: 5 (Similarity: 0.7398)\n",
      "\n",
      "Test Paper 8:\n",
      "  1. Recommended Paper ID: 47 (Similarity: 0.7417)\n",
      "  2. Recommended Paper ID: 420 (Similarity: 0.7320)\n",
      "  3. Recommended Paper ID: 100 (Similarity: 0.7234)\n",
      "  4. Recommended Paper ID: 168 (Similarity: 0.7227)\n",
      "  5. Recommended Paper ID: 297 (Similarity: 0.7201)\n",
      "  6. Recommended Paper ID: 583 (Similarity: 0.7144)\n",
      "  7. Recommended Paper ID: 448 (Similarity: 0.7133)\n",
      "  8. Recommended Paper ID: 331 (Similarity: 0.7082)\n",
      "  9. Recommended Paper ID: 345 (Similarity: 0.7042)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7042)\n",
      "\n",
      "Test Paper 9:\n",
      "  1. Recommended Paper ID: 452 (Similarity: 0.7492)\n",
      "  2. Recommended Paper ID: 493 (Similarity: 0.7433)\n",
      "  3. Recommended Paper ID: 419 (Similarity: 0.7289)\n",
      "  4. Recommended Paper ID: 583 (Similarity: 0.7120)\n",
      "  5. Recommended Paper ID: 495 (Similarity: 0.7111)\n",
      "  6. Recommended Paper ID: 481 (Similarity: 0.7111)\n",
      "  7. Recommended Paper ID: 440 (Similarity: 0.7108)\n",
      "  8. Recommended Paper ID: 358 (Similarity: 0.7100)\n",
      "  9. Recommended Paper ID: 345 (Similarity: 0.7073)\n",
      "  10. Recommended Paper ID: 116 (Similarity: 0.7069)\n",
      "\n",
      "Test Paper 10:\n",
      "  1. Recommended Paper ID: 529 (Similarity: 0.8073)\n",
      "  2. Recommended Paper ID: 557 (Similarity: 0.7889)\n",
      "  3. Recommended Paper ID: 495 (Similarity: 0.7855)\n",
      "  4. Recommended Paper ID: 494 (Similarity: 0.7851)\n",
      "  5. Recommended Paper ID: 379 (Similarity: 0.7807)\n",
      "  6. Recommended Paper ID: 419 (Similarity: 0.7777)\n",
      "  7. Recommended Paper ID: 100 (Similarity: 0.7774)\n",
      "  8. Recommended Paper ID: 353 (Similarity: 0.7768)\n",
      "  9. Recommended Paper ID: 583 (Similarity: 0.7766)\n",
      "  10. Recommended Paper ID: 556 (Similarity: 0.7745)\n",
      "\n",
      "Test Paper 11:\n",
      "  1. Recommended Paper ID: 495 (Similarity: 0.8227)\n",
      "  2. Recommended Paper ID: 248 (Similarity: 0.8132)\n",
      "  3. Recommended Paper ID: 452 (Similarity: 0.8079)\n",
      "  4. Recommended Paper ID: 440 (Similarity: 0.8078)\n",
      "  5. Recommended Paper ID: 494 (Similarity: 0.8044)\n",
      "  6. Recommended Paper ID: 419 (Similarity: 0.8013)\n",
      "  7. Recommended Paper ID: 481 (Similarity: 0.8011)\n",
      "  8. Recommended Paper ID: 5 (Similarity: 0.8000)\n",
      "  9. Recommended Paper ID: 507 (Similarity: 0.7986)\n",
      "  10. Recommended Paper ID: 353 (Similarity: 0.7981)\n",
      "\n",
      "Test Paper 12:\n",
      "  1. Recommended Paper ID: 54 (Similarity: 0.7305)\n",
      "  2. Recommended Paper ID: 510 (Similarity: 0.7246)\n",
      "  3. Recommended Paper ID: 16 (Similarity: 0.7243)\n",
      "  4. Recommended Paper ID: 501 (Similarity: 0.7193)\n",
      "  5. Recommended Paper ID: 100 (Similarity: 0.6890)\n",
      "  6. Recommended Paper ID: 94 (Similarity: 0.6794)\n",
      "  7. Recommended Paper ID: 5 (Similarity: 0.6710)\n",
      "  8. Recommended Paper ID: 580 (Similarity: 0.6701)\n",
      "  9. Recommended Paper ID: 563 (Similarity: 0.6675)\n",
      "  10. Recommended Paper ID: 477 (Similarity: 0.6657)\n",
      "\n",
      "Test Paper 13:\n",
      "  1. Recommended Paper ID: 317 (Similarity: 0.6974)\n",
      "  2. Recommended Paper ID: 395 (Similarity: 0.6971)\n",
      "  3. Recommended Paper ID: 496 (Similarity: 0.6847)\n",
      "  4. Recommended Paper ID: 464 (Similarity: 0.6701)\n",
      "  5. Recommended Paper ID: 101 (Similarity: 0.6636)\n",
      "  6. Recommended Paper ID: 311 (Similarity: 0.6438)\n",
      "  7. Recommended Paper ID: 258 (Similarity: 0.6436)\n",
      "  8. Recommended Paper ID: 373 (Similarity: 0.6436)\n",
      "  9. Recommended Paper ID: 201 (Similarity: 0.6426)\n",
      "  10. Recommended Paper ID: 109 (Similarity: 0.6419)\n",
      "\n",
      "Test Paper 14:\n",
      "  1. Recommended Paper ID: 14 (Similarity: 0.8369)\n",
      "  2. Recommended Paper ID: 184 (Similarity: 0.8223)\n",
      "  3. Recommended Paper ID: 419 (Similarity: 0.8210)\n",
      "  4. Recommended Paper ID: 119 (Similarity: 0.8166)\n",
      "  5. Recommended Paper ID: 312 (Similarity: 0.8158)\n",
      "  6. Recommended Paper ID: 163 (Similarity: 0.8153)\n",
      "  7. Recommended Paper ID: 583 (Similarity: 0.8120)\n",
      "  8. Recommended Paper ID: 529 (Similarity: 0.8038)\n",
      "  9. Recommended Paper ID: 116 (Similarity: 0.8033)\n",
      "  10. Recommended Paper ID: 23 (Similarity: 0.8021)\n",
      "\n",
      "Test Paper 15:\n",
      "  1. Recommended Paper ID: 387 (Similarity: 0.7338)\n",
      "  2. Recommended Paper ID: 278 (Similarity: 0.7331)\n",
      "  3. Recommended Paper ID: 331 (Similarity: 0.7234)\n",
      "  4. Recommended Paper ID: 176 (Similarity: 0.7233)\n",
      "  5. Recommended Paper ID: 378 (Similarity: 0.7212)\n",
      "  6. Recommended Paper ID: 297 (Similarity: 0.7155)\n",
      "  7. Recommended Paper ID: 491 (Similarity: 0.7103)\n",
      "  8. Recommended Paper ID: 41 (Similarity: 0.7084)\n",
      "  9. Recommended Paper ID: 61 (Similarity: 0.7051)\n",
      "  10. Recommended Paper ID: 578 (Similarity: 0.7028)\n",
      "\n",
      "Test Paper 16:\n",
      "  1. Recommended Paper ID: 521 (Similarity: 0.7301)\n",
      "  2. Recommended Paper ID: 285 (Similarity: 0.7196)\n",
      "  3. Recommended Paper ID: 61 (Similarity: 0.7179)\n",
      "  4. Recommended Paper ID: 124 (Similarity: 0.7177)\n",
      "  5. Recommended Paper ID: 289 (Similarity: 0.7173)\n",
      "  6. Recommended Paper ID: 180 (Similarity: 0.7146)\n",
      "  7. Recommended Paper ID: 353 (Similarity: 0.7120)\n",
      "  8. Recommended Paper ID: 270 (Similarity: 0.7116)\n",
      "  9. Recommended Paper ID: 419 (Similarity: 0.7112)\n",
      "  10. Recommended Paper ID: 53 (Similarity: 0.7076)\n",
      "\n",
      "Test Paper 17:\n",
      "  1. Recommended Paper ID: 214 (Similarity: 0.7768)\n",
      "  2. Recommended Paper ID: 260 (Similarity: 0.7764)\n",
      "  3. Recommended Paper ID: 61 (Similarity: 0.7729)\n",
      "  4. Recommended Paper ID: 448 (Similarity: 0.7704)\n",
      "  5. Recommended Paper ID: 508 (Similarity: 0.7699)\n",
      "  6. Recommended Paper ID: 312 (Similarity: 0.7661)\n",
      "  7. Recommended Paper ID: 550 (Similarity: 0.7634)\n",
      "  8. Recommended Paper ID: 5 (Similarity: 0.7622)\n",
      "  9. Recommended Paper ID: 283 (Similarity: 0.7610)\n",
      "  10. Recommended Paper ID: 529 (Similarity: 0.7591)\n",
      "\n",
      "Test Paper 18:\n",
      "  1. Recommended Paper ID: 130 (Similarity: 0.7672)\n",
      "  2. Recommended Paper ID: 219 (Similarity: 0.7491)\n",
      "  3. Recommended Paper ID: 300 (Similarity: 0.7040)\n",
      "  4. Recommended Paper ID: 279 (Similarity: 0.6613)\n",
      "  5. Recommended Paper ID: 108 (Similarity: 0.6512)\n",
      "  6. Recommended Paper ID: 198 (Similarity: 0.6357)\n",
      "  7. Recommended Paper ID: 280 (Similarity: 0.6322)\n",
      "  8. Recommended Paper ID: 288 (Similarity: 0.6319)\n",
      "  9. Recommended Paper ID: 495 (Similarity: 0.6301)\n",
      "  10. Recommended Paper ID: 192 (Similarity: 0.6261)\n",
      "\n",
      "Test Paper 19:\n",
      "  1. Recommended Paper ID: 52 (Similarity: 0.7541)\n",
      "  2. Recommended Paper ID: 370 (Similarity: 0.7261)\n",
      "  3. Recommended Paper ID: 583 (Similarity: 0.7236)\n",
      "  4. Recommended Paper ID: 419 (Similarity: 0.7235)\n",
      "  5. Recommended Paper ID: 507 (Similarity: 0.7219)\n",
      "  6. Recommended Paper ID: 544 (Similarity: 0.7211)\n",
      "  7. Recommended Paper ID: 61 (Similarity: 0.7179)\n",
      "  8. Recommended Paper ID: 229 (Similarity: 0.7167)\n",
      "  9. Recommended Paper ID: 19 (Similarity: 0.7151)\n",
      "  10. Recommended Paper ID: 294 (Similarity: 0.7151)\n",
      "\n",
      "Test Paper 20:\n",
      "  1. Recommended Paper ID: 120 (Similarity: 0.8194)\n",
      "  2. Recommended Paper ID: 590 (Similarity: 0.8161)\n",
      "  3. Recommended Paper ID: 495 (Similarity: 0.8158)\n",
      "  4. Recommended Paper ID: 34 (Similarity: 0.8151)\n",
      "  5. Recommended Paper ID: 353 (Similarity: 0.8030)\n",
      "  6. Recommended Paper ID: 23 (Similarity: 0.7899)\n",
      "  7. Recommended Paper ID: 391 (Similarity: 0.7883)\n",
      "  8. Recommended Paper ID: 116 (Similarity: 0.7836)\n",
      "  9. Recommended Paper ID: 159 (Similarity: 0.7835)\n",
      "  10. Recommended Paper ID: 583 (Similarity: 0.7832)\n",
      "\n",
      "Test Paper 21:\n",
      "  1. Recommended Paper ID: 23 (Similarity: 0.7810)\n",
      "  2. Recommended Paper ID: 558 (Similarity: 0.7767)\n",
      "  3. Recommended Paper ID: 353 (Similarity: 0.7748)\n",
      "  4. Recommended Paper ID: 590 (Similarity: 0.7745)\n",
      "  5. Recommended Paper ID: 493 (Similarity: 0.7690)\n",
      "  6. Recommended Paper ID: 320 (Similarity: 0.7678)\n",
      "  7. Recommended Paper ID: 297 (Similarity: 0.7671)\n",
      "  8. Recommended Paper ID: 507 (Similarity: 0.7668)\n",
      "  9. Recommended Paper ID: 61 (Similarity: 0.7656)\n",
      "  10. Recommended Paper ID: 448 (Similarity: 0.7650)\n",
      "\n",
      "Test Paper 22:\n",
      "  1. Recommended Paper ID: 478 (Similarity: 0.7838)\n",
      "  2. Recommended Paper ID: 498 (Similarity: 0.7734)\n",
      "  3. Recommended Paper ID: 127 (Similarity: 0.7725)\n",
      "  4. Recommended Paper ID: 263 (Similarity: 0.7670)\n",
      "  5. Recommended Paper ID: 493 (Similarity: 0.7643)\n",
      "  6. Recommended Paper ID: 5 (Similarity: 0.7616)\n",
      "  7. Recommended Paper ID: 369 (Similarity: 0.7580)\n",
      "  8. Recommended Paper ID: 180 (Similarity: 0.7580)\n",
      "  9. Recommended Paper ID: 199 (Similarity: 0.7564)\n",
      "  10. Recommended Paper ID: 119 (Similarity: 0.7556)\n",
      "\n",
      "Test Paper 23:\n",
      "  1. Recommended Paper ID: 256 (Similarity: 0.6922)\n",
      "  2. Recommended Paper ID: 147 (Similarity: 0.6890)\n",
      "  3. Recommended Paper ID: 445 (Similarity: 0.6858)\n",
      "  4. Recommended Paper ID: 577 (Similarity: 0.6838)\n",
      "  5. Recommended Paper ID: 370 (Similarity: 0.6823)\n",
      "  6. Recommended Paper ID: 402 (Similarity: 0.6812)\n",
      "  7. Recommended Paper ID: 200 (Similarity: 0.6766)\n",
      "  8. Recommended Paper ID: 165 (Similarity: 0.6755)\n",
      "  9. Recommended Paper ID: 331 (Similarity: 0.6704)\n",
      "  10. Recommended Paper ID: 53 (Similarity: 0.6703)\n",
      "\n",
      "Test Paper 24:\n",
      "  1. Recommended Paper ID: 5 (Similarity: 0.7822)\n",
      "  2. Recommended Paper ID: 552 (Similarity: 0.7811)\n",
      "  3. Recommended Paper ID: 300 (Similarity: 0.7664)\n",
      "  4. Recommended Paper ID: 280 (Similarity: 0.7628)\n",
      "  5. Recommended Paper ID: 379 (Similarity: 0.7604)\n",
      "  6. Recommended Paper ID: 557 (Similarity: 0.7545)\n",
      "  7. Recommended Paper ID: 248 (Similarity: 0.7539)\n",
      "  8. Recommended Paper ID: 152 (Similarity: 0.7538)\n",
      "  9. Recommended Paper ID: 550 (Similarity: 0.7533)\n",
      "  10. Recommended Paper ID: 170 (Similarity: 0.7458)\n",
      "\n",
      "Test Paper 25:\n",
      "  1. Recommended Paper ID: 435 (Similarity: 0.8147)\n",
      "  2. Recommended Paper ID: 493 (Similarity: 0.8127)\n",
      "  3. Recommended Paper ID: 119 (Similarity: 0.8009)\n",
      "  4. Recommended Paper ID: 468 (Similarity: 0.7920)\n",
      "  5. Recommended Paper ID: 23 (Similarity: 0.7862)\n",
      "  6. Recommended Paper ID: 216 (Similarity: 0.7777)\n",
      "  7. Recommended Paper ID: 100 (Similarity: 0.7771)\n",
      "  8. Recommended Paper ID: 462 (Similarity: 0.7743)\n",
      "  9. Recommended Paper ID: 583 (Similarity: 0.7733)\n",
      "  10. Recommended Paper ID: 218 (Similarity: 0.7731)\n",
      "\n",
      "Test Paper 26:\n",
      "  1. Recommended Paper ID: 530 (Similarity: 0.7468)\n",
      "  2. Recommended Paper ID: 180 (Similarity: 0.7440)\n",
      "  3. Recommended Paper ID: 419 (Similarity: 0.7149)\n",
      "  4. Recommended Paper ID: 579 (Similarity: 0.7145)\n",
      "  5. Recommended Paper ID: 513 (Similarity: 0.7120)\n",
      "  6. Recommended Paper ID: 150 (Similarity: 0.7115)\n",
      "  7. Recommended Paper ID: 285 (Similarity: 0.7108)\n",
      "  8. Recommended Paper ID: 514 (Similarity: 0.7052)\n",
      "  9. Recommended Paper ID: 199 (Similarity: 0.6996)\n",
      "  10. Recommended Paper ID: 443 (Similarity: 0.6988)\n",
      "\n",
      "Test Paper 27:\n",
      "  1. Recommended Paper ID: 94 (Similarity: 0.7654)\n",
      "  2. Recommended Paper ID: 25 (Similarity: 0.7411)\n",
      "  3. Recommended Paper ID: 502 (Similarity: 0.7399)\n",
      "  4. Recommended Paper ID: 529 (Similarity: 0.7366)\n",
      "  5. Recommended Paper ID: 452 (Similarity: 0.7318)\n",
      "  6. Recommended Paper ID: 119 (Similarity: 0.7280)\n",
      "  7. Recommended Paper ID: 379 (Similarity: 0.7245)\n",
      "  8. Recommended Paper ID: 152 (Similarity: 0.7175)\n",
      "  9. Recommended Paper ID: 218 (Similarity: 0.7155)\n",
      "  10. Recommended Paper ID: 176 (Similarity: 0.7137)\n",
      "\n",
      "Test Paper 28:\n",
      "  1. Recommended Paper ID: 286 (Similarity: 0.7820)\n",
      "  2. Recommended Paper ID: 455 (Similarity: 0.7727)\n",
      "  3. Recommended Paper ID: 5 (Similarity: 0.7660)\n",
      "  4. Recommended Paper ID: 371 (Similarity: 0.7629)\n",
      "  5. Recommended Paper ID: 248 (Similarity: 0.7623)\n",
      "  6. Recommended Paper ID: 448 (Similarity: 0.7530)\n",
      "  7. Recommended Paper ID: 23 (Similarity: 0.7509)\n",
      "  8. Recommended Paper ID: 478 (Similarity: 0.7507)\n",
      "  9. Recommended Paper ID: 440 (Similarity: 0.7444)\n",
      "  10. Recommended Paper ID: 544 (Similarity: 0.7427)\n",
      "\n",
      "Test Paper 29:\n",
      "  1. Recommended Paper ID: 152 (Similarity: 0.7912)\n",
      "  2. Recommended Paper ID: 557 (Similarity: 0.7650)\n",
      "  3. Recommended Paper ID: 383 (Similarity: 0.7607)\n",
      "  4. Recommended Paper ID: 320 (Similarity: 0.7586)\n",
      "  5. Recommended Paper ID: 170 (Similarity: 0.7539)\n",
      "  6. Recommended Paper ID: 379 (Similarity: 0.7491)\n",
      "  7. Recommended Paper ID: 16 (Similarity: 0.7373)\n",
      "  8. Recommended Paper ID: 507 (Similarity: 0.7330)\n",
      "  9. Recommended Paper ID: 556 (Similarity: 0.7294)\n",
      "  10. Recommended Paper ID: 300 (Similarity: 0.7273)\n",
      "\n",
      "Test Paper 30:\n",
      "  1. Recommended Paper ID: 431 (Similarity: 0.8944)\n",
      "  2. Recommended Paper ID: 495 (Similarity: 0.8773)\n",
      "  3. Recommended Paper ID: 452 (Similarity: 0.8411)\n",
      "  4. Recommended Paper ID: 391 (Similarity: 0.8376)\n",
      "  5. Recommended Paper ID: 379 (Similarity: 0.8344)\n",
      "  6. Recommended Paper ID: 583 (Similarity: 0.8324)\n",
      "  7. Recommended Paper ID: 529 (Similarity: 0.8307)\n",
      "  8. Recommended Paper ID: 493 (Similarity: 0.8302)\n",
      "  9. Recommended Paper ID: 68 (Similarity: 0.8269)\n",
      "  10. Recommended Paper ID: 174 (Similarity: 0.8262)\n",
      "\n",
      "Test Paper 31:\n",
      "  1. Recommended Paper ID: 98 (Similarity: 0.8001)\n",
      "  2. Recommended Paper ID: 8 (Similarity: 0.7745)\n",
      "  3. Recommended Paper ID: 77 (Similarity: 0.7674)\n",
      "  4. Recommended Paper ID: 216 (Similarity: 0.7595)\n",
      "  5. Recommended Paper ID: 169 (Similarity: 0.7487)\n",
      "  6. Recommended Paper ID: 462 (Similarity: 0.7478)\n",
      "  7. Recommended Paper ID: 415 (Similarity: 0.7440)\n",
      "  8. Recommended Paper ID: 108 (Similarity: 0.7435)\n",
      "  9. Recommended Paper ID: 14 (Similarity: 0.7414)\n",
      "  10. Recommended Paper ID: 116 (Similarity: 0.7357)\n",
      "\n",
      "Test Paper 32:\n",
      "  1. Recommended Paper ID: 557 (Similarity: 0.7894)\n",
      "  2. Recommended Paper ID: 5 (Similarity: 0.7882)\n",
      "  3. Recommended Paper ID: 419 (Similarity: 0.7831)\n",
      "  4. Recommended Paper ID: 529 (Similarity: 0.7814)\n",
      "  5. Recommended Paper ID: 14 (Similarity: 0.7776)\n",
      "  6. Recommended Paper ID: 452 (Similarity: 0.7726)\n",
      "  7. Recommended Paper ID: 376 (Similarity: 0.7715)\n",
      "  8. Recommended Paper ID: 379 (Similarity: 0.7701)\n",
      "  9. Recommended Paper ID: 353 (Similarity: 0.7696)\n",
      "  10. Recommended Paper ID: 311 (Similarity: 0.7561)\n",
      "\n",
      "Test Paper 33:\n",
      "  1. Recommended Paper ID: 442 (Similarity: 0.7943)\n",
      "  2. Recommended Paper ID: 230 (Similarity: 0.7753)\n",
      "  3. Recommended Paper ID: 94 (Similarity: 0.7736)\n",
      "  4. Recommended Paper ID: 5 (Similarity: 0.7705)\n",
      "  5. Recommended Paper ID: 248 (Similarity: 0.7702)\n",
      "  6. Recommended Paper ID: 379 (Similarity: 0.7675)\n",
      "  7. Recommended Paper ID: 419 (Similarity: 0.7667)\n",
      "  8. Recommended Paper ID: 297 (Similarity: 0.7630)\n",
      "  9. Recommended Paper ID: 452 (Similarity: 0.7626)\n",
      "  10. Recommended Paper ID: 278 (Similarity: 0.7615)\n",
      "\n",
      "Test Paper 34:\n",
      "  1. Recommended Paper ID: 5 (Similarity: 0.7739)\n",
      "  2. Recommended Paper ID: 477 (Similarity: 0.7665)\n",
      "  3. Recommended Paper ID: 508 (Similarity: 0.7652)\n",
      "  4. Recommended Paper ID: 170 (Similarity: 0.7584)\n",
      "  5. Recommended Paper ID: 152 (Similarity: 0.7571)\n",
      "  6. Recommended Paper ID: 41 (Similarity: 0.7558)\n",
      "  7. Recommended Paper ID: 238 (Similarity: 0.7517)\n",
      "  8. Recommended Paper ID: 100 (Similarity: 0.7474)\n",
      "  9. Recommended Paper ID: 23 (Similarity: 0.7469)\n",
      "  10. Recommended Paper ID: 353 (Similarity: 0.7436)\n",
      "\n",
      "Test Paper 35:\n",
      "  1. Recommended Paper ID: 393 (Similarity: 0.7481)\n",
      "  2. Recommended Paper ID: 376 (Similarity: 0.7346)\n",
      "  3. Recommended Paper ID: 581 (Similarity: 0.7340)\n",
      "  4. Recommended Paper ID: 55 (Similarity: 0.7281)\n",
      "  5. Recommended Paper ID: 502 (Similarity: 0.7273)\n",
      "  6. Recommended Paper ID: 176 (Similarity: 0.7199)\n",
      "  7. Recommended Paper ID: 311 (Similarity: 0.7170)\n",
      "  8. Recommended Paper ID: 101 (Similarity: 0.7133)\n",
      "  9. Recommended Paper ID: 180 (Similarity: 0.7116)\n",
      "  10. Recommended Paper ID: 198 (Similarity: 0.7097)\n",
      "\n",
      "Test Paper 36:\n",
      "  1. Recommended Paper ID: 583 (Similarity: 0.8066)\n",
      "  2. Recommended Paper ID: 448 (Similarity: 0.8062)\n",
      "  3. Recommended Paper ID: 61 (Similarity: 0.8009)\n",
      "  4. Recommended Paper ID: 495 (Similarity: 0.7976)\n",
      "  5. Recommended Paper ID: 41 (Similarity: 0.7974)\n",
      "  6. Recommended Paper ID: 180 (Similarity: 0.7935)\n",
      "  7. Recommended Paper ID: 379 (Similarity: 0.7905)\n",
      "  8. Recommended Paper ID: 507 (Similarity: 0.7895)\n",
      "  9. Recommended Paper ID: 529 (Similarity: 0.7822)\n",
      "  10. Recommended Paper ID: 493 (Similarity: 0.7814)\n",
      "\n",
      "Test Paper 37:\n",
      "  1. Recommended Paper ID: 300 (Similarity: 0.7202)\n",
      "  2. Recommended Paper ID: 218 (Similarity: 0.7123)\n",
      "  3. Recommended Paper ID: 170 (Similarity: 0.6920)\n",
      "  4. Recommended Paper ID: 108 (Similarity: 0.6908)\n",
      "  5. Recommended Paper ID: 399 (Similarity: 0.6899)\n",
      "  6. Recommended Paper ID: 258 (Similarity: 0.6823)\n",
      "  7. Recommended Paper ID: 47 (Similarity: 0.6790)\n",
      "  8. Recommended Paper ID: 130 (Similarity: 0.6784)\n",
      "  9. Recommended Paper ID: 561 (Similarity: 0.6750)\n",
      "  10. Recommended Paper ID: 478 (Similarity: 0.6718)\n",
      "\n",
      "Test Paper 38:\n",
      "  1. Recommended Paper ID: 227 (Similarity: 0.7884)\n",
      "  2. Recommended Paper ID: 312 (Similarity: 0.7752)\n",
      "  3. Recommended Paper ID: 184 (Similarity: 0.7741)\n",
      "  4. Recommended Paper ID: 14 (Similarity: 0.7533)\n",
      "  5. Recommended Paper ID: 34 (Similarity: 0.7520)\n",
      "  6. Recommended Paper ID: 550 (Similarity: 0.7473)\n",
      "  7. Recommended Paper ID: 578 (Similarity: 0.7448)\n",
      "  8. Recommended Paper ID: 422 (Similarity: 0.7396)\n",
      "  9. Recommended Paper ID: 268 (Similarity: 0.7369)\n",
      "  10. Recommended Paper ID: 370 (Similarity: 0.7355)\n",
      "\n",
      "Test Paper 39:\n",
      "  1. Recommended Paper ID: 312 (Similarity: 0.7616)\n",
      "  2. Recommended Paper ID: 306 (Similarity: 0.7471)\n",
      "  3. Recommended Paper ID: 119 (Similarity: 0.7390)\n",
      "  4. Recommended Paper ID: 435 (Similarity: 0.7315)\n",
      "  5. Recommended Paper ID: 292 (Similarity: 0.7305)\n",
      "  6. Recommended Paper ID: 300 (Similarity: 0.7303)\n",
      "  7. Recommended Paper ID: 69 (Similarity: 0.7246)\n",
      "  8. Recommended Paper ID: 525 (Similarity: 0.7175)\n",
      "  9. Recommended Paper ID: 218 (Similarity: 0.7079)\n",
      "  10. Recommended Paper ID: 126 (Similarity: 0.7052)\n",
      "\n",
      "Test Paper 40:\n",
      "  1. Recommended Paper ID: 52 (Similarity: 0.7409)\n",
      "  2. Recommended Paper ID: 556 (Similarity: 0.7345)\n",
      "  3. Recommended Paper ID: 583 (Similarity: 0.7308)\n",
      "  4. Recommended Paper ID: 180 (Similarity: 0.7266)\n",
      "  5. Recommended Paper ID: 370 (Similarity: 0.7245)\n",
      "  6. Recommended Paper ID: 544 (Similarity: 0.7229)\n",
      "  7. Recommended Paper ID: 156 (Similarity: 0.7222)\n",
      "  8. Recommended Paper ID: 169 (Similarity: 0.7201)\n",
      "  9. Recommended Paper ID: 516 (Similarity: 0.7191)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7184)\n",
      "\n",
      "Test Paper 41:\n",
      "  1. Recommended Paper ID: 504 (Similarity: 0.8187)\n",
      "  2. Recommended Paper ID: 166 (Similarity: 0.8061)\n",
      "  3. Recommended Paper ID: 294 (Similarity: 0.7913)\n",
      "  4. Recommended Paper ID: 448 (Similarity: 0.7721)\n",
      "  5. Recommended Paper ID: 477 (Similarity: 0.7705)\n",
      "  6. Recommended Paper ID: 494 (Similarity: 0.7514)\n",
      "  7. Recommended Paper ID: 23 (Similarity: 0.7510)\n",
      "  8. Recommended Paper ID: 538 (Similarity: 0.7480)\n",
      "  9. Recommended Paper ID: 312 (Similarity: 0.7471)\n",
      "  10. Recommended Paper ID: 510 (Similarity: 0.7446)\n",
      "\n",
      "Test Paper 42:\n",
      "  1. Recommended Paper ID: 98 (Similarity: 0.8279)\n",
      "  2. Recommended Paper ID: 360 (Similarity: 0.8095)\n",
      "  3. Recommended Paper ID: 216 (Similarity: 0.7958)\n",
      "  4. Recommended Paper ID: 61 (Similarity: 0.7894)\n",
      "  5. Recommended Paper ID: 120 (Similarity: 0.7863)\n",
      "  6. Recommended Paper ID: 377 (Similarity: 0.7850)\n",
      "  7. Recommended Paper ID: 119 (Similarity: 0.7807)\n",
      "  8. Recommended Paper ID: 137 (Similarity: 0.7803)\n",
      "  9. Recommended Paper ID: 160 (Similarity: 0.7744)\n",
      "  10. Recommended Paper ID: 174 (Similarity: 0.7703)\n",
      "\n",
      "Test Paper 43:\n",
      "  1. Recommended Paper ID: 328 (Similarity: 0.7509)\n",
      "  2. Recommended Paper ID: 216 (Similarity: 0.7381)\n",
      "  3. Recommended Paper ID: 38 (Similarity: 0.7319)\n",
      "  4. Recommended Paper ID: 98 (Similarity: 0.7090)\n",
      "  5. Recommended Paper ID: 55 (Similarity: 0.7002)\n",
      "  6. Recommended Paper ID: 228 (Similarity: 0.6975)\n",
      "  7. Recommended Paper ID: 297 (Similarity: 0.6956)\n",
      "  8. Recommended Paper ID: 369 (Similarity: 0.6938)\n",
      "  9. Recommended Paper ID: 393 (Similarity: 0.6904)\n",
      "  10. Recommended Paper ID: 502 (Similarity: 0.6887)\n",
      "\n",
      "Test Paper 44:\n",
      "  1. Recommended Paper ID: 125 (Similarity: 0.7132)\n",
      "  2. Recommended Paper ID: 165 (Similarity: 0.6803)\n",
      "  3. Recommended Paper ID: 425 (Similarity: 0.6605)\n",
      "  4. Recommended Paper ID: 354 (Similarity: 0.6600)\n",
      "  5. Recommended Paper ID: 152 (Similarity: 0.6337)\n",
      "  6. Recommended Paper ID: 432 (Similarity: 0.6287)\n",
      "  7. Recommended Paper ID: 454 (Similarity: 0.6255)\n",
      "  8. Recommended Paper ID: 491 (Similarity: 0.6209)\n",
      "  9. Recommended Paper ID: 100 (Similarity: 0.6204)\n",
      "  10. Recommended Paper ID: 106 (Similarity: 0.6158)\n",
      "\n",
      "Test Paper 45:\n",
      "  1. Recommended Paper ID: 258 (Similarity: 0.7793)\n",
      "  2. Recommended Paper ID: 449 (Similarity: 0.7467)\n",
      "  3. Recommended Paper ID: 180 (Similarity: 0.7437)\n",
      "  4. Recommended Paper ID: 228 (Similarity: 0.7270)\n",
      "  5. Recommended Paper ID: 574 (Similarity: 0.7245)\n",
      "  6. Recommended Paper ID: 199 (Similarity: 0.7187)\n",
      "  7. Recommended Paper ID: 328 (Similarity: 0.7171)\n",
      "  8. Recommended Paper ID: 401 (Similarity: 0.7105)\n",
      "  9. Recommended Paper ID: 14 (Similarity: 0.7102)\n",
      "  10. Recommended Paper ID: 205 (Similarity: 0.7101)\n",
      "\n",
      "Test Paper 46:\n",
      "  1. Recommended Paper ID: 406 (Similarity: 0.7117)\n",
      "  2. Recommended Paper ID: 258 (Similarity: 0.7036)\n",
      "  3. Recommended Paper ID: 457 (Similarity: 0.7031)\n",
      "  4. Recommended Paper ID: 126 (Similarity: 0.7027)\n",
      "  5. Recommended Paper ID: 148 (Similarity: 0.6790)\n",
      "  6. Recommended Paper ID: 64 (Similarity: 0.6781)\n",
      "  7. Recommended Paper ID: 201 (Similarity: 0.6739)\n",
      "  8. Recommended Paper ID: 521 (Similarity: 0.6731)\n",
      "  9. Recommended Paper ID: 498 (Similarity: 0.6713)\n",
      "  10. Recommended Paper ID: 502 (Similarity: 0.6611)\n",
      "\n",
      "Test Paper 47:\n",
      "  1. Recommended Paper ID: 4 (Similarity: 0.7655)\n",
      "  2. Recommended Paper ID: 448 (Similarity: 0.7642)\n",
      "  3. Recommended Paper ID: 375 (Similarity: 0.7562)\n",
      "  4. Recommended Paper ID: 210 (Similarity: 0.7510)\n",
      "  5. Recommended Paper ID: 141 (Similarity: 0.7490)\n",
      "  6. Recommended Paper ID: 550 (Similarity: 0.7449)\n",
      "  7. Recommended Paper ID: 286 (Similarity: 0.7432)\n",
      "  8. Recommended Paper ID: 297 (Similarity: 0.7416)\n",
      "  9. Recommended Paper ID: 508 (Similarity: 0.7333)\n",
      "  10. Recommended Paper ID: 444 (Similarity: 0.7322)\n",
      "\n",
      "Test Paper 48:\n",
      "  1. Recommended Paper ID: 418 (Similarity: 0.8727)\n",
      "  2. Recommended Paper ID: 462 (Similarity: 0.8578)\n",
      "  3. Recommended Paper ID: 77 (Similarity: 0.8323)\n",
      "  4. Recommended Paper ID: 98 (Similarity: 0.8222)\n",
      "  5. Recommended Paper ID: 435 (Similarity: 0.7949)\n",
      "  6. Recommended Paper ID: 493 (Similarity: 0.7914)\n",
      "  7. Recommended Paper ID: 328 (Similarity: 0.7895)\n",
      "  8. Recommended Paper ID: 216 (Similarity: 0.7866)\n",
      "  9. Recommended Paper ID: 25 (Similarity: 0.7839)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7803)\n",
      "\n",
      "Test Paper 49:\n",
      "  1. Recommended Paper ID: 514 (Similarity: 0.8071)\n",
      "  2. Recommended Paper ID: 303 (Similarity: 0.8036)\n",
      "  3. Recommended Paper ID: 100 (Similarity: 0.7977)\n",
      "  4. Recommended Paper ID: 583 (Similarity: 0.7968)\n",
      "  5. Recommended Paper ID: 297 (Similarity: 0.7919)\n",
      "  6. Recommended Paper ID: 94 (Similarity: 0.7869)\n",
      "  7. Recommended Paper ID: 477 (Similarity: 0.7864)\n",
      "  8. Recommended Paper ID: 448 (Similarity: 0.7818)\n",
      "  9. Recommended Paper ID: 205 (Similarity: 0.7813)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7812)\n",
      "\n",
      "Test Paper 50:\n",
      "  1. Recommended Paper ID: 68 (Similarity: 0.7254)\n",
      "  2. Recommended Paper ID: 14 (Similarity: 0.7106)\n",
      "  3. Recommended Paper ID: 205 (Similarity: 0.7052)\n",
      "  4. Recommended Paper ID: 180 (Similarity: 0.7042)\n",
      "  5. Recommended Paper ID: 583 (Similarity: 0.7039)\n",
      "  6. Recommended Paper ID: 452 (Similarity: 0.7007)\n",
      "  7. Recommended Paper ID: 440 (Similarity: 0.6970)\n",
      "  8. Recommended Paper ID: 65 (Similarity: 0.6951)\n",
      "  9. Recommended Paper ID: 495 (Similarity: 0.6934)\n",
      "  10. Recommended Paper ID: 391 (Similarity: 0.6920)\n",
      "\n",
      "Test Paper 51:\n",
      "  1. Recommended Paper ID: 288 (Similarity: 0.7846)\n",
      "  2. Recommended Paper ID: 493 (Similarity: 0.7686)\n",
      "  3. Recommended Paper ID: 441 (Similarity: 0.7505)\n",
      "  4. Recommended Paper ID: 100 (Similarity: 0.7488)\n",
      "  5. Recommended Paper ID: 279 (Similarity: 0.7483)\n",
      "  6. Recommended Paper ID: 498 (Similarity: 0.7356)\n",
      "  7. Recommended Paper ID: 488 (Similarity: 0.7279)\n",
      "  8. Recommended Paper ID: 353 (Similarity: 0.7221)\n",
      "  9. Recommended Paper ID: 452 (Similarity: 0.7129)\n",
      "  10. Recommended Paper ID: 192 (Similarity: 0.7088)\n",
      "\n",
      "Test Paper 52:\n",
      "  1. Recommended Paper ID: 498 (Similarity: 0.8496)\n",
      "  2. Recommended Paper ID: 495 (Similarity: 0.8358)\n",
      "  3. Recommended Paper ID: 590 (Similarity: 0.8307)\n",
      "  4. Recommended Paper ID: 583 (Similarity: 0.8249)\n",
      "  5. Recommended Paper ID: 23 (Similarity: 0.8167)\n",
      "  6. Recommended Paper ID: 494 (Similarity: 0.8151)\n",
      "  7. Recommended Paper ID: 529 (Similarity: 0.8134)\n",
      "  8. Recommended Paper ID: 435 (Similarity: 0.8115)\n",
      "  9. Recommended Paper ID: 431 (Similarity: 0.8098)\n",
      "  10. Recommended Paper ID: 478 (Similarity: 0.8080)\n",
      "\n",
      "Test Paper 53:\n",
      "  1. Recommended Paper ID: 297 (Similarity: 0.8248)\n",
      "  2. Recommended Paper ID: 557 (Similarity: 0.8160)\n",
      "  3. Recommended Paper ID: 530 (Similarity: 0.8124)\n",
      "  4. Recommended Paper ID: 205 (Similarity: 0.8064)\n",
      "  5. Recommended Paper ID: 512 (Similarity: 0.8063)\n",
      "  6. Recommended Paper ID: 516 (Similarity: 0.8063)\n",
      "  7. Recommended Paper ID: 258 (Similarity: 0.7955)\n",
      "  8. Recommended Paper ID: 353 (Similarity: 0.7910)\n",
      "  9. Recommended Paper ID: 34 (Similarity: 0.7905)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7898)\n",
      "\n",
      "Test Paper 54:\n",
      "  1. Recommended Paper ID: 127 (Similarity: 0.8047)\n",
      "  2. Recommended Paper ID: 544 (Similarity: 0.7978)\n",
      "  3. Recommended Paper ID: 448 (Similarity: 0.7890)\n",
      "  4. Recommended Paper ID: 556 (Similarity: 0.7800)\n",
      "  5. Recommended Paper ID: 16 (Similarity: 0.7779)\n",
      "  6. Recommended Paper ID: 293 (Similarity: 0.7586)\n",
      "  7. Recommended Paper ID: 507 (Similarity: 0.7584)\n",
      "  8. Recommended Paper ID: 47 (Similarity: 0.7572)\n",
      "  9. Recommended Paper ID: 5 (Similarity: 0.7568)\n",
      "  10. Recommended Paper ID: 419 (Similarity: 0.7507)\n",
      "\n",
      "Test Paper 55:\n",
      "  1. Recommended Paper ID: 98 (Similarity: 0.8130)\n",
      "  2. Recommended Paper ID: 360 (Similarity: 0.8036)\n",
      "  3. Recommended Paper ID: 119 (Similarity: 0.7495)\n",
      "  4. Recommended Paper ID: 530 (Similarity: 0.7465)\n",
      "  5. Recommended Paper ID: 216 (Similarity: 0.7427)\n",
      "  6. Recommended Paper ID: 180 (Similarity: 0.7359)\n",
      "  7. Recommended Paper ID: 574 (Similarity: 0.7278)\n",
      "  8. Recommended Paper ID: 160 (Similarity: 0.7197)\n",
      "  9. Recommended Paper ID: 435 (Similarity: 0.7125)\n",
      "  10. Recommended Paper ID: 143 (Similarity: 0.7117)\n",
      "\n",
      "Test Paper 56:\n",
      "  1. Recommended Paper ID: 218 (Similarity: 0.7820)\n",
      "  2. Recommended Paper ID: 168 (Similarity: 0.7409)\n",
      "  3. Recommended Paper ID: 419 (Similarity: 0.7346)\n",
      "  4. Recommended Paper ID: 312 (Similarity: 0.7282)\n",
      "  5. Recommended Paper ID: 370 (Similarity: 0.7274)\n",
      "  6. Recommended Paper ID: 444 (Similarity: 0.7241)\n",
      "  7. Recommended Paper ID: 163 (Similarity: 0.7237)\n",
      "  8. Recommended Paper ID: 281 (Similarity: 0.7225)\n",
      "  9. Recommended Paper ID: 530 (Similarity: 0.7175)\n",
      "  10. Recommended Paper ID: 435 (Similarity: 0.7132)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all papers using the trained model\n",
    "def get_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    paper_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "            batch_embeddings = model(input_ids, attention_mask)  # Keep as CUDA tensor\n",
    "            embeddings.append(batch_embeddings)  # Store without converting to NumPy\n",
    "            paper_indices.extend(batch[\"paper_index\"].numpy())\n",
    "\n",
    "    return torch.cat(embeddings, dim=0), np.array(paper_indices)  # Return PyTorch tensor\n",
    "\n",
    "# Get embeddings for train and test sets\n",
    "train_dataloader = DataLoader(PaperDataset(X_train, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(PaperDataset(X_test, tokenizer, 256), batch_size=8, shuffle=False)\n",
    "\n",
    "train_embeddings, train_indices = get_embeddings(train_dataloader, model)\n",
    "test_embeddings, test_indices = get_embeddings(test_dataloader, model)\n",
    "\n",
    "# Compute cosine similarity in CUDA\n",
    "train_embeddings = F.normalize(train_embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "test_embeddings = F.normalize(test_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sample Test Embedding:\", test_embeddings[0][:15])  # First 15 values\n",
    "print(\"Sample Train Embedding:\", train_embeddings[0][:15])  # First 15 values\n",
    "\n",
    "similarity_matrix = torch.matmul(test_embeddings, train_embeddings.T).cpu().numpy()  # Cosine similarity\n",
    "\n",
    "# Select top-N most similar papers\n",
    "top_n = 10\n",
    "top_indices = np.argsort(-similarity_matrix, axis=1)[:, :top_n]\n",
    "\n",
    "# Print recommended papers\n",
    "recommended_paper_ids = []\n",
    "\n",
    "for i, test_idx in enumerate(top_indices):\n",
    "    recommended_for_test = []\n",
    "    print(f\"\\nTest Paper {i+1}:\")\n",
    "    \n",
    "    for j, train_idx in enumerate(test_idx):\n",
    "        recommended_paper_id = X_train.iloc[train_indices[train_idx]][\"Id\"]\n",
    "        recommended_for_test.append(recommended_paper_id)\n",
    "        \n",
    "        print(f\"  {j+1}. Recommended Paper ID: {recommended_paper_id} (Similarity: {similarity_matrix[i, train_idx]:.4f})\")\n",
    "    \n",
    "    recommended_paper_ids.append(recommended_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af7a75-f560-4fce-84d8-c483c7855b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
