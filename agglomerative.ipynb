{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theodoros Kassa Aragie\n",
    "\n",
    "251163893\n",
    "\n",
    "ECE 9309\n",
    "\n",
    "Prof. Soodeh Nikan\n",
    "\n",
    "February 17, 2025\n",
    "\n",
    "# Implementing Agglomerative Clustering for a Recommender System\n",
    "\n",
    "Database: https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from helper import *\n",
    "\n",
    "# SciKit-Learn Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "import scipy.cluster.hierarchy as shc \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tensorflow/Keras Imports\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "\n",
    "# Restrict TensorFlow to only use the /GPU:0\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Set default Tensor type to double (float64)\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "# Ensure dump and parquets folders are ready\n",
    "if not os.path.isdir(\"./dump\"):\n",
    "    os.makedirs(\"./dump\")\n",
    "\n",
    "# Prepare the Universal Sentence Encoder (USE)\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned DF\n",
    "cleanedDF:pd.DataFrame = pd.read_csv(\n",
    "    \"database_clean.csv\"\n",
    ").drop(columns=[\"id\"])\n",
    "\n",
    "# Load USE embeddings from cleaned dataset\n",
    "embeddingsDF:pd.DataFrame = pd.read_csv(\n",
    "    \"use_embeddings.csv\"\n",
    ").drop(columns=[\"id\"])\n",
    "\n",
    "# Train 90%, Test 10%\n",
    "# Calculate the training portion and the remainder\n",
    "trnDF, tstDF = train_test_split(\n",
    "    # No need to split into X and Y\n",
    "    embeddingsDF, test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Trn: {trnDF.shape}, Tst: {tstDF.shape}\")\n",
    "\n",
    "# Compress via PCA to 2 features for AC plot:\n",
    "pca:PCA = PCA(n_components=2).fit(trnDF)\n",
    "trnComp:pd.DataFrame = pd.DataFrame(\n",
    "    pca.transform(trnDF), columns=[\"P1\", \"P2\"]\n",
    ")\n",
    "tstComp:pd.DataFrame = pd.DataFrame(\n",
    "    pca.transform(tstDF), columns=[\"P1\", \"P2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data and Tune for Best Agglomerative Clustering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram\n",
    "plt.figure(figsize =(8, 8)) \n",
    "plt.title('Visualising the data') \n",
    "Dendrogram = shc.dendrogram((shc.linkage(trnDF, method ='ward')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom wrapper for AgglomerativeClustering\n",
    "# (to work with GridSearchCV)\n",
    "class AgglomerativeClusteringCV(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, n_clusters:int=2):\n",
    "        # Set hyperparameters depending on inputted fields\n",
    "        self.n_clusters = n_clusters\n",
    "        self.model = None\n",
    "    def fit(self, X, y=None):\n",
    "        # AC model with current selection of hyperparameters\n",
    "        self.model = AgglomerativeClustering(\n",
    "            n_clusters=self.n_clusters,\n",
    "        )\n",
    "        # Fit and store cluster labels\n",
    "        self.labels_ = self.model.fit_predict(X)\n",
    "        return self\n",
    "    def fit_predict(self, X, y=None):\n",
    "        return self.fit(X).labels_\n",
    "\n",
    "# Custom Silhouette Scorer\n",
    "def silhScorer(estimator:AgglomerativeClusteringCV, X:npt.NDArray):\n",
    "    # Get cluster labels\n",
    "    labels:npt.NDArray = estimator.fit_predict(X)\n",
    "    # Need at least 2 clusters)\n",
    "    if len(set(labels)) < 2:\n",
    "        return -1\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Define Hyperparameter Grid\n",
    "paramGrid:dict[str, list] = {\n",
    "    # Number of clusters to search\n",
    "    'n_clusters': [2, 3, 4, 5, 6, 7],\n",
    "}\n",
    "\n",
    "# Check if search was already saved, otherwise run it\n",
    "gridSearch:GridSearchCV\n",
    "if not os.path.isfile(\"./dump/gridSearch.pkl\"):\n",
    "    gridSearch = GridSearchCV(\n",
    "        estimator=AgglomerativeClusteringCV(),\n",
    "        param_grid=paramGrid,\n",
    "        scoring=silhScorer, # Use custom Silhouette Score function\n",
    "        cv=3, # 3-fold cross-validation\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Fit GridSearch\n",
    "    gridSearch.fit(trnDF)\n",
    "    # Save it for later\n",
    "    joblib.dump(gridSearch, \"./dump/gridSearch.pkl\")\n",
    "else:\n",
    "    gridSearch = joblib.load(\"./dump/gridSearch.pkl\")\n",
    "\n",
    "# Best Parameters & Score\n",
    "print(\"Best Parameters:\", gridSearch.best_params_)\n",
    "print(\"Best Silhouette Score:\", gridSearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate optimal AC model\n",
    "bestAC:AgglomerativeClustering = AgglomerativeClustering(\n",
    "    **gridSearch.best_params_\n",
    ")\n",
    "# Calculate labels\n",
    "trnLabels = bestAC.fit_predict(trnDF)\n",
    "trnDF[\"cluster\"] = trnLabels\n",
    "tstLabels = bestAC.fit_predict(tstDF)\n",
    "tstDF[\"cluster\"] = tstLabels\n",
    "\n",
    "# Plotting optimial AC clusters\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.suptitle(\"Agglomerative Clusters\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    trnComp[\"P1\"], trnComp[\"P2\"], \n",
    "    c=trnLabels,\n",
    "    cmap=\"rainbow\"\n",
    ")\n",
    "plt.xlabel(\"P1\")\n",
    "plt.ylabel(\"P2\")\n",
    "plt.title(\"Training Set\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    tstComp[\"P1\"], tstComp[\"P2\"], \n",
    "    c=tstLabels,\n",
    "    cmap=\"rainbow\"\n",
    ")\n",
    "plt.xlabel(\"P1\")\n",
    "plt.ylabel(\"P2\")\n",
    "plt.title(\"Testing Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Recomendations, Cosine Similarity, Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend top n papers (default to top_n=10)\n",
    "def recommendPapers(top_n:int=10):\n",
    "    # Define lists\n",
    "    corrMeanList:list[float] = []\n",
    "    corrMedList:list[float] = []\n",
    "    simsList:list[npt.NDArray] = []\n",
    "    cosMeanList:list[float] = []\n",
    "    cosMedList:list[float] = []\n",
    "    topList:list[list[int]] = []\n",
    "    tfidfList:list[npt.NDArray] = []\n",
    "    testVecList:list[npt.NDArray] = []\n",
    "    topVecList:list[npt.NDArray] = []\n",
    "    # For each paper in the test set\n",
    "    # (so tstDF shape from before adding the cluster column):\n",
    "    for testPaperIndex in range(tstDF.shape[0]):\n",
    "        testPaperClusters:pd.DataFrame = tstDF.iloc[\n",
    "            testPaperIndex\n",
    "        ][\"cluster\"]\n",
    "        # Get all training papers in the same cluster\n",
    "        sameClusterPapers:pd.DataFrame = trnDF[\n",
    "            trnDF[\"cluster\"] == testPaperClusters\n",
    "        ]\n",
    "        # Ensure index alignment before selecting embeddings\n",
    "        sameClusterIndices:pd.Index = sameClusterPapers.index.intersection(\n",
    "            trnDF.index\n",
    "        )\n",
    "        # Get appropriate vectors\n",
    "        testEmbeddings:npt.NDArray = tstDF.iloc[\n",
    "            testPaperIndex\n",
    "        ].values.reshape(1, -1)\n",
    "        trainEmbeddings:npt.NDArray = trnDF.loc[\n",
    "            sameClusterIndices\n",
    "        ].values  # Exclude cluster column\n",
    "        # Compute the distances between points\n",
    "        distances:npt.NDArray = euclidean_distances(testEmbeddings, trainEmbeddings)[0]\n",
    "        # Get top-N most similar papers, highest similarity first\n",
    "        topIndices:npt.NDArray = np.argsort(distances)[-top_n:][::-1]\n",
    "        topPapers:list[int] = sorted(sameClusterIndices[topIndices].to_list())\n",
    "        topEmbeddings:npt.NDArray = trnDF.iloc[topIndices].values\n",
    "\n",
    "        # Print top 10 recommendations for current paper:\n",
    "        print(f\"Top 10 papers for paper {testPaperIndex}:\")\n",
    "        print(cleanedDF.iloc[topPapers])\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities:npt.NDArray = cosine_similarity(\n",
    "            testEmbeddings, topEmbeddings\n",
    "        )\n",
    "        # Pearson correlation\n",
    "        corrMean, corrMedian = compute_pearson_correlation(\n",
    "            testEmbeddings, topEmbeddings.reshape(1, top_n, -1)\n",
    "        )\n",
    "        # Cosine similarity score \n",
    "        cosMean, cosMedian = cosine_similarity_mean_median(similarities)\n",
    "        # TF-IDF\n",
    "        (\n",
    "            tfidfTestMatrix, testVectors, topRecommendedVectors\n",
    "        ) = compute_tfidf_similarity(\n",
    "            test_texts=cleanedDF.loc[[0], \"abstract\"].tolist(),\n",
    "            top10_texts=[cleanedDF.loc[idx, \"abstract\"] for idx in topPapers]\n",
    "        )\n",
    "        # Store values for each index\n",
    "        corrMeanList.append(corrMean)\n",
    "        corrMedList.append(corrMedian)\n",
    "        simsList.append(similarities)\n",
    "        cosMeanList.append(cosMean)\n",
    "        cosMedList.append(cosMedian)\n",
    "        topList.append(topPapers)\n",
    "        tfidfList.append(tfidfTestMatrix)\n",
    "        testVecList.append(testVectors)\n",
    "        topVecList.append(topRecommendedVectors)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"correlation_mean\": corrMeanList,\n",
    "        \"correlation_median\": corrMedList,\n",
    "        \"cosine_similarity\": simsList,\n",
    "        \"cosine_mean\": cosMeanList,\n",
    "        \"cosine_median\": cosMedList,\n",
    "        \"top_paper_indices\": topList,\n",
    "        \"tfidf_matrix\": tfidfList,\n",
    "        \"test_vectors\": testVecList,\n",
    "        \"top_vectors\": topVecList\n",
    "    })\n",
    "# Example usage on a query string\n",
    "recDF:pd.DataFrame= recommendPapers(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(bestAC, \"./dump/acModel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "loadedModel:AgglomerativeClustering = joblib.load(\"./dump/acModel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend top n papers (default to top_n=10)\n",
    "def selectPapers(query:str, top_n:int=10):\n",
    "    # Embed query string using Universal Serial Encoder\n",
    "    queryEmbeddings:npt.NDArray = embed(query.split()).numpy()\n",
    "    queryClusters = loadedModel.fit_predict(queryEmbeddings)\n",
    "    # Define lists\n",
    "    corrMeanList:list[float] = []\n",
    "    corrMedList:list[float] = []\n",
    "    simsList:list[npt.NDArray] = []\n",
    "    topList:list[list[int]] = []\n",
    "    cosMeanList:list[float] = []\n",
    "    cosMedList:list[float] = []\n",
    "    tfidfList:list[npt.NDArray] = []\n",
    "    testVecList:list[npt.NDArray] = []\n",
    "    topVecList:list[npt.NDArray] = []\n",
    "    # Get all training papers in the same cluster\n",
    "    sameClusterPapers:pd.DataFrame = trnDF[\n",
    "        trnDF[\"cluster\"] == queryClusters\n",
    "    ]\n",
    "    # Ensure index alignment before selecting embeddings\n",
    "    sameClusterIndices:pd.Index = sameClusterPapers.index.intersection(\n",
    "        trnDF.index\n",
    "    )\n",
    "    # Get appropriate vectors\n",
    "    trainEmbeddings:npt.NDArray = trnDF.loc[\n",
    "        sameClusterIndices\n",
    "    ].values  # Exclude cluster column\n",
    "    # Compute the distances between points\n",
    "    distances:npt.NDArray = euclidean_distances(queryEmbeddings, trainEmbeddings)[0]\n",
    "    # Get top-N most similar papers, highest similarity first\n",
    "    topIndices:npt.NDArray = np.argsort(distances)[-top_n:][::-1]\n",
    "    topPapers:list[int] = sorted(sameClusterIndices[topIndices].to_list())\n",
    "    topEmbeddings:npt.NDArray = trnDF.iloc[topIndices].values\n",
    "    # Compute cosine similarity\n",
    "    similarities:npt.NDArray = cosine_similarity(\n",
    "        queryEmbeddings, topEmbeddings\n",
    "    )\n",
    "    # Pearson correlation\n",
    "    corrMean, corrMedian = compute_pearson_correlation(\n",
    "        queryEmbeddings, topEmbeddings.reshape(1, top_n, -1)\n",
    "    )\n",
    "    # Cosine similarity score \n",
    "    cosMean, cosMedian = cosine_similarity_mean_median(similarities)\n",
    "    # TF-IDF\n",
    "    (\n",
    "        tfidfQueryMatrix, queryVectors, topSelectedVectors\n",
    "    ) = compute_tfidf_similarity(\n",
    "        test_texts=cleanedDF.loc[[0], \"abstract\"].tolist(),\n",
    "        top10_texts=[cleanedDF.loc[idx, \"abstract\"] for idx in topPapers]\n",
    "    )\n",
    "    # Store values for each index\n",
    "    corrMeanList.append(corrMean)\n",
    "    corrMedList.append(corrMedian)\n",
    "    simsList.append(similarities)\n",
    "    cosMeanList.append(cosMean)\n",
    "    cosMedList.append(cosMedian)\n",
    "    topList.append(topPapers)\n",
    "    tfidfList.append(tfidfQueryMatrix)\n",
    "    testVecList.append(queryVectors)\n",
    "    topVecList.append(topSelectedVectors)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"correlation_mean\": corrMeanList,\n",
    "        \"correlation_median\": corrMedList,\n",
    "        \"cosine_similarity\": simsList,\n",
    "        \"cosine_mean\": cosMeanList,\n",
    "        \"cosine_median\": cosMedList,\n",
    "        \"top_paper_indices\": topList,\n",
    "        \"tfidf_matrix\": tfidfList,\n",
    "        \"test_vectors\": testVecList,\n",
    "        \"top_vectors\": topVecList\n",
    "    })\n",
    "\n",
    "# Test on new papers\n",
    "queryDF:pd.DataFrame = selectPapers(query=\"stochastic hybrid system modeling\", top_n=10)\n",
    "print(cleanedDF.iloc[queryDF[\"top_paper_indices\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert titles into TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_matrix = vectorizer.fit_transform(cleanedDF[\"title\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Plot top 20 TF-IDF terms\n",
    "tfidf_df.mean().nlargest(20).plot(kind='bar', figsize=(10, 5))\n",
    "plt.title(\"Top 20 Most Important Terms in Titles\")\n",
    "plt.xlabel(\"Term\")\n",
    "plt.ylabel(\"TF-IDF Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
